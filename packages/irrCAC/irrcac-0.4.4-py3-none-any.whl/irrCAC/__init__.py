"""Inter-rater reliability

In statistics, inter-rater reliability (also called by various similar names,
such as inter-rater agreement, inter-rater concordance, inter-observer
reliability, and so on) is the degree of agreement among raters. It is a score
of how much homogeneity or consensus exists in the ratings given by various
judges.

There are a number of statistics that can be used to determine inter-rater
reliability. Different statistics are appropriate for different types of
measurement. Some options are

* Joint-probability of agreement
* `Cohen's kappa <https://en.wikipedia.org/wiki/Cohen%27s_kappa>`_
* `Scott's pi <https://en.wikipedia.org/wiki/Scott%27s_pi>`_
* `Fleiss' kappa <https://en.wikipedia.org/wiki/Fleiss%27_kappa>`_
* `Krippendorff's alpha <https://en.wikipedia.org/wiki/Krippendorff%27s_alpha>`_
* `Concordance Correlation Coefficient <https://reduced.to/q27kr>`_
* `Intra-Class Correlation <https://en.wikipedia.org/wiki/Intraclass_correlation>`_

and many others.
"""
