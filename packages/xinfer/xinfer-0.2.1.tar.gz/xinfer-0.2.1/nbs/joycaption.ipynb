{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                     Available Models                                     </span>\n",
       "┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Implementation </span>┃<span style=\"font-weight: bold\"> Model ID                                        </span>┃<span style=\"font-weight: bold\"> Input --&gt; Output    </span>┃\n",
       "┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> timm           </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> eva02_large_patch14_448.mim_m38m_ft_in22k_in1k  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image --&gt; class     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> timm           </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> eva02_large_patch14_448.mim_m38m_ft_in1k        </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image --&gt; class     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> timm           </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> eva02_large_patch14_448.mim_in22k_ft_in22k_in1k </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image --&gt; class     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> timm           </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> eva02_large_patch14_448.mim_in22k_ft_in1k       </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image --&gt; class     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> timm           </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> eva02_base_patch14_448.mim_in22k_ft_in22k_in1k  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image --&gt; class     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> timm           </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> eva02_base_patch14_448.mim_in22k_ft_in1k        </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image --&gt; class     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> timm           </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> eva02_small_patch14_336.mim_in22k_ft_in1k       </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image --&gt; class     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> timm           </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> eva02_tiny_patch14_336.mim_in22k_ft_in1k        </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image --&gt; class     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> transformers   </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> Salesforce/blip2-opt-6.7b-coco                  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> transformers   </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> Salesforce/blip2-flan-t5-xxl                    </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> transformers   </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> Salesforce/blip2-opt-6.7b                       </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> transformers   </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> Salesforce/blip2-opt-2.7b                       </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> transformers   </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> fancyfeast/llama-joycaption-alpha-two-hf-llava  </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> transformers   </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> vikhyatk/moondream2                             </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> transformers   </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> sashakunitsyn/vlrm-blip2-opt-2.7b               </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> ultralytics    </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> yolov8x                                         </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image --&gt; objects   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> ultralytics    </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> yolov8m                                         </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image --&gt; objects   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> ultralytics    </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> yolov8l                                         </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image --&gt; objects   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> ultralytics    </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> yolov8s                                         </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image --&gt; objects   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> ultralytics    </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> yolov8n                                         </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> image --&gt; objects   </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> ...            </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> ...                                             </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> ...                 </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> ...            </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> ...                                             </span>│<span style=\"color: #008000; text-decoration-color: #008000\"> ...                 </span>│\n",
       "└────────────────┴─────────────────────────────────────────────────┴─────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                     Available Models                                     \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mImplementation\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mModel ID                                       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mInput --> Output   \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mtimm          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35meva02_large_patch14_448.mim_m38m_ft_in22k_in1k \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage --> class    \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtimm          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35meva02_large_patch14_448.mim_m38m_ft_in1k       \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage --> class    \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtimm          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35meva02_large_patch14_448.mim_in22k_ft_in22k_in1k\u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage --> class    \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtimm          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35meva02_large_patch14_448.mim_in22k_ft_in1k      \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage --> class    \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtimm          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35meva02_base_patch14_448.mim_in22k_ft_in22k_in1k \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage --> class    \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtimm          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35meva02_base_patch14_448.mim_in22k_ft_in1k       \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage --> class    \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtimm          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35meva02_small_patch14_336.mim_in22k_ft_in1k      \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage --> class    \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtimm          \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35meva02_tiny_patch14_336.mim_in22k_ft_in1k       \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage --> class    \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtransformers  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mSalesforce/blip2-opt-6.7b-coco                 \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtransformers  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mSalesforce/blip2-flan-t5-xxl                   \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtransformers  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mSalesforce/blip2-opt-6.7b                      \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtransformers  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mSalesforce/blip2-opt-2.7b                      \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtransformers  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mfancyfeast/llama-joycaption-alpha-two-hf-llava \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtransformers  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mvikhyatk/moondream2                            \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtransformers  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35msashakunitsyn/vlrm-blip2-opt-2.7b              \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36multralytics   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35myolov8x                                        \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage --> objects  \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36multralytics   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35myolov8m                                        \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage --> objects  \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36multralytics   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35myolov8l                                        \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage --> objects  \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36multralytics   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35myolov8s                                        \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage --> objects  \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36multralytics   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35myolov8n                                        \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32mimage --> objects  \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m...           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m...                                            \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m...                \u001b[0m\u001b[32m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m...           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m...                                            \u001b[0m\u001b[35m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m...                \u001b[0m\u001b[32m \u001b[0m│\n",
       "└────────────────┴─────────────────────────────────────────────────┴─────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import xinfer\n",
    "\n",
    "xinfer.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-21 12:44:01.475\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxinfer.transformers.joycaption\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mModel: fancyfeast/llama-joycaption-alpha-two-hf-llava\u001b[0m\n",
      "\u001b[32m2024-10-21 12:44:01.476\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxinfer.transformers.joycaption\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mDevice: cuda\u001b[0m\n",
      "\u001b[32m2024-10-21 12:44:01.476\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxinfer.transformers.joycaption\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mDtype: bfloat16\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e60a6b42df4b8a99904f6cc195d157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = xinfer.create_model(\"fancyfeast/llama-joycaption-alpha-two-hf-llava\", device=\"cuda\", dtype=\"bfloat16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The image is a photograph, capturing a middle-aged woman standing on a stage during a performance or presentation. She has long, straight brown hair and wears glasses. Her outfit consists of a loose, green button-up shirt with long sleeves, and she appears to be gesturing towards the audience during her speech. Her expression is warm and engaged, with a slight smile on her face. The background is mostly dark, highlighting her figure. In the right background, there is part of a podium or stage setup'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = \"../assets/demo/0a6ee446579d2885.jpg\"\n",
    "prompt = \"Describe the image in detail.\"\n",
    "model.infer(image, prompt, max_new_tokens=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                 Model Stats                                  </span>\n",
       "╭───────────────────────────┬────────────────────────────────────────────────╮\n",
       "│<span style=\"font-weight: bold\"> Attribute                 </span>│<span style=\"font-weight: bold\"> Value                                          </span>│\n",
       "├───────────────────────────┼────────────────────────────────────────────────┤\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Model ID                  </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> fancyfeast/llama-joycaption-alpha-two-hf-llava </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Device                    </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> cuda                                           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Dtype                     </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> torch.bfloat16                                 </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Number of Inferences      </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 1                                              </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Total Inference Time (ms) </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 3924.9784                                      </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Average Latency (ms)      </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 3924.9784                                      </span>│\n",
       "╰───────────────────────────┴────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                 Model Stats                                  \u001b[0m\n",
       "╭───────────────────────────┬────────────────────────────────────────────────╮\n",
       "│\u001b[1m \u001b[0m\u001b[1mAttribute                \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mValue                                         \u001b[0m\u001b[1m \u001b[0m│\n",
       "├───────────────────────────┼────────────────────────────────────────────────┤\n",
       "│\u001b[36m \u001b[0m\u001b[36mModel ID                 \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mfancyfeast/llama-joycaption-alpha-two-hf-llava\u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mDevice                   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mcuda                                          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mDtype                    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mtorch.bfloat16                                \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mNumber of Inferences     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m1                                             \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mTotal Inference Time (ms)\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m3924.9784                                     \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mAverage Latency (ms)     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m3924.9784                                     \u001b[0m\u001b[35m \u001b[0m│\n",
       "╰───────────────────────────┴────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.stats.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                 Model Stats                                  </span>\n",
       "╭───────────────────────────┬────────────────────────────────────────────────╮\n",
       "│<span style=\"font-weight: bold\"> Attribute                 </span>│<span style=\"font-weight: bold\"> Value                                          </span>│\n",
       "├───────────────────────────┼────────────────────────────────────────────────┤\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Model ID                  </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> fancyfeast/llama-joycaption-alpha-two-hf-llava </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Device                    </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> cuda                                           </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Dtype                     </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> torch.bfloat16                                 </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Number of Inferences      </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 3                                              </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Total Inference Time (ms) </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 12268.8490                                     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Average Latency (ms)      </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 4089.6163                                      </span>│\n",
       "╰───────────────────────────┴────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                 Model Stats                                  \u001b[0m\n",
       "╭───────────────────────────┬────────────────────────────────────────────────╮\n",
       "│\u001b[1m \u001b[0m\u001b[1mAttribute                \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mValue                                         \u001b[0m\u001b[1m \u001b[0m│\n",
       "├───────────────────────────┼────────────────────────────────────────────────┤\n",
       "│\u001b[36m \u001b[0m\u001b[36mModel ID                 \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mfancyfeast/llama-joycaption-alpha-two-hf-llava\u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mDevice                   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mcuda                                          \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mDtype                    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35mtorch.bfloat16                                \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mNumber of Inferences     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m3                                             \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mTotal Inference Time (ms)\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m12268.8490                                    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mAverage Latency (ms)     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m4089.6163                                     \u001b[0m\u001b[35m \u001b[0m│\n",
       "╰───────────────────────────┴────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "images = [image] * batch_size\n",
    "prompts = [prompt] * batch_size\n",
    "\n",
    "model.infer_batch(images, prompts)\n",
    "model.stats.print_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.49 s, sys: 45.5 ms, total: 7.53 s\n",
      "Wall time: 7.28 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The image is a photograph capturing a woman in mid-action, possibly speaking or gesturing during a public event or speech. She stands in the center of the image, dressed in a bright green, long-sleeved dress buttoned up to the top. She is smiling, conveying a sense of engagement and friendliness. Her long, wavy brown hair partly covers her shoulders, adding texture contrast. She wears glasses with thin frames, reflecting a scholarly or professional aura. Around her left wrist, she'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.infer(images[0], prompt, max_new_tokens=100)\n",
    "model.infer(images[1], prompt, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.02 s, sys: 45.2 ms, total: 4.07 s\n",
      "Wall time: 3.83 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The image is a photograph depicting a middle-aged woman speaking on stage. She has light skin and shoulder-length brown hair. She is wearing rectangular glasses and a light green blouse with three-quarter length sleeves, accented by thin straps running through the front. Her blouse has subtle details, such as a collar and two slightly raised chest pockets on either side. She wears a light blue bracelet on her right wrist. Her facial expression appears calm and engaged, and she has a slight smile. Her hands are open',\n",
       " 'The image is a photograph of a woman on stage, possibly in a theater or a performance venue. She is the focal point of the image, standing in the center with her hands raised slightly as if gesturing. The woman appears to be mid-speech or demonstration. She has light skin and long, wavy hair that falls just past her shoulders. She wears glasses and is dressed in a green outfit with long sleeves, possibly a button-up shirt with the sleeves rolled up to her elbows, suggesting']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.infer_batch(images, prompts, max_new_tokens=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.launch_gradio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xinfer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
