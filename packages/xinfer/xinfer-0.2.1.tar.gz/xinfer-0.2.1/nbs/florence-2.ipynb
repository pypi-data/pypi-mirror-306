{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                            Available Models                            </span>\n",
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Implementation </span>â”ƒ<span style=\"font-weight: bold\"> Model ID                      </span>â”ƒ<span style=\"font-weight: bold\"> Input --&gt; Output    </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> transformers   </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\"> microsoft/Florence-2-base-ft  </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> transformers   </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\"> microsoft/Florence-2-large-ft </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> transformers   </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\"> microsoft/Florence-2-base     </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>â”‚\n",
       "â”‚<span style=\"color: #008080; text-decoration-color: #008080\"> transformers   </span>â”‚<span style=\"color: #800080; text-decoration-color: #800080\"> microsoft/Florence-2-large    </span>â”‚<span style=\"color: #008000; text-decoration-color: #008000\"> image-text --&gt; text </span>â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                            Available Models                            \u001b[0m\n",
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mImplementation\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mModel ID                     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mInput --> Output   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mtransformers  \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35mmicrosoft/Florence-2-base-ft \u001b[0m\u001b[35m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mtransformers  \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35mmicrosoft/Florence-2-large-ft\u001b[0m\u001b[35m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mtransformers  \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35mmicrosoft/Florence-2-base    \u001b[0m\u001b[35m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â”‚\u001b[36m \u001b[0m\u001b[36mtransformers  \u001b[0m\u001b[36m \u001b[0mâ”‚\u001b[35m \u001b[0m\u001b[35mmicrosoft/Florence-2-large   \u001b[0m\u001b[35m \u001b[0mâ”‚\u001b[32m \u001b[0m\u001b[32mimage-text --> text\u001b[0m\u001b[32m \u001b[0mâ”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import xinfer\n",
    "\n",
    "xinfer.list_models(\"florence-2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-29 21:05:27.411\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxinfer.models\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mModel: microsoft/Florence-2-large-ft\u001b[0m\n",
      "\u001b[32m2024-10-29 21:05:27.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxinfer.models\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1mDevice: cuda\u001b[0m\n",
      "\u001b[32m2024-10-29 21:05:27.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mxinfer.models\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mDtype: float16\u001b[0m\n",
      "/home/dnth/mambaforge-pypy3/envs/xinfer/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "Florence2LanguageForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A woman with glasses is gesturing with both hands.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xinfer.create_model(\"microsoft/Florence-2-large-ft\", device=\"cuda\", dtype=\"float16\")\n",
    "\n",
    "image = \"../assets/demo/0a6ee446579d2885.jpg\"\n",
    "prompt = \"<CAPTION>\"\n",
    "model.infer(image, prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.',\n",
       " 'A woman with glasses is gesturing with both hands.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 40\n",
    "model.infer_batch([image] * batch_size, [prompt] * batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In this image I can see a woman wearing green color dress and spectacles. Background is in black color.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = \"../assets/demo/0a6ee446579d2885.jpg\"\n",
    "prompt = \"<DETAILED_CAPTION>\"\n",
    "model.infer(image, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A woman is standing in front of a white wall. She is wearing a green shirt and has bracelets on her wrists. The woman has glasses on and her hair is long and brown. '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = \"../assets/demo/0a6ee446579d2885.jpg\"\n",
    "prompt = \"<MORE_DETAILED_CAPTION>\"\n",
    "model.infer(image, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bboxes': [], 'labels': []}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = \"../assets/demo/0a6ee446579d2885.jpg\"\n",
    "prompt = \"<CAPTION_TO_PHRASE_GROUNDING>\"\n",
    "model.infer(image, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bboxes': [[325.1200256347656,\n",
       "   221.33999633789062,\n",
       "   468.4800109863281,\n",
       "   268.94000244140625],\n",
       "  [337.40802001953125,\n",
       "   179.1800079345703,\n",
       "   466.4320373535156,\n",
       "   340.3399963378906],\n",
       "  [237.05601501464844, 124.0999984741211, 632.3200073242188, 678.97998046875]],\n",
       " 'labels': ['glasses', 'human face', 'woman']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = \"../assets/demo/0a6ee446579d2885.jpg\"\n",
    "prompt = \"<OD>\"\n",
    "model.infer(image, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bboxes': [[221.69601440429688,\n",
       "   124.0999984741211,\n",
       "   643.5840454101562,\n",
       "   678.97998046875],\n",
       "  [337.40802001953125,\n",
       "   179.86000061035156,\n",
       "   466.4320373535156,\n",
       "   340.3399963378906],\n",
       "  [325.1200256347656,\n",
       "   221.33999633789062,\n",
       "   468.4800109863281,\n",
       "   268.94000244140625]],\n",
       " 'labels': ['woman in green shirt with glasses on stage',\n",
       "  'human face',\n",
       "  'glasses']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = \"../assets/demo/0a6ee446579d2885.jpg\"\n",
    "prompt = \"<DENSE_REGION_CAPTION>\"\n",
    "model.infer(image, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bboxes': [[237.05601501464844,\n",
       "   124.0999984741211,\n",
       "   632.3200073242188,\n",
       "   678.97998046875],\n",
       "  [299.52001953125, 126.81999969482422, 501.2480163574219, 343.05999755859375],\n",
       "  [338.4320068359375, 179.1800079345703, 467.4560241699219, 340.3399963378906],\n",
       "  [505.3440246582031, 542.97998046875, 612.864013671875, 621.1799926757812],\n",
       "  [345.6000061035156, 557.9400024414062, 461.31201171875, 627.97998046875],\n",
       "  [325.1200256347656,\n",
       "   220.66000366210938,\n",
       "   468.4800109863281,\n",
       "   268.94000244140625],\n",
       "  [390.656005859375, 241.05999755859375, 429.5680236816406, 281.8600158691406],\n",
       "  [390.656005859375, 289.3399963378906, 440.83203125, 308.3800048828125]],\n",
       " 'labels': ['', '', '', '', '', '', '', '']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = \"../assets/demo/0a6ee446579d2885.jpg\"\n",
    "prompt = \"<REGION_PROPOSAL>\"\n",
    "model.infer(image, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"1000\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.launch_gradio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xinfer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
