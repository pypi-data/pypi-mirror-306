{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sociaml.analysis import *\n",
    "from sociaml.preprocessing import *\n",
    "\n",
    "import glob\n",
    "import tqdm\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "access_token = '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'\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = access_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file = '../tests/test_assets/President_Obama_takes_the_Oath_of_Office_(HD).ogv.720p.vp9.webm'\n",
    "\n",
    "audio_file = \"../tests/test_assets/audio.mp3\"\n",
    "audio_extractor = AudioExtractor()\n",
    "audio, samplerate = audio_extractor.process(video_file, audio_path=audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7ef8b985a2d0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/david/Programming/sociaML/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n",
      "WARNING:py.warnings:/home/david/Programming/sociaML/.venv/lib/python3.12/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m transcriber \u001b[38;5;241m=\u001b[39m \u001b[43mTranscriberAndDiarizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyannote_api_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHUGGINGFACE_TOKEN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmerge_consecutive_speakers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m anonymizer \u001b[38;5;241m=\u001b[39m Anonymizer()\n\u001b[1;32m      3\u001b[0m transcription \u001b[38;5;241m=\u001b[39m transcriber\u001b[38;5;241m.\u001b[39mprocess(video_file)\n",
      "File \u001b[0;32m~/Programming/sociaML/src/sociaml/preprocessing.py:172\u001b[0m, in \u001b[0;36mTranscriberAndDiarizer.__init__\u001b[0;34m(self, pyannote_api_key, merge_consecutive_speakers, min_segment_duration, device)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline \u001b[38;5;241m=\u001b[39m Pipeline\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyannote/speaker-diarization-3.1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    168\u001b[0m     use_auth_token\u001b[38;5;241m=\u001b[39mpyannote_api_key\n\u001b[1;32m    169\u001b[0m )\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhisper_model \u001b[38;5;241m=\u001b[39m \u001b[43mwhisper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmedium\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/sociaML/.venv/lib/python3.12/site-packages/whisper/__init__.py:154\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, device, download_root, in_memory)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m checkpoint_file\n\u001b[1;32m    153\u001b[0m dims \u001b[38;5;241m=\u001b[39m ModelDimensions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheckpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdims\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 154\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mWhisper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m alignment_heads \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Programming/sociaML/.venv/lib/python3.12/site-packages/whisper/model.py:263\u001b[0m, in \u001b[0;36mWhisper.__init__\u001b[0;34m(self, dims)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims \u001b[38;5;241m=\u001b[39m dims\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m AudioEncoder(\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_mels,\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_audio_ctx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_audio_layer,\n\u001b[1;32m    262\u001b[0m )\n\u001b[0;32m--> 263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mTextDecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_vocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_text_ctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_text_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_text_head\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_text_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# use the last half among the decoder layers for time alignment by default;\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# to use a specific set of heads, see `set_alignment_heads()` below.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m all_heads \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_text_layer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_text_head, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool\n\u001b[1;32m    274\u001b[0m )\n",
      "File \u001b[0;32m~/Programming/sociaML/.venv/lib/python3.12/site-packages/whisper/model.py:218\u001b[0m, in \u001b[0;36mTextDecoder.__init__\u001b[0;34m(self, n_vocab, n_ctx, n_state, n_head, n_layer)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(n_vocab, n_state)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mempty(n_ctx, n_state))\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks: Iterable[ResidualAttentionBlock] \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m    217\u001b[0m     [\n\u001b[0;32m--> 218\u001b[0m         \u001b[43mResidualAttentionBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_head\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcross_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_layer)\n\u001b[1;32m    220\u001b[0m     ]\n\u001b[1;32m    221\u001b[0m )\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln \u001b[38;5;241m=\u001b[39m LayerNorm(n_state)\n\u001b[1;32m    224\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(n_ctx, n_ctx)\u001b[38;5;241m.\u001b[39mfill_(\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf)\u001b[38;5;241m.\u001b[39mtriu_(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Programming/sociaML/.venv/lib/python3.12/site-packages/whisper/model.py:156\u001b[0m, in \u001b[0;36mResidualAttentionBlock.__init__\u001b[0;34m(self, n_state, n_head, cross_attention)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn_ln \u001b[38;5;241m=\u001b[39m LayerNorm(n_state) \u001b[38;5;28;01mif\u001b[39;00m cross_attention \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m n_mlp \u001b[38;5;241m=\u001b[39m n_state \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m--> 156\u001b[0m     \u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_mlp\u001b[49m\u001b[43m)\u001b[49m, nn\u001b[38;5;241m.\u001b[39mGELU(), Linear(n_mlp, n_state)\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_ln \u001b[38;5;241m=\u001b[39m LayerNorm(n_state)\n",
      "File \u001b[0;32m~/Programming/sociaML/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:112\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/sociaML/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:118\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m~/Programming/sociaML/.venv/lib/python3.12/site-packages/torch/nn/init.py:518\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity, generator)\u001b[0m\n\u001b[1;32m    516\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transcriber = TranscriberAndDiarizer(pyannote_api_key=os.getenv('HUGGINGFACE_TOKEN'),merge_consecutive_speakers=False)\n",
    "anonymizer = Anonymizer()\n",
    "transcription = transcriber.process(video_file)\n",
    "transcription = anonymizer.process(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription(contributions=[Contribution(start=0.6890937500000001, end=1.4653437500000002, speaker='SPEAKER_00', transcript='Please raise your right.'), Contribution(start=1.4653437500000002, end=1.83659375, speaker='SPEAKER_01', transcript='hand.'), Contribution(start=1.8534687500000002, end=2.8997187500000003, speaker='SPEAKER_01', transcript='and repeat after me.'), Contribution(start=3.5915937500000004, end=10.62846875, speaker='SPEAKER_00', transcript='I, <PERSON>, do solemnly swear... I, <PERSON>, do solemnly swear... That I will faithfully execute...'), Contribution(start=10.78034375, end=12.113468750000003, speaker='SPEAKER_00', transcript='that I will faithfully execute.'), Contribution(start=12.450968750000001, end=17.024093750000002, speaker='SPEAKER_00', transcript='The Office of President of the United States. The Office of President of the United States.'), Contribution(start=17.20971875, end=19.04909375, speaker='SPEAKER_01', transcript='and will to the best of my ability.'), Contribution(start=19.234718750000003, end=20.93909375, speaker='SPEAKER_00', transcript='and will to the best of my ability.'), Contribution(start=21.00659375, end=22.91346875, speaker='SPEAKER_00', transcript='preserve, protect, and defend.'), Contribution(start=22.99784375, end=24.65159375, speaker='SPEAKER_00', transcript='preserve, protect and defend.'), Contribution(start=24.65159375, end=26.47409375, speaker='SPEAKER_01', transcript='the Constitution of the United States.'), Contribution(start=26.47409375, end=33.13971875, speaker='SPEAKER_00', transcript='the Constitution of the United States. So help you God. So help me God. Congratulations, Mr. President. Thank you, Mr. Chief Justice. Thank you so much.'), Contribution(start=34.337843750000005, end=34.962218750000005, speaker='SPEAKER_00', transcript='Thank you, sir.'), Contribution(start=37.15596875, end=37.94909375, speaker='SPEAKER_00', transcript='Hey!'), Contribution(start=44.445968750000006, end=46.96034375, speaker='SPEAKER_00', transcript='All right. Thank you, everybody. Come.')])\n"
     ]
    }
   ],
   "source": [
    "#for contribution in transcription.contributions:\n",
    "#    print(contribution)\n",
    "\n",
    "import json\n",
    "\n",
    "filename = 'result.json'\n",
    "if os.path.exists(filename):\n",
    "    with open(filename, 'r') as fp:\n",
    "        transcription = Transcription.from_json(json.load(fp))\n",
    "else:\n",
    "    # save to json\n",
    "    with open(filename, 'w') as fp:\n",
    "        json.dump(transcription.to_json(), fp)\n",
    "\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sociaml.analysis import Analysis\n",
    "from sociaml.datastructures import Transcription, AnalysisMode\n",
    "\n",
    "from sociaml.text_analysis import GlobalSentimentAnalyzer, GlobalEkmanEmotionAnalyzer, ParticipantEkmanEmotionAnalyzer, ParticipantSentimentAnalyzer, ParticipantNLTKTokenCountAnalyzer, ParticipantContributionCount, ParticipantSentenceTransformerEmbeddingAnalyzer\n",
    "from sociaml.audio_analysis import ParticipantAudioSpeakingTimeAnalyzer, ParticipantMFCCAnalyzer, ParticipantAudioSilenceTimeAnalyzer, GlobalAudioEmotionAnalyzer,ParticipantAudioEmotionAnalyzer\n",
    "# from sociaml.video_analysis import ParticipantPyFeatVideoFeatureAnalyzer, GlobalPyFeatVideoFeatureAnalyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    analysis = Analysis(\n",
    "                        \n",
    "                        # textual features\n",
    "                        #GlobalSentimentAnalyzer(mode=AnalysisMode.ENTIRE), \n",
    "                        #GlobalEkmanEmotionAnalyzer(mode=AnalysisMode.ENTIRE),\n",
    "                        ParticipantEkmanEmotionAnalyzer(),\n",
    "                        ParticipantSentimentAnalyzer(),\n",
    "                        ParticipantNLTKTokenCountAnalyzer(),\n",
    "                        ParticipantContributionCount(),\n",
    "                        ParticipantSentenceTransformerEmbeddingAnalyzer(),\n",
    "                        \n",
    "                        \n",
    "                        # audio features\n",
    "                        GlobalAudioEmotionAnalyzer(mode=AnalysisMode.ENTIRE),\n",
    "                        ParticipantAudioEmotionAnalyzer(),\n",
    "                        ParticipantAudioSpeakingTimeAnalyzer(),\n",
    "                        ParticipantMFCCAnalyzer(),\n",
    "                        ParticipantAudioSilenceTimeAnalyzer(),\n",
    "                        \n",
    "                        \n",
    "                        # visual features\n",
    "                        # ParticipantPyFeatVideoFeatureAnalyzer(),\n",
    "                        # GlobalPyFeatVideoFeatureAnalyzer(skip_frames=500)\n",
    "                        \n",
    "                )\n",
    "    global_data, participant_data, contribution_data = analysis.analyze(transcription, str(video_file), str(audio_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contribution_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
