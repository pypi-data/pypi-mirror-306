{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate Forecast Skill vs Observations\n",
    "\n",
    "This example shows how to evaluate Salient's probabilistic forecasts against observations and calculate meaningful metrics. It demonstrates [validation best practices](https://salientpredictions.notion.site/Validation-0220c48b9460429fa86f577914ea5248) such as:\n",
    "\n",
    "- Proper scoring using the Continuous Ranked Probability Score (CRPS)\n",
    "  - Considers the full forecast distribution to reward both accuracy and precision\n",
    "  - Less sensitive to climatology decisions than metrics like Anomaly Correlation\n",
    "- A long backtesting period (2015-2022)\n",
    "  - Short evaluation periods are subject to noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<requests.sessions.Session at 0x7f7fabf06fd0>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the environment:\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "try:\n",
    "    import salientsdk as sk\n",
    "except ModuleNotFoundError as e:\n",
    "    if os.path.exists(\"../salientsdk\"):\n",
    "        sys.path.append(os.path.abspath(\"..\"))\n",
    "        import salientsdk as sk\n",
    "    else:\n",
    "        raise ModuleNotFoundError(\"Install salient SDK with: pip install salientsdk\")\n",
    "\n",
    "# Prevent wrapping on tables for readability\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.expand_frame_repr\", False)\n",
    "\n",
    "sk.set_file_destination(\"validation_example\")\n",
    "sk.login(\"username\", \"password\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize The Validation\n",
    "\n",
    "This notebook is written flexibly so you have the option of validating Salient and other forecasts multiple ways. These variables will control what, when, and how the validation proceeds:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Set the meteorological variable that we'll be evaluating:\n",
    "var = \"temp\"\n",
    "# var = \"precip\"\n",
    "\n",
    "# 2. Set the forecast look-ahead amount:\n",
    "timescale = \"sub-seasonal\"  # weeks 1-5\n",
    "# timescale = \"seasonal\"  # months 1-3\n",
    "# timescale = \"long-range\" # quarters 1-4\n",
    "\n",
    "# 3. Set the number of hindcasts to download for validation:\n",
    "split_set = \"sample\"  # fast demonstration of mechanics\n",
    "# split_set = \"test\"  # recommended to validate out-of-sample with hindcast_summary\n",
    "# split_set = \"all\"  # download hindcasts since 2000\n",
    "\n",
    "(start_date, end_date) = {\n",
    "    \"sample\": (\"2021-04-01\", \"2021-08-31\"),\n",
    "    \"test\": (\"2015-01-01\", \"2022-12-31\"),\n",
    "    \"all\": (\"2000-01-01\", \"2022-12-31\"),\n",
    "}[split_set]\n",
    "\n",
    "# 4. Decide whether or not to validate vs station observations:\n",
    "validate_obs = True  # Calculate skill of debiased forecast vs met stations\n",
    "# validate_obs = False  # Calculate skill of undebiased forecast vs ERA5\n",
    "\n",
    "\n",
    "fld = \"vals\"\n",
    "model = \"blend\"  # Validate the primary Salient blend model\n",
    "ref_model = \"clim\"  # Works across all timescale values.\n",
    "force = False  # If \"False\", cache data calls.  Set to \"True\" to overwrite caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Area of Interest\n",
    "\n",
    "The Salient SDK uses a \"Location\" object to specify the geographic bounds of a request. In this case, we will be validating against a vector of airport locations that are used to settle the Chicago Mercantile Exchange's Cooling and Heating Degree Day contracts. The SDK contains an example file with all of the locations pre-defined, as well as additional useful information. With `load_location_file` we can see that the file contains:\n",
    "\n",
    "- `lat` / `lon`: latitude and longitude of the met station, standard for a `location_file`\n",
    "- `name`: the 3-letter IATA airport code of the location, also `location_file` standard\n",
    "- `ghcnd`: the global climate network ID of the station\n",
    "- `cme`: the CME code for the location used to create CDD/HDD strips\n",
    "- `description`: full name of the airport\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         lat        lon name        ghcnd cme                description                     geometry\n",
      "0   33.62972  -84.44224  ATL  USW00013874   1         Atlanta Hartsfield   POINT (-84.44224 33.62972)\n",
      "1   42.36057  -71.00975  BOS  USW00014739   W               Boston Logan   POINT (-71.00975 42.36057)\n",
      "2   34.19966 -118.36543  BUR  USW00023152   P  Burbank-Glendale-Pasadena  POINT (-118.36543 34.19966)\n",
      "3   41.96017  -87.93164  ORD  USW00094846   2             Chicago O'Hare   POINT (-87.93164 41.96017)\n",
      "4   39.04443  -84.67241  CVG  USW00093814   3     Cincinnati (Covington)   POINT (-84.67241 39.04443)\n",
      "5   32.89744  -97.02196  DFW  USW00003927   5          Dallas-Fort Worth   POINT (-97.02196 32.89744)\n",
      "6   29.98438  -95.36072  IAH  USW00012960   R        Houston-George Bush   POINT (-95.36072 29.98438)\n",
      "7   36.07190 -115.16343  LAS  USW00023169   0         Las Vegas McCarran   POINT (-115.16343 36.0719)\n",
      "8   44.88523  -93.23133  MSP  USW00014922   Q         Minneapolis-StPaul   POINT (-93.23133 44.88523)\n",
      "9   40.77945  -73.88027  LGA  USW00014732   4        New York La Guardia   POINT (-73.88027 40.77945)\n",
      "10  39.87326  -75.22681  PHL  USW00013739   6               Philadelphia   POINT (-75.22681 39.87326)\n",
      "11  45.59578 -122.60919  PDX  USW00024229   7                   Portland  POINT (-122.60919 45.59578)\n",
      "12  38.50659 -121.49604  SAC  USW00023232   S            Sacramento Exec  POINT (-121.49604 38.50659)\n"
     ]
    }
   ],
   "source": [
    "# fmt: off\n",
    "loc = sk.Location(location_file=sk.upload_location_file(\n",
    "    lats =[33.62972     ,      42.36057,      34.19966,      41.96017,      39.04443,      32.89744,      29.98438,      36.07190,      44.88523,      40.77945,      39.87326,      45.59578,      38.50659],\n",
    "    lons =[-84.44224    ,     -71.00975,    -118.36543,     -87.93164,     -84.67241,     -97.02196,     -95.36072,    -115.16343,     -93.23133,     -73.88027,     -75.22681,    -122.60919,    -121.49604],\n",
    "    names=[\"ATL\"        ,         \"BOS\",         \"BUR\",         \"ORD\",         \"CVG\",         \"DFW\",         \"IAH\",         \"LAS\",         \"MSP\",         \"LGA\",         \"PHL\",         \"PDX\",         \"SAC\"],\n",
    "    ghcnd=[\"USW00013874\", \"USW00014739\", \"USW00023152\", \"USW00094846\", \"USW00093814\", \"USW00003927\", \"USW00012960\", \"USW00023169\", \"USW00014922\", \"USW00014732\", \"USW00013739\", \"USW00024229\", \"USW00023232\"],\n",
    "    cme  =[\"1\"          ,           \"W\",           \"P\",           \"2\",           \"3\",           \"5\",           \"R\",           \"0\",           \"Q\",           \"4\",           \"6\",           \"7\",           \"S\"],\n",
    "    geoname=\"cmeus\",\n",
    "    force=force,\n",
    "    description=[\"Atlanta Hartsfield\", \"Boston Logan\", \"Burbank-Glendale-Pasadena\", \"Chicago O'Hare\", \"Cincinnati (Covington)\",\"Dallas-Fort Worth\", \"Houston-George Bush\", \"Las Vegas McCarran\", \"Minneapolis-StPaul\", \"New York La Guardia\",\"Philadelphia\", \"Portland\", \"Sacramento Exec\"],\n",
    "))\n",
    "# fmt: on\n",
    "stations = loc.load_location_file()\n",
    "print(stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull precomputed skill\n",
    "\n",
    "Salient pre-calculates skill metrics as part of our internal validation and model improvement process. Use the `hindcast_summary` api endpoint to request pre-calculated skill scores. This is the \"easy\" way to validate Salient's forecasts: we've already done all the work for you.\n",
    "\n",
    "The remainder of this notebook will show you how to reproduce this skill calculation by requesting historical forecasts, historical data, and calculating a skill score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Reference CRPS  Salient CRPS  Salient CRPS Skill Score (%)\n",
      "Location Lead                                                              \n",
      "ATL      Week 1            1.52          0.48                          68.7\n",
      "         Week 2            1.52          1.06                          30.1\n",
      "         Week 3            1.53          1.36                          11.0\n",
      "         Week 4            1.52          1.41                           7.5\n",
      "         Week 5            1.52          1.41                           7.5\n",
      "...                         ...           ...                           ...\n",
      "SAC      Week 1            1.22          0.49                          60.0\n",
      "         Week 2            1.22          0.89                          27.4\n",
      "         Week 3            1.23          1.09                          11.3\n",
      "         Week 4            1.23          1.12                           8.9\n",
      "         Week 5            1.23          1.12                           9.2\n",
      "\n",
      "[65 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# hindcast_summary prefers single lat/lon values, so we'll iterate over each geo-pair.\n",
    "skill_summ = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(\n",
    "            sk.hindcast_summary(\n",
    "                loc=sk.Location(lat=row[\"lat\"], lon=row[\"lon\"]),\n",
    "                interp_method=\"linear\",\n",
    "                metric=\"crps\",\n",
    "                variable=var,\n",
    "                timescale=timescale,\n",
    "                reference=ref_model,\n",
    "                split_set=\"test\" if split_set == \"sample\" else split_set,\n",
    "                verbose=False,\n",
    "                force=force,\n",
    "            )\n",
    "        )\n",
    "        .assign(Location=row[\"name\"])\n",
    "        .drop(columns=[\"Reference Model\"])\n",
    "        .set_index([\"Location\", \"Lead\"])\n",
    "        for _, row in stations.iterrows()\n",
    "    ],\n",
    "    ignore_index=False,\n",
    ")\n",
    "print(skill_summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_skill(\n",
    "    df: pd.DataFrame, col: str = \"Salient CRPS Skill Score (%)\", title: str = \"Skill\"\n",
    ") -> None:\n",
    "    \"\"\"Plot skill scores in a table.\"\"\"\n",
    "    df = df.reset_index()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for location in df[\"Location\"].unique():\n",
    "        subset = df[df[\"Location\"] == location]\n",
    "        plt.plot(subset[\"Lead\"], subset[col], label=location)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Lead\")\n",
    "    plt.ylabel(col)\n",
    "    plt.legend(title=\"Location\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_skill(skill_summ, title=f\"hindcast_summary crps {model} vs {ref_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical Data\n",
    "\n",
    "To calculate forecast skill, we will want to compare forecasts made in the past with actuals. There are two flavors of actual data: (1) The ERA5 reanalysis dataset and (2) point weather station observations.\n",
    "\n",
    "Salient's forecast natively predicts (1) ERA5, but contains a debiasing function to remove bias between ERA5 and (2) station observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical ERA5 Data\n",
    "\n",
    "Download daily historical values from [`data_timeseries`](https://sdk.salientpredictions.com/api/#salientsdk.data_timeseries) and then aggregate to match the forecasts, so that we can ensure that all forecasts use the same dates.\n",
    "\n",
    "Also, get observed weather station data in the same format by downloading directly from NCEI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 22kB\n",
      "Dimensions:   (time: 198, location: 13)\n",
      "Coordinates:\n",
      "  * time      (time) datetime64[ns] 2kB 2021-03-27 2021-03-28 ... 2021-10-10\n",
      "  * location  (location) object 104B 'ATL' 'BOS' 'BUR' ... 'PHL' 'PDX' 'SAC'\n",
      "    lat       (location) float64 104B 33.63 42.36 34.2 ... 39.87 45.6 38.51\n",
      "    lon       (location) float64 104B -84.44 -71.01 -118.4 ... -122.6 -121.5\n",
      "Data variables:\n",
      "    vals      (time, location) float64 21kB 21.77 10.66 14.5 ... 12.31 16.97\n",
      "Attributes:\n",
      "    long_name:   2 metre temperature\n",
      "    units:       degC\n",
      "    clim_start:  1990-01-01\n",
      "    clim_end:    2019-12-31\n"
     ]
    }
   ],
   "source": [
    "# Get additional historical data beyond end_date to make sure we have enough\n",
    "# observed days to compare with the final forecast.\n",
    "duration = {\"sub-seasonal\": 8 * 5, \"seasonal\": 31 * 3, \"long-range\": 95 * 4}[timescale]\n",
    "hist = sk.data_timeseries(\n",
    "    loc=loc,\n",
    "    variable=var,\n",
    "    field=fld,\n",
    "    start=np.datetime64(start_date) - np.timedelta64(5, \"D\"),\n",
    "    end=np.datetime64(end_date) + np.timedelta64(duration, \"D\"),\n",
    "    frequency=\"daily\",\n",
    "    # reference_clim=\"30_yr\",  implicitly uses 30 yr climatology\n",
    "    verbose=False,\n",
    "    force=force,\n",
    ")\n",
    "print(xr.load_dataset(hist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical Observed Data\n",
    "\n",
    "Get observed historical data from meteorological stations. In this case, we'll write a `get_ghcnd` function that downloads observed station data and returns a list of pandas `DataFrame`s. If you have observed data from a proprietary source, you can substitute a function here that returns a vector of `DataFrame`s. Make sure that the `DataFrame`s have a column that matches the `variable` of interest (such as `temp`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Function: GHCNd Observed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "import requests\n",
    "\n",
    "\n",
    "def get_ghcnd(\n",
    "    ghcnd_id: str | Iterable[str],\n",
    "    start_date: str = \"2000-01-01\",\n",
    "    xdd: float = (65 - 32) * 5 / 9,\n",
    "    destination: str = \"-default\",\n",
    "    force: bool = False,\n",
    ") -> pd.DataFrame | list[pd.DataFrame]:\n",
    "    \"\"\"Get GHCNd observed data timeseries for a single station.\n",
    "\n",
    "    Global Historical Climatology Network - Daily.\n",
    "\n",
    "    Args:\n",
    "        ghcnd_id (str | list[str]): GHCND station ID or list of IDs\n",
    "        start_date (str): omit data before this date\n",
    "        xdd (float): base temperature for heating/cooling degree days, in degC.\n",
    "        destination (str): The directory to download the data to\n",
    "        force (bool): if True (default False), force update of observed data\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame | list[pd.DataFrame]: observed data timeseries with\n",
    "        columns `time`, `precip`, `wspd`, `tmin`, `tmax`, `tavg`, `hdd` and `cdd`\n",
    "        in units `degC`.  Also meta-data columns for `ghcn_id`, `lat`, `lon`, `elev`, and `name`.\n",
    "        Will return a list of DataFrames if wban_id is iterable.\n",
    "\n",
    "\n",
    "    Examples:\n",
    "        >>> obs_single = get_ghcnd(\"USW00013874\")\n",
    "        >>> obs_vector = get_ghcnd([\"USW00013874\", \"USW00014739\"])\n",
    "    \"\"\"\n",
    "    if isinstance(ghcnd_id, Iterable) and not isinstance(ghcnd_id, str):\n",
    "        return [get_ghcnd(single_id, start_date, xdd, force) for single_id in ghcnd_id]\n",
    "\n",
    "    file_name = os.path.join(sk.get_file_destination(), f\"{ghcnd_id}.csv\")\n",
    "    if force or not os.path.exists(file_name):\n",
    "        GHCND_ROOT = \"https://www.ncei.noaa.gov\"\n",
    "        GHCND_DIR = \"data/global-historical-climatology-network-daily/access\"\n",
    "        ghcnd_url = f\"{GHCND_ROOT}/{GHCND_DIR}/{ghcnd_id}.csv\"\n",
    "        r = requests.get(ghcnd_url)\n",
    "        r.raise_for_status()\n",
    "        with open(file_name, \"w\") as f:\n",
    "            f.write(r.text)\n",
    "\n",
    "    # Gusts: WSF1, WSF2, WSF5 are fastest 1-min, 2-min, 5-sec wind speed\n",
    "    # Humidity: RHAV, RHMN, RHMX\n",
    "    keep_cols = {\n",
    "        \"STATION\": \"ghcnd_id\",\n",
    "        \"DATE\": \"time\",\n",
    "        \"LATITUDE\": \"lat\",\n",
    "        \"LONGITUDE\": \"lon\",\n",
    "        \"ELEVATION\": \"elev\",  # meters\n",
    "        \"NAME\": \"name\",\n",
    "        \"TMAX\": \"tmax\",\n",
    "        \"TMIN\": \"tmin\",\n",
    "        \"PRCP\": \"precip\",\n",
    "        \"TAVG\": \"temp\",\n",
    "        \"AWND\": \"wspd\",\n",
    "    }\n",
    "\n",
    "    obs = pd.read_csv(file_name, usecols=keep_cols.keys())\n",
    "    obs.rename(columns=keep_cols, inplace=True)\n",
    "\n",
    "    # ncei uses 9999 for missing data\n",
    "    INVALID_NUMBER = 9999\n",
    "    obs.replace(INVALID_NUMBER, pd.NA, inplace=True)\n",
    "\n",
    "    # ncei uses 10ths of values\n",
    "    columns_to_decimalize = [\"precip\", \"tmax\", \"tmin\", \"wspd\", \"temp\"]\n",
    "    for col in columns_to_decimalize:\n",
    "        if col in obs.columns:\n",
    "            obs[col] = obs[col] / 10.0\n",
    "\n",
    "    obs = obs[obs[\"time\"] >= start_date]\n",
    "    obs[\"time\"] = pd.to_datetime(obs[\"time\"])\n",
    "\n",
    "    # Only calculate degree days if both tmin and tmax are available\n",
    "    if \"tmin\" in obs.columns and \"tmax\" in obs.columns:\n",
    "        # Note that these long_names are identical to what data_timeseries returns:\n",
    "        obs[\"tmax\"].attrs[\"units\"] = \"degC\"\n",
    "        obs[\"tmax\"].attrs[\"long_name\"] = \"2 metre daily temperature\"\n",
    "\n",
    "        obs[\"tmin\"].attrs[\"units\"] = \"degC\"\n",
    "        obs[\"tmin\"].attrs[\"long_name\"] = \"2 metre daily temperature\"\n",
    "\n",
    "        # HDD & CDD settle off the mean of tmin/tmax, not the daily mean\n",
    "        temp = pd.concat([obs[\"tmin\"], obs[\"tmax\"]], axis=1).mean(axis=1)\n",
    "        obs[\"cdd\"] = (temp - xdd).clip(lower=0).round(1)\n",
    "        obs[\"hdd\"] = (xdd - temp).clip(lower=0).round(1)\n",
    "\n",
    "        # Some stations such as USW00023152 don't record temp but do have tmin/tmax.\n",
    "        # Replace missing temp with mean(tmin,tmax) as an approximation.\n",
    "        temp_na_mask = obs[\"temp\"].isna()\n",
    "        obs.loc[temp_na_mask, \"temp\"] = temp[temp_na_mask]\n",
    "\n",
    "        obs[\"hdd\"].attrs[\"units\"] = \"HDD day-1 (degC)\"\n",
    "        obs[\"hdd\"].attrs[\"long_name\"] = \"Heating Degree Days\"\n",
    "\n",
    "        obs[\"cdd\"].attrs[\"units\"] = \"CDD day-1 (degC)\"\n",
    "        obs[\"cdd\"].attrs[\"long_name\"] = \"Cooling Degree Days\"\n",
    "\n",
    "    obs[\"temp\"].attrs[\"units\"] = \"degC\"\n",
    "    obs[\"temp\"].attrs[\"long_name\"] = \"2 metre temperature\"\n",
    "\n",
    "    obs[\"precip\"].attrs[\"units\"] = \"mm day-1\"\n",
    "    obs[\"precip\"].attrs[\"long_name\"] = \"Total precipitation\"\n",
    "\n",
    "    obs[\"wspd\"].attrs[\"units\"] = \"m s**-1\"\n",
    "    obs[\"wspd\"].attrs[\"long_name\"] = \"Wind Speed\"\n",
    "\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ghcnd_id       time       lat       lon   elev                                               name  precip  tmax  tmin  wspd  temp  cdd   hdd\n",
      "33327  USW00013874 2021-04-01  33.62972 -84.44224  308.2  ATLANTA HARTSFIELD JACKSON INTERNATIONAL AIRPO...     0.0  11.7   4.4   8.8   8.4  0.0  10.3\n",
      "33328  USW00013874 2021-04-02  33.62972 -84.44224  308.2  ATLANTA HARTSFIELD JACKSON INTERNATIONAL AIRPO...     0.0  13.9   0.6   4.0   6.7  0.0  11.1\n",
      "33329  USW00013874 2021-04-03  33.62972 -84.44224  308.2  ATLANTA HARTSFIELD JACKSON INTERNATIONAL AIRPO...     0.0  17.8   1.7   1.3   9.1  0.0   8.6\n",
      "33330  USW00013874 2021-04-04  33.62972 -84.44224  308.2  ATLANTA HARTSFIELD JACKSON INTERNATIONAL AIRPO...     0.0  22.8   4.4   2.5  13.8  0.0   4.7\n",
      "33331  USW00013874 2021-04-05  33.62972 -84.44224  308.2  ATLANTA HARTSFIELD JACKSON INTERNATIONAL AIRPO...     0.0  25.6   9.4   2.5  17.5  0.0   0.8\n",
      "...            ...        ...       ...       ...    ...                                                ...     ...   ...   ...   ...   ...  ...   ...\n",
      "34625  USW00013874 2024-10-20  33.62972 -84.44224  308.2  ATLANTA HARTSFIELD JACKSON INTERNATIONAL AIRPO...     0.0  25.6  11.1   2.6  17.8  0.0   0.0\n",
      "34626  USW00013874 2024-10-21  33.62972 -84.44224  308.2  ATLANTA HARTSFIELD JACKSON INTERNATIONAL AIRPO...     0.0  26.7  11.7   2.2  19.4  0.9   0.0\n",
      "34627  USW00013874 2024-10-22  33.62972 -84.44224  308.2  ATLANTA HARTSFIELD JACKSON INTERNATIONAL AIRPO...     0.0  26.1  12.8   2.0  19.3  1.1   0.0\n",
      "34628  USW00013874 2024-10-23  33.62972 -84.44224  308.2  ATLANTA HARTSFIELD JACKSON INTERNATIONAL AIRPO...     0.0  27.8  13.9   NaN  20.8  2.5   0.0\n",
      "34629  USW00013874 2024-10-24  33.62972 -84.44224  308.2  ATLANTA HARTSFIELD JACKSON INTERNATIONAL AIRPO...     0.0  28.3  15.6   NaN  20.8  3.6   0.0\n",
      "\n",
      "[1303 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "if validate_obs:\n",
    "    obs_df = get_ghcnd(stations.ghcnd, start_date=start_date, force=force)\n",
    "    # Output is a vector of DataFrames, one per station.  Let's inspect the first:\n",
    "    print(obs_df[0])\n",
    "else:\n",
    "    print(\"skipped: not comparing to observed data\")\n",
    "    obs_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `make_observed_ds` function to reformat the `DataFrame`s of observed data into an `xarray.Dataset` with the same format as the historical timeseries returned by `data_timeseries`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 146kB\n",
      "Dimensions:       (time: 1303, location: 13)\n",
      "Coordinates:\n",
      "  * time          (time) datetime64[ns] 10kB 2021-04-01 ... 2024-10-24\n",
      "  * location      (location) <U3 156B 'ATL' 'BOS' 'BUR' ... 'PHL' 'PDX' 'SAC'\n",
      "    lat_station   (location) float64 104B 33.63 42.36 34.2 ... 39.87 45.6 38.51\n",
      "    lon_station   (location) float64 104B -84.44 -71.01 -118.4 ... -122.6 -121.5\n",
      "    elev_station  (location) float64 104B 308.2 3.2 222.7 204.8 ... 2.1 6.7 5.9\n",
      "Data variables:\n",
      "    vals          (time, location) float64 136kB 8.4 10.3 22.5 ... 17.5 8.1 16.8\n",
      "Attributes:\n",
      "    short_name:  temp\n",
      "    units:       degC\n",
      "    long_name:   2 metre temperature\n"
     ]
    }
   ],
   "source": [
    "if validate_obs:\n",
    "    obs = sk.observed.make_observed_ds(\n",
    "        obs_df=obs_df,  # a DataFrame or vector of DataFrames\n",
    "        name=stations.name,  # this will populate the location coordinate\n",
    "        variable=var,  # make sure that the DataFrame(s) in obs_df contain this column name\n",
    "        time_col=\"time\",  # the name of the column in obs_df containing the date\n",
    "    )\n",
    "    print(obs)\n",
    "else:\n",
    "    print(\"skipped: not comparing to observed data\")\n",
    "    obs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare observed and ERA5 datasets\n",
    "\n",
    "Via `make_observed_ds`, the observed station data (`obs`) is formatted the same as the ERA5 historical data (`hist`). This means we can easily compare one to the other and see the degree of bias that exists between the two.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 83kB\n",
      "Dimensions:       (time: 193, location: 13)\n",
      "Coordinates:\n",
      "  * time          (time) datetime64[ns] 2kB 2021-04-01 2021-04-02 ... 2021-10-10\n",
      "  * location      (location) <U3 156B 'ATL' 'BOS' 'BUR' ... 'PHL' 'PDX' 'SAC'\n",
      "    lat_station   (location) float64 104B 33.63 42.36 34.2 ... 39.87 45.6 38.51\n",
      "    lon_station   (location) float64 104B -84.44 -71.01 -118.4 ... -122.6 -121.5\n",
      "    elev_station  (location) float64 104B 308.2 3.2 222.7 204.8 ... 2.1 6.7 5.9\n",
      "    lat           (location) float64 104B 33.63 42.36 34.2 ... 39.87 45.6 38.51\n",
      "    lon           (location) float64 104B -84.44 -71.01 -118.4 ... -122.6 -121.5\n",
      "Data variables:\n",
      "    obs           (time, location) float64 20kB 8.4 10.3 22.5 ... 18.8 14.0 15.7\n",
      "    hist          (time, location) float64 20kB 8.25 8.943 21.13 ... 12.31 16.97\n",
      "    delta_raw     (time, location) float64 20kB 0.1502 1.357 ... 1.688 -1.273\n",
      "    delta         (time, location) float64 20kB nan nan nan nan ... nan nan nan\n",
      "Attributes:\n",
      "    short_name:  temp\n",
      "    units:       degC\n",
      "    long_name:   2 metre temperature\n"
     ]
    }
   ],
   "source": [
    "if validate_obs:\n",
    "    # Pull observed and ERA5 into a single dataset for easy comparison:\n",
    "    merged = xr.merge(\n",
    "        [\n",
    "            obs.rename({\"vals\": \"obs\"}),\n",
    "            xr.load_dataset(hist).rename({\"vals\": \"hist\"}),\n",
    "        ],\n",
    "        join=\"inner\",\n",
    "    )\n",
    "    merged[\"delta_raw\"] = merged[\"obs\"] - merged[\"hist\"]\n",
    "    # Daily bias is noisy.  Smooth it out to make trends clearer.\n",
    "    # This will induce nans at the beginning and end of the timeseries.\n",
    "    merged[\"delta\"] = (\n",
    "        merged[\"delta_raw\"]\n",
    "        .rolling(time=max(1, int(len(merged[\"time\"]) * 0.1)), center=True)\n",
    "        .mean()\n",
    "    )\n",
    "\n",
    "    print(merged)\n",
    "else:\n",
    "    print(\"skipped: not comparing to observed data\")\n",
    "    merged = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if validate_obs:\n",
    "    # Visualize obs-era5 bias over time at each station\n",
    "    merged[\"delta\"].plot.line(x=\"time\", hue=\"location\")\n",
    "    plt.axhline(y=0, color=\"k\", linestyle=\"--\", alpha=0.5)\n",
    "    plt.title(\"Delta of Observed to Historical Values\")\n",
    "    plt.ylabel(f'{merged.attrs[\"long_name\"]} obs - hist [{merged.attrs[\"units\"]}]')\n",
    "    plt.xlabel(\"\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast\n",
    "\n",
    "The [`forecast_timeseries`](https://sdk.salientpredictions.com/api/#salientsdk.forecast_timeseries) API endpoint and SDK function returns Salient's native temporally granular weekly/monthly/quarterly forecasts.\n",
    "\n",
    "This is the most heavyweight call in the notebook, since it's getting multiple historical forecasts. In the first cell, we set a `split_set` variable that controls the amount of data to requeest via the `start_date` and `end_date` variables:\n",
    "\n",
    "- `sample` - a single year of data, good for quickly making sure that the mechanics of the process work.\n",
    "- `test` - gets data from 2015-2022, which is guaranteed out-of-sample from the training process. This requests a medium amount of data and is recommended for most validation processes.\n",
    "- `all` - gets data from 2000-2022, representing the full historical evaluation record. This will download quite a bit of data and is not recommended for most applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_hindcast_dates is a utility that returns all valid hindcast initializations.\n",
    "date_range = sk.get_hindcast_dates(start_date=start_date, end_date=end_date, timescale=timescale)\n",
    "\n",
    "fcst = sk.forecast_timeseries(\n",
    "    loc=loc,\n",
    "    variable=var,\n",
    "    field=fld,\n",
    "    date=date_range,\n",
    "    timescale=timescale,\n",
    "    model=[model, ref_model],\n",
    "    reference_clim=\"30_yr\",  # this is the same climatology used by data_timeseries\n",
    "    debias=False,\n",
    "    verbose=False,\n",
    "    force=force,\n",
    "    strict=False,  # There is missing data in 2020.  Work around it.\n",
    ")\n",
    "fcst[\"debias\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            file_name        date  model  debias\n",
      "0   validation_example/forecast_timeseries_2080b77...  2021-04-01  blend   False\n",
      "1   validation_example/forecast_timeseries_d2f085b...  2021-04-01   clim   False\n",
      "2   validation_example/forecast_timeseries_3e8ae2d...  2021-04-04  blend   False\n",
      "3   validation_example/forecast_timeseries_cbd0d20...  2021-04-04   clim   False\n",
      "4   validation_example/forecast_timeseries_f8f2b29...  2021-04-07  blend   False\n",
      "..                                                ...         ...    ...     ...\n",
      "62  validation_example/forecast_timeseries_c53d5ea...  2021-08-21  blend    True\n",
      "63  validation_example/forecast_timeseries_6edae86...  2021-08-24  blend    True\n",
      "64  validation_example/forecast_timeseries_86981d1...  2021-08-25  blend    True\n",
      "65  validation_example/forecast_timeseries_ea48660...  2021-08-28  blend    True\n",
      "66  validation_example/forecast_timeseries_07800f3...  2021-08-31  blend    True\n",
      "\n",
      "[201 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Get debiased hindcasts, if we are validating vs observations\n",
    "if validate_obs:\n",
    "    fcst_debias = sk.forecast_timeseries(\n",
    "        loc=loc,\n",
    "        variable=var,\n",
    "        field=fld,\n",
    "        date=date_range,\n",
    "        timescale=timescale,\n",
    "        model=model,  # debias only avilable for model blend\n",
    "        reference_clim=\"30_yr\",\n",
    "        debias=True,  # Note debias\n",
    "        verbose=False,\n",
    "        force=force,\n",
    "        strict=False,\n",
    "    )\n",
    "    fcst_debias[\"model\"] = model\n",
    "    fcst_debias[\"debias\"] = True\n",
    "    fcst = pd.concat([fcst, fcst_debias], axis=0)\n",
    "\n",
    "print(fcst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 13kB\n",
      "Dimensions:                 (quantiles: 23, location: 13, lead_weekly: 5,\n",
      "                             nbnds: 2)\n",
      "Coordinates:\n",
      "  * quantiles               (quantiles) float64 184B 0.01 0.025 ... 0.975 0.99\n",
      "  * location                (location) object 104B 'ATL' 'BOS' ... 'PDX' 'SAC'\n",
      "    lat                     (location) float64 104B 33.63 42.36 ... 45.6 38.51\n",
      "    lon                     (location) float64 104B -84.44 -71.01 ... -121.5\n",
      "    forecast_period_weekly  (lead_weekly, nbnds) datetime64[ns] 80B 2021-04-0...\n",
      "  * lead_weekly             (lead_weekly) int32 20B 1 2 3 4 5\n",
      "    forecast_date_weekly    datetime64[ns] 8B 2021-04-01\n",
      "Dimensions without coordinates: nbnds\n",
      "Data variables:\n",
      "    vals_weekly             (lead_weekly, location, quantiles) float64 12kB 9...\n",
      "Attributes:\n",
      "    clim_period:  ['1990-01-01', '2019-12-31']\n",
      "    region:       north-america\n",
      "    short_name:   temp\n",
      "    timescale:    sub-seasonal\n"
     ]
    }
   ],
   "source": [
    "# Check to see if there are any missing forecasts:\n",
    "fcst_na = fcst[fcst[\"file_name\"].isna()]\n",
    "if not fcst_na.empty:\n",
    "    print(\"Missing forecast dates:\")\n",
    "    print(fcst_na)\n",
    "\n",
    "# Example forecast file is for a single model and a single forecast_date\n",
    "print(xr.load_dataset(fcst[\"file_name\"].values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Skill Metrics\n",
    "\n",
    "Compare the forecast and observed datasets to see how well they match.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_fcst = sk.skill.crps(\n",
    "    observations=hist,\n",
    "    forecasts=fcst[(fcst[\"model\"] == model) & (fcst[\"debias\"] == False)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_ref = sk.skill.crps(\n",
    "    observations=hist,\n",
    "    forecasts=fcst[(fcst[\"model\"] == ref_model) & (fcst[\"debias\"] == False)],\n",
    ")\n",
    "skills = [\n",
    "    skill_ref.assign_coords(source=ref_model),\n",
    "    skill_fcst.assign_coords(source=model),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 2kB\n",
      "Dimensions:       (location: 13, lead_weekly: 5, source: 3)\n",
      "Coordinates:\n",
      "  * location      (location) object 104B 'ATL' 'BOS' 'BUR' ... 'PHL' 'PDX' 'SAC'\n",
      "    lat           (location) float64 104B 33.63 42.36 34.2 ... 39.87 45.6 38.51\n",
      "    lon           (location) float64 104B -84.44 -71.01 -118.4 ... -122.6 -121.5\n",
      "  * lead_weekly   (lead_weekly) int32 20B 1 2 3 4 5\n",
      "  * source        (source) <U8 96B 'clim' 'blend' 'debiased'\n",
      "    lat_station   (location) float64 104B 33.63 42.36 34.2 ... 39.87 45.6 38.51\n",
      "    lon_station   (location) float64 104B -84.44 -71.01 -118.4 ... -122.6 -121.5\n",
      "    elev_station  (location) float64 104B 308.2 3.2 222.7 204.8 ... 2.1 6.7 5.9\n",
      "Data variables:\n",
      "    crps          (source, lead_weekly, location) float64 2kB 0.79 1.2 ... 1.02\n",
      "Attributes:\n",
      "    clim_period:  ['1990-01-01', '2019-12-31']\n",
      "    region:       north-america\n",
      "    short_name:   crps\n",
      "    timescale:    sub-seasonal\n",
      "    long_name:    CRPS\n"
     ]
    }
   ],
   "source": [
    "if validate_obs:\n",
    "    skill_obs = sk.skill.crps(\n",
    "        observations=obs,\n",
    "        forecasts=fcst[(fcst[\"model\"] == model) & (fcst[\"debias\"] == True)],\n",
    "    )\n",
    "    skills.append(skill_obs.assign_coords(source=\"debiased\"))\n",
    "\n",
    "skills = xr.concat(skills, dim=\"source\").round(2)\n",
    "print(skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills[\"crps\"].plot.line(\n",
    "    x=\"lead_weekly\", hue=\"source\", col=\"location\", col_wrap=3, figsize=(10, 10)\n",
    ")\n",
    "plt.suptitle(f\"{var} CRPS\", fontsize=16)\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Relative Skill\n",
    "\n",
    "CRPS shows skill without context. A \"skill score\" will compare two different skills to generate relative value. In the example below, we will compare the Salient blend with climatology (historical averages).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Reference CRPS  Salient CRPS  Salient CRPS Skill Score (%)\n",
      "Location Lead                                                              \n",
      "ATL      Week 1            0.79          0.36                          54.6\n",
      "         Week 2            0.73          0.69                           6.2\n",
      "         Week 3            0.69          0.75                          -8.2\n",
      "         Week 4            0.66          0.73                          -9.4\n",
      "         Week 5            0.66          0.77                         -15.7\n",
      "...                         ...           ...                           ...\n",
      "SAC      Week 1            1.13          0.44                          60.7\n",
      "         Week 2            1.19          0.91                          23.8\n",
      "         Week 3            1.18          1.03                          12.6\n",
      "         Week 4            1.17          0.97                          17.1\n",
      "         Week 5            1.15          0.97                          15.8\n",
      "\n",
      "[65 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "skill_score = sk.skill.crpss(forecast=skill_fcst, reference=skill_ref)\n",
    "\n",
    "# Represent the skill scores as a human-readable table of the same format as we generated\n",
    "# for the hindcast_summary results.\n",
    "skill_table = (\n",
    "    xr.merge(\n",
    "        [\n",
    "            (skill_ref.rename({\"crps\": \"Reference CRPS\"})).round(2),\n",
    "            skill_fcst.rename({\"crps\": \"Salient CRPS\"}).round(2),\n",
    "            (skill_score * 100).rename({\"crpss\": \"Salient CRPS Skill Score (%)\"}).round(1),\n",
    "        ]\n",
    "    )\n",
    "    .to_dataframe()\n",
    "    .reset_index()\n",
    "    .dropna(how=\"any\")\n",
    "    .drop(columns=[\"lat\", \"lon\"])\n",
    "    .rename(columns={\"location\": \"Location\", \"lead_weekly\": \"Lead\"})\n",
    ")\n",
    "skill_table[\"Lead\"] = \"Week \" + skill_table[\"Lead\"].astype(str)\n",
    "skill_table.set_index([\"Location\", \"Lead\"], inplace=True)\n",
    "\n",
    "print(skill_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_skill(skill_summ, title=f\"manually-calculated crps {model} vs {ref_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare manually-calculated to pre-computed skill\n",
    "\n",
    "Now that we have a CRPS calculated manually as well as downloaded from `hindcast_summary` we can evaluate how close the two values are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_merge = pd.merge(\n",
    "    skill_summ.add_prefix(\"Summary \"),\n",
    "    skill_table.add_prefix(\"Manual \"),\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ")\n",
    "\n",
    "print(skill_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the manually-calculated skill score with the precomputed skill scores published by `hindcast_summary`.\n",
    "\n",
    "Note that when using `split_set = sample` the values won't match exactly. In this case we are plotting skill scores calculated from a single year of forecasts against the skill scores from the `test` set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_cols(col_name: str) -> None:\n",
    "    \"\"\"Plot manual and precalculated skill columns.\"\"\"\n",
    "    summary_col = f\"Summary {col_name}\"\n",
    "    manual_col = f\"Manual {col_name}\"\n",
    "\n",
    "    df = skill_merge.reset_index()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for location in df[\"Location\"].unique():\n",
    "        subset = df[df[\"Location\"] == location]\n",
    "        plt.scatter(subset[summary_col], subset[manual_col], label=location, s=100)\n",
    "\n",
    "    # Same limits for both axes\n",
    "    min_limit = min(df[summary_col].min(), df[manual_col].min())\n",
    "    max_limit = max(df[summary_col].max(), df[manual_col].max())\n",
    "    plt.xlim(min_limit, max_limit)\n",
    "    plt.ylim(min_limit, max_limit)\n",
    "    plt.plot(\n",
    "        [min_limit, max_limit], [min_limit, max_limit], color=\"gray\", linestyle=\"--\", linewidth=1\n",
    "    )\n",
    "    plt.gca().set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "    plt.title(f\"Summary vs Manual {col_name}\")\n",
    "    plt.xlabel(summary_col)\n",
    "    plt.ylabel(manual_col)\n",
    "    plt.legend(title=\"Location\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "    if split_set == \"sample\":\n",
    "        plt.text(\n",
    "            min_limit,\n",
    "            max_limit,\n",
    "            \"Results not expected to match for split_set = 'summary'.\\nUse split_set = 'test' for a full comparison.\",\n",
    "            fontsize=10,\n",
    "            verticalalignment=\"top\",\n",
    "            horizontalalignment=\"left\",\n",
    "            color=\"red\",\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "compare_cols(\"Salient CRPS Skill Score (%)\")\n",
    "# compare_cols(\"Salient CRPS\")\n",
    "# compare_cols(\"Reference CRPS\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salient",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
