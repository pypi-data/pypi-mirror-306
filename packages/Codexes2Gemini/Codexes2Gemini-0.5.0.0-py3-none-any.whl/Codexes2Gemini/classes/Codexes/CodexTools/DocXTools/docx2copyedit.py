#  Copyright (c) 2023. Fred Zimmerman.  Personal or educational use only.  All commercial and enterprise use must be licensed, contact wfz@nimblebooks.com

# from typing import NamedTuple
import csv
import os
import re

from docx import Document

from app.utilities.utilities import create_safe_dir_from_file_path

'''
This code is getting the core properties of a document The code block is returning 
a list of dictionaries with all the core properties and their values
- generated by stenography autopilot [ üöóüë©‚Äç‚úàÔ∏è ] 
'''


def main(filename, output_dir):
    paths = create_safe_dir_from_file_path(filename, output_dir)
    thisdoc_dir = paths[0]
    thisdoc_basename = paths[1]

    document = Document(filename)
    core_properties = document.core_properties
    core_properties_list = get_core_properties(document)
    styles = get_all_styles_in_docx(document)
    fonts_list = get_unique_fonts_in_docx(document)
    # acronyms = codex_find_acronyms(document)
    # abbreviations = schwartz_hearst(document)
    word_frequencies = build_word_usage_freq_dict(document)

    hyphen_sentences = find_hyphen_sentences(document)
    endash_sentences = en_dash(document)
    quotes_with_unmatched_quotes = unmatched_quotes(document)

    # pretty print list of dicts in core properties list
    for index, core_property in enumerate(core_properties_list):
        print(index, core_property)

    print("\n")

    for word, number_of_usages in sorted(word_frequencies.items()):
        with open(thisdoc_dir + '/' + 'word_usage_freq.csv', 'a') as f:
            writer = csv.writer(f)
            writer.writerow([word, number_of_usages])

    print(*styles, sep='\n', file=open(thisdoc_dir + '/' + 'styles.txt', 'w'))
    print(*fonts_list, sep='\n', file=open(thisdoc_dir + '/' + 'fonts.txt', 'w'))
    # print(*acronyms, sep='\n', file=open(thisdoc_dir +  '/' + 'acronyms.txt', 'w'))

    if len(hyphen_sentences) > 0:
        print(*hyphen_sentences, sep='\n', file=open(thisdoc_dir + '/' + 'hyphen_sentences.txt', 'w'))
    if len(endash_sentences) > 0:
        print(*endash_sentences, sep='\n', file=open(thisdoc_dir + '/' + 'endash_sentences.txt', 'w'))
    if len(quotes_with_unmatched_quotes) > 0:
        print(*quotes_with_unmatched_quotes, sep='\n',
              file=open(thisdoc_dir + '/' + 'quotes_with_unmatched_quotes.txt', 'w'))

    return core_properties_list, styles, fonts_list  # word_frequencies, hyphen_sentences, endash_sentences, quotes_with_unmatched_quotes


def get_core_properties(document):
    core_properties = document.core_properties
    # get a list of all the core properties
    core_properties_list = [{"author: ": core_properties.author},
                            {"category: ": core_properties.category},
                            {"comments: ": core_properties.comments},
                            {"content_status: ": core_properties.content_status,
                             "created: ": core_properties.created,
                             "identifier: ": core_properties.identifier,
                             "keywords: ": core_properties.keywords,
                             "language: ": core_properties.language,
                             "last_modified_by: ": core_properties.last_modified_by,
                             "last_printed: ": core_properties.last_printed,
                             "modified: ": core_properties.modified,
                             "revision: ": core_properties.revision,
                             "subject: ": core_properties.subject,
                             "title: ": core_properties.title},
                            {"version: ": core_properties.version}]
    return core_properties_list


'''
This code is getting all the styles used in a document
- generated by stenography autopilot [ üöóüë©‚Äç‚úàÔ∏è ] 
'''


def get_all_styles_in_docx(document):
    # get all unique styles used in the document
    # get a list of all styles used in the documen
    styles = document.styles
    styles_list = set([style.name for style in styles])
    return styles_list


# build word usage freq dictionary
'''
This code is creating a dictionary that counts the number of times each word appears 
in the document
- generated by stenography autopilot [ üöóüë©‚Äç‚úàÔ∏è ] 
'''


def build_word_usage_freq_dict(document):
    word_usage_freq_dict = {}
    for paragraph in document.paragraphs:
        for word in paragraph.text.split():
            if word in word_usage_freq_dict:
                word_usage_freq_dict[word] += 1
            else:
                word_usage_freq_dict[word] = 1
    return word_usage_freq_dict


def get_unique_fonts_in_docx(document):
    # get a list of all the fonts usedin the document
    fonts_used = []
    for par_number, par in enumerate(document.paragraphs):
        # print('Paragraph', par_number, ':', par.style.font.name)
        fonts_used.append(par.style.font.name)
        for run_number, run in enumerate(par.runs):
            # print('    Run', run_number, ':', run.font.name)
            fonts_used.append(run.font.name)
    unique_fonts = set(fonts_used)
    return unique_fonts


def get_all_paragraphs_in_docx(document):
    # get a list of all the paragraphs
    paragraphs = document.paragraphs
    return paragraphs


def regex_change_in_all_paragraphs_in_docx(document, regex, replace_str):
    # get a list of all the paragraphs
    for paragraph in document.paragraphs:
        paragraph_replace_text(paragraph, regex, replace_str)
    return document


def codex_find_acronyms(document):
    acronyms = []
    for paragraph in document.paragraphs:
        if '(' in paragraph.text and ')' in paragraph.text:  # check if there is a parenthesis in the text of the current paragraph object, then add it to the list of acronyms using append() method.  If you don't find any parenthesis, skip to next step with continue statement below.  
            acronym = ''  # create an empty string variable called acronym that will hold all letters before and after parentheses (if any) from each paragraph object's text attribute (i.e., p_obj.)  This will be used later to compare against other words/phrases from same document object's paragraphs attribute (i.e., doc_obj.)

            for char in paragraph.text:  # loop over each character within each p_obj's text attribute and check if it is a letter or not by using .isalpha() method on each character within p_obj's text attribute...

                if char == '(':  # ...if you find an open parentheses, break out of this inner loop by using break statement so you can start looking for letters after the close parentheses...

                    break

                elif char == ')':  # ...else if you find a close parentheses, ignore it because we are only interested in letters before the parentheses...

                    continue

                elif char.isalpha():  # ...else if you find a letter, add it to acronym variable using .append() method so that you can use this string of letters later to compare against other words/phrases from same document object's paragraphs attribute (i.e., doc_obj.)

                    acronym += char  # append each character within p_obj's text attribute to acronym variable

            for paragraph in document.paragraphs:  # loop over each paragraph object within doc_obj's paragraphs attribute and check if any of these objects' text attributes contain the string stored in acronym variable...

                if acronym in paragraph.text:  # ...if there is a match, then add the first word from that paragraph object's text attribute (i.e., p_obj) to list of acronyms using .append() method so we can return this list as our function output...

                    acronyms.append(paragraph.text[
                                        0])  # append first word from current p_obj (i.e., current paragraph) within doc_obj's paragraphs attribute to list of acronyms

        else:  # else if there is no parenthesis found in current p_obj, skip all code below and move on to next step with continue statement below...

            continue  # skip all code below and move on to next step with continue statement above

    return acronyms  # after looping through all paragraphs objects within doc_obj, return list of acronyms as function output


def en_dash(document):
    endash_sentences = []
    for para in document.paragraphs:
        for run in para.runs:
            if '‚Äì' in run.text:
                endash_sentences.append(run.text)
    return endash_sentences


def find_hyphen_sentences(document):
    hyphen_sentences = []
    for para in document.paragraphs:
        # get sentences with hyphens
        if '-' in para.text:
            hyphen_sentences.append(para.text)
    return hyphen_sentences


def unmatched_quotes(document):
    sentences = []

    for paragraph in document.paragraphs:
        if re.search('[‚Äú‚Äù]', paragraph.text) and not re.search('[‚Äú]', paragraph.text) or \
                not re.search('[‚Äù]', paragraph.text) and re.search('[‚Äú]', paragraph.text):

            sentences += [sentence for sentence in str(paragraph).split(".") if "." in sentence]

        elif "[" in str(paragraph).replace(" ", "") or "]" in str(paragraph).replace(" ", ""):

            sentences += [sentence for sentence in str(paragraph).split(".") if "." in sentence]

    return sentences


def compile_regex(expression):
    regex = re.compile(expression)
    return regex


def paragraph_replace_text(paragraph, regex, replace_str):
    """Return `paragraph` after replacing all matches for `regex` with `replace_str`.

    `regex` is a compiled regular expression prepared with `re.compile(pattern)`
    according to the Python library documentation for the `re` module.
    """
    # --- a paragraph may contain more than one match, loop until all are replaced ---
    run = ''
    text = ''
    while True:
        text = paragraph.text
        match = regex.search(text)
        if not match:
            break

        # --- when there's a match, we need to modify run.text for each run that
        # --- contains any part of the match-string.
        runs = iter(paragraph.runs)
        start, end = match.start(), match.end()

        # --- Skip over any leading runs that do not contain the match ---
        for run in runs:
            run_len = len(run.text)
            if start < run_len:
                break
            start, end = start - run_len, end - run_len

        # --- Match starts somewhere in the current run. Replace match-str prefix
        # --- occurring in this run with entire replacement str.
        run_text = run.text
        run_len = len(run_text)
        run.text = "%s%s%s" % (run_text[:start], replace_str, run_text[end:])
        end -= run_len  # --- note this is run-len before replacement ---

        # --- Remove any suffix of match word that occurs in following runs. Note that
        # --- such a suffix will always begin at the first character of the run. Also
        # --- note a suffix can span one or more entire following runs.
        for run in runs:  # --- next and remaining runs, uses same iterator ---
            if end <= 0:
                break
            run_text = run.text
            run_len = len(run_text)
            run.text = run_text[end:]
            end -= run_len

    # --- optionally get rid of any "spanned" runs that are now empty. This
    # --- could potentially delete things like inline pictures, so use your judgement.
    # for run in paragraph.runs:
    #     if run.text == "":
    #         r = run._r
    #         r.getparent().remove(r)

    return paragraph


# def find_wrong_terms(document):
#     paragraphs_with_wrong_terms = []
#     with open(thisdoc_dir +  '/' + 'editorsguide.csv', 'r') as guide:
#         reader = csv.reader(guide)
#         for row in reader:
#             wrongterms = [row[1] for row in reader]
#             discussion = [row[2] for row in reader]
#             for index, paragraph in enumerate(document.paragraphs):
#                 for sentence in paragraph.text.split('.'):
#                     for wrongterm in wrongterms:
#                         if difflib.get_close_matches(wrongterm, sentence.split(), n=1, cutoff=0.5):
#                             paragraphs_with_wrong_terms.append((index, wrongterm, discussion))
#     return paragraphs_with_wrong_terms

if __name__ == '__main__':

    import argparse

    argparser = argparse.ArgumentParser()
    argparser.add_argument('--output_dir', default='output', help='base directory to save output files')
    argparser.add_argument('--filename', default='test/docx/lorem.docx', help='docx file to be processed')
    args = argparser.parse_args()
    output_dir = args.output_dir
    filename = args.filename

    print('args:', args)

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    main(filename, output_dir)
