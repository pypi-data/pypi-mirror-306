{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description**: Showcase CAPPr for [DPO](https://arxiv.org/abs/2305.18290) metric\n",
    "evaluation\n",
    "\n",
    "Why? CAPPr caches the prompt (the prompt is identical for the preferred and dispreferred\n",
    "responses), automatically batches (with a progress bar), and handles\n",
    "tokenization-weirdness. You don't need to create a dataset and dataloader. Just pass in\n",
    "raw strings.\n",
    "\n",
    "Note: This is almost-purely for showcasing purposes. You can't actually use this to\n",
    "train a model. To do that, I'd need to stay in torch land and not set up the model for\n",
    "inference. Currently, CAPPr converts torch tensors to numpy arrays, disables gradient\n",
    "computations, and sets the model in eval mode.\n",
    "\n",
    "**Estimated run time**: ~10 sec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizerBase,\n",
    ")\n",
    "\n",
    "from cappr import Example\n",
    "from cappr.huggingface.classify import log_probs_conditional_examples\n",
    "from cappr.utils.classify import agg_log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start w/ Sebastian Raschka's DPO function from [this\n",
    "notebook](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/04_preference-tuning-with-dpo/dpo-from-scratch.ipynb).\n",
    "\n",
    "CAPPr will supply each of the arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dpo_loss(\n",
    "    model_chosen_logprobs: torch.Tensor,\n",
    "    model_rejected_logprobs: torch.Tensor,\n",
    "    reference_chosen_logprobs: torch.Tensor,\n",
    "    reference_rejected_logprobs: torch.Tensor,\n",
    "    beta: float = 0.1,\n",
    "):\n",
    "    \"\"\"Compute the DPO loss for a batch of policy and reference model log probabilities.\n",
    "\n",
    "    Args:\n",
    "        policy_chosen_logprobs: Log probabilities of the policy model for the chosen responses. Shape: (batch_size,)\n",
    "        policy_rejected_logprobs: Log probabilities of the policy model for the rejected responses. Shape: (batch_size,)\n",
    "        reference_chosen_logprobs: Log probabilities of the reference model for the chosen responses. Shape: (batch_size,)\n",
    "        reference_rejected_logprobs: Log probabilities of the reference model for the rejected responses. Shape: (batch_size,)\n",
    "        beta: Temperature parameter for the DPO loss; typically something in the range of 0.1 to 0.5. We ignore the reference model as beta -> 0.\n",
    "        label_smoothing: conservativeness for DPO loss.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of three tensors: (loss, chosen_rewards, rejected_rewards).\n",
    "    \"\"\"\n",
    "    model_logratios = model_chosen_logprobs - model_rejected_logprobs\n",
    "    reference_logratios = reference_chosen_logprobs - reference_rejected_logprobs\n",
    "    logits = model_logratios - reference_logratios\n",
    "\n",
    "    # DPO (Eq. 7 of https://arxiv.org/pdf/2305.18290.pdf)\n",
    "    losses = -F.logsigmoid(beta * logits)\n",
    "\n",
    "    # Optional values to track progress during training\n",
    "    chosen_rewards = (model_chosen_logprobs - reference_chosen_logprobs).detach()\n",
    "    rejected_rewards = (model_rejected_logprobs - reference_rejected_logprobs).detach()\n",
    "\n",
    "    # .mean() to average over the samples in the batch\n",
    "    return losses.mean(), chosen_rewards.mean(), rejected_rewards.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the CAPPr stuff\n",
    "\n",
    "It makes sense to use `Example` objects here. It looks like this:\n",
    "\n",
    "```python\n",
    "Example(\n",
    "    prompt=prompt,\n",
    "    completions=(preferred_response, dispreferred_response),\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_log_prob(\n",
    "    examples: Example | Sequence[Example],\n",
    "    model: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    "    batch_size: int = 2,\n",
    "    batch_size_completions: int | None = None,\n",
    "):\n",
    "    log_probs = log_probs_conditional_examples(\n",
    "        examples,\n",
    "        (model, tokenizer),\n",
    "        batch_size=batch_size,\n",
    "        batch_size_completions=batch_size_completions,\n",
    "    )\n",
    "    return agg_log_probs(log_probs, func=np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpo(\n",
    "    examples: Example | Sequence[Example],\n",
    "    model: PreTrainedModel,\n",
    "    model_ref: PreTrainedModel,\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    "    beta: float = 0.1,\n",
    "    batch_size: int = 2,\n",
    "    batch_size_completions: int | None = None,\n",
    "):\n",
    "    model_logprobs = avg_log_prob(\n",
    "        examples, model, tokenizer, batch_size, batch_size_completions\n",
    "    )\n",
    "    model_ref_logprobs = avg_log_prob(\n",
    "        examples, model_ref, tokenizer, batch_size, batch_size_completions\n",
    "    )\n",
    "    model_chosen_logprobs, model_rejected_logprobs = model_logprobs.T\n",
    "    reference_chosen_logprobs, reference_rejected_logprobs = model_ref_logprobs.T\n",
    "    return compute_dpo_loss(\n",
    "        torch.from_numpy(model_chosen_logprobs),\n",
    "        torch.from_numpy(model_rejected_logprobs),\n",
    "        torch.from_numpy(reference_chosen_logprobs),\n",
    "        torch.from_numpy(reference_rejected_logprobs),\n",
    "        beta=beta,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy models and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"\n",
    "model: PreTrainedModel = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_ref = \"openai-community/gpt2-medium\"\n",
    "model_ref: PreTrainedModel = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_ref, device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124439808"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354823168"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ref.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps:0\n"
     ]
    }
   ],
   "source": [
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps:0\n"
     ]
    }
   ],
   "source": [
    "print(model_ref.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warm up\n",
    "_ = model(**tokenizer([\"warm up\"], return_tensors=\"pt\").to(model.device))\n",
    "_ = model_ref(**tokenizer([\"warm up\"], return_tensors=\"pt\").to(model.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy preference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_dataset = [\n",
    "    # Tuples of: prompt, preferred_response, dispreferred_response\n",
    "    (\"Say yes\", \"Yes\", \"No way\"),\n",
    "    (\"How useful is this demo?\", \"Not too useful lol\", \"It's amazing!\"),\n",
    "    (\"For instruct models, format the string yourself\", \"Ok fine\", \"That's stupid!\"),\n",
    "    (\"We'll just throw in raw strings\", \"k\", \"1 + 1 = 3\"),\n",
    "    (\"There are 5 examples here\", \"Correct\", \"No, there are ï£¿ number of examples\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    Example(\n",
    "        prompt=prompt,\n",
    "        completions=(preferred_response, dispreferred_response),\n",
    "    )\n",
    "    for prompt, preferred_response, dispreferred_response in preference_dataset\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c887b3fc4e84cb9a7119c12bd0ec95b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "conditional log-probs:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add63b86a2624d959e21ada0b9f81bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "conditional log-probs:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss, chosen_reward, rejected_reward = dpo(examples, model, model_ref, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.6778, dtype=torch.float64),\n",
       " tensor(-0.1116, dtype=torch.float64),\n",
       " tensor(-0.4374, dtype=torch.float64))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, chosen_reward, rejected_reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cappr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
