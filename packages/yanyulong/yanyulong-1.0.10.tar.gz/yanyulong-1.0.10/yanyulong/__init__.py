def get_yu():
    yu='''
    yu/
    '''
    print(yu)

def get_long():
    long='''
    long/
    '''
    print(long)
def lunshu_by_t(key):
    json_data=[
    {
        "title": "关联规则",
        "content": "反映了一个事物与其他事物之间相互依存性和关联性，如果两个事物或多个事物之间存在关联规则，那么其中的一个事物就可以由其他事物来预测到。关联规则必须在频繁项集中诞生且满足一定的置信度阈值\n整体步骤：1、生成频繁项集和生成规则2、找到强关联规则3、找出所有满足强关联规则的项集\n涉及的概念：项集（项的集合，包含k项的集合称为k项集）频繁项集（满足最小支持度阈值的项集）支持度（如果有两个不相交的非空集合X、Y(物品集)，N为数据记录总数，X集合和Y集合共同出现的次数占记录总数的比例：support(X->Y)=|X交Y|/N）置信度（集合X和集合Y共同出现的次数占集合X出现次数的比例，confidence(X->Y)=|X交Y|/|X|）提升度（表示置信度与Y总体发生的概率之比，lift(X->Y)=confidence(X->Y)/P(Y)）强关联规则（满足最小支持度阈值和最小置信度阈值的规则被称为强关联规则）"
    },
    {
        "title": "关联规则-Apriori算法",
        "content": "工作原理：1、首先扫描整个数据集，然后产生一个大的候选项集，计算每个候选项的次数，然后基于预先设定的最小支持度阈值，找出频繁一项集集合2、然后基于频繁一项集和原数据集找到频繁二项集3、同样的办法直到生成频繁N项集，其中已不可在生成满足最小支持度的N+1项集，也就是极大频繁项集4、根据Apriori定律1：频繁项集的子集一定是频繁项集，所以到此就找到了所有的频繁项集，然后又可以根据Apriori定律2：非频繁项集的超集一定是非频繁项集来帮助算法构建频繁项集树，加快收敛速度5、然后为每一个频繁项集创建一颗置信树（并不只为极大频繁项集创建置信树，还要为极大频繁项集的所有子集都创建置信树）6、最后可以得出数据之间的关联规则，且该关联规则满足支持度和置信度的阈值\n优点：1、使用先验原理，大大提高了频繁项集逐层产生的效率    2、简单易理解，数据集要求低\n缺点：1、每一步产生的候选项集时循环产生的组合过多，没有排除不该参与组合的元素    2、每次计算项集的支持度时，都需要将数据库中的记录全部扫描一遍，如果是一个大型数据库的话，这种扫描会大大增加计算机I/O的开销，而这种代价是伴随着数据库记录的增加呈几何级增长的，因此人们开始追求更好的算法\n应用：推荐系统：用关联算法做协同过滤，Apriori不适于非重复项集数元素较多的案例，建议分析的商品种类为10类左右。"
    },
    {
        "title": "关联规则-FP-growth算法",
        "content": "定义：该算法建立在Apriori算法概念之上，不同之处是它采用了更高级的数据结构FP-tree减少数据扫描次数，只需要扫描两次数据库，相比于Apriori减少了I/O操作，克服了Apriori算法需要多次扫描数据库的问题\n为了减少I/O次数，FP-growth引入了一些数据结构来临时存储数据，数据结构包括三部分：1、一个项头表里面记录了所有的频繁一项集出现的次数，并且按照次数降序排序  2、FP-Tree将原始数据集映射到了内存中的一棵FP树  3、节点链表，所有项头表的频繁一项集都是一个节点链表的头，它依次指向FP树中的该频繁一项集出现的位置。这样做主要是方便项头表和FP-tree之间的联系查找和更新，也好理解。\n算法流程：1、扫描数据，得到所有频繁一项集的计数，然后删除低于支持度阈值的项集，将频繁一项集放入项头表，并按支持度降序排列。2、扫描数据，将读到的原始数据剔除非频繁一项集，并将每一条再按支持度降序排列  3、读入排序后的数据集，逐条插入FP树，插入时按照排序后的顺序插入FP树中，排序靠前的是祖先节点，靠后的是子孙节点，如果有共用的祖先，则对应的共用祖先节点计数加1，插入后如果有新的节点出现，则项头表对应的节点会通过节点链表连接上新节点，直到所有数据都插入到FP树上，FP树建立完成。  4、从项头表的底部依次向上找到项头表对应的条件模式基。从条件模式基递归挖掘得到项头表项的频繁项集 5、如果不限制频繁项集的项数，则返回步骤4所有的频繁项集，否则只返回满足项数要求的频繁项集\n优点：优点：FP-growth一般快于Apriori，因为只扫描两次数据库\n缺点：缺点：1、FP-growth实现比较困难，在某些数据集上性能可能会下降   2、适用数据类型：离散型数据"
    },
    {
        "title": "随机森林",
        "content": "概述：在机器学习中，直接建立一个高性能的分类器是很困难的，但是如果构建一系列性能较差的弱分类器，再将这些弱分类器集成起来，也许就能得到一个性能较高的分类器。通常根据训练集的不同，会训练得到不同的基分类器，这时可以通过训练集的不同来构造不同的基分类器，最终把他们集成起来，形成一个组合分类器。  组合分类器是一个复合模型，由多个基分类器组合而成，基分类器通过投票，组合分类器基于投票的结果进行预测。组合分类器往往比基分类器更加准确，常用的组合方法：装袋、提升、随机森林\n构建分类器的过程一般有两种集成方法1、利用训练集的不同子集训练得到不同的基分类器2、利用同一个训练集的不同属性子集构建不同的基分类器\n随机森林是bagging的一个扩展变体，它是以决策树为基学习器构建bagging的基础上，进一步在决策树的训练过程中引入了随机属性选择。具体步骤：1、从样本集中用自助法选出n个样本组成子集  2、从所有属性n中随机选择K个属性，选择最佳分裂属性(ID3、C4.5、CART)作为节点建立决策树，每棵树都最大程度生长，不进行剪枝 3、重复以上两步m次，建立m棵决策树  4、这m个决策树形成随机森林，在分类问题中通过投票决定输出属于哪一类，在回归问题中输出所有决策树输出的平均值\n优点：1、准确率高。2、各个初级学习器可以并行计算。3、能处理高维持征的数据样本，不需要降维。4、能够评估各个特征在分类问题中的重要性。5、因为采用随机选择特征，对部分特征缺失不数感。6、由于采用有放回抽样，训练出来的模型方差小,泛化能力强\n缺点：1、取值较多的特征会对RF的决策树产生更大影响有可能影响模型的效果。2、bagging保证了预测准确率，但损失了解释率。3、在某些噪音比较大的特征上，RF还是容易陷入过拟合"
    },
    {
        "title": "Bagging",
        "content": "对训练集进行有放回的抽取训练样例，从而为每一个基学习器都构建一个与训练集相当大小但各不相同的训练集，从而训练出不同的基学习器。Bagging是并行计算来训练每个弱学习器，且每个弱学习器的权重相等，且训练集权重也相等\n算法流程:1、从大小为N的原始数据集中独立随机地抽取m个数据(m<=n),形成一个自助数据集 2、重复第一步K次，产生K个独立的自助数据集。3、利用K个数据集训练出K个最优模型(K次可以并行进行) 4、分类问题中的分警结果根据K个模型的结果投票决定，回归问题:对K个模型的值求平均得到结果\n特点：1、通过降低基学习器的方差改善了泛化误差。2、由于每一个样本被选中的概率相同，所以装袋并不侧重于训练数据集中的任何实例，因为对于噪声数据，装袋不太受过分拟合的影响。3、由于是多个决策树组成，所以装袋提升了准确率的同时损失了解释性，哪个变是起到重要作用未知"
    },
    {
        "title": "boosting",
        "content": "主要作用和bagging类似，都是将昔干基分类器整合为一个分类器的方法，boosting是个顺序的过程，每个后续模里都会尝试纠正先前模型的错误，后续的模型依赖之前的模型\n算法步骤:1、首先给每一个训练样例赋予相同的权重 2、然后训练第一个基分类器并用它对训练集进行测试，对于那些分类错误的测试样本提高权重(实际算法中是降低分类正确的样本的权重)3、随后用调整后的带权训练集训练第二个基分类器 4、最后重复这个过程直到最后得到一个足够好的学习器\n提升是一个选代的过程，用于自适应地改变训练样本的分布，使得基分类警聚焦在那些很难分的样本上，不像bagging，提升给每一个训练样本赋予一个权值，而且可以在每一轮提升过程结束时自动地调整权值"
    },
    {
        "title": "adaboost",
        "content": "自适应在于前一个基分类器分错的样本会得到加权，加权后的全体样本再次被用来训练下一个基分类器，采用选代的思想，继承boosting，每次选代只训练一个弱学习器，训练好的弱学习器将参与下一次选代。\n步骤:1、初始化训练数据的权值分布，如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值:1/N 2、通过训练集训练得到弱分类器，将分类正确的样本降低权重，同时提升分类错误样本的权重，将权重更新后的训练集用于训练下一个分类器，如此选代下去 3、将这些得到的弱分类器泪合成强分类器，加大那些分类误差率小的分类器的权重，使其在最终的分类函数中起较大的决定作用，降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的作用，总之就是误差率低的弱分类器在最终分类器中权重较大，起较大的决定作用，否则较小。\n优点：1、很好地利用了弱分类器进行级联。2、提供的是框架，可以使用各种方法构建弱分类器，很灵活。3、不容易发生过拟合。4、具有很高的精度。5、相对于bagging和随机森林，adaboost充分考虑了每个分类器的权重\n缺点：对异常样本敏感，异常样本可能会在迭代过程中获得较高的权重值，最终影响模型效果。"
    },
    {
        "title": "XGboost",
        "content": "（1）xgboost本质上还是GBDT,但是把速和效率做到了极致，不同于传统的GBDT，只利用了一阶导数信息，xGboost对损失函数做了二阶求导泰勒展开，并在目标函数之外加入了正则项整体求最优解，用以权衡目标函数的下降和模型的复杂程度，避免过拟合，传统GBDTI以CART作为基学习器，XGboost还支持线性分类器，也就是xgboost相当于带L1和L2正则化项的逻辑回归(分类问题)或者线性回归(回归问题)\n（2）表勒展开的一次项和二次项系数不依赖于损失函数，所以xgboost支持自定义损失函数\n（3）XGboost在损失区数里加入正则项，用于控制模型复杂度，正则项降低了模型的复杂度，使学习出来的模型更加简单，防止过拟合，这点优于GBDT\n（4）xGboost还借鉴了随机森林算法，支持列抽样不仅能够降低过拟合还能减少计算，优于GBDT\n（5）Xgboost进行完一次选代后，会将叶子节点的权重乘上一个缩减系数，相当于学习速率\n(XxGboost中的eta)，主要是为了削弱每棵树的影响，让后面有更大的学习空间，实际应用中一般把eta设置的小一点(小学习率可使得后面的学习更加仔细)，选代次数设置的大一点\n（6）应用场景：预测用户行为喜好，商品推荐"
    },
    {
        "title": "梯度提升树（GBDT）",
        "content": "（1）梯度提升决策树，在GBDT中基学习器都是分类回归树，也就是CART，且使用的都是CART中回树。\n（2）对于初级学习器的结果来说一般都是低方差，高混差，因此GBDT的训练过程就是通过降低差来不断提高精度\n（3）GBDT的核心是在于累加所有树的结果作为最终结果，例如对年龄的累加，10岁加5岁，分类树的结果显然是累加不了的，所以GBDT中都是回归树，不是分类树，尽管GBDT调整后也可用于分类，但是不代表GBDT的树是分类树\n（4）GBDT的核心在于每一颗树学习的是之前所有树的结论和残差，这个残差就是一个加预测值后能得到真实值的累加是\n（5）主要使用到的损失函数有:0-1损失函数，对数损失区数，平方损失函数等，目标是:希望损失函数能够不断减小，还有希望损失函数能够尽快减小。\n（6）应用场景：常用于大数据挖掘竟赛；可用于几乎所有的回归问题，也可用于二分类但不适合多分类问题；广告推荐；排序问题"
    },
    {
        "title": "ELT，ETL",
        "content": "ELT，概念：即extract  load  transform(数据抽取，加载，转换)\n抽取：（1全量抽取）当数据源中有新数据加入或发生数据更新操作时，系统不会发出提醒，此时采用全量抽取，类似于数据迁移或复制，将数据源中的数据原封不动的从数据库中抽取出来，转换成自己的ETL工具可以识别的格式，一般在系统初始化时使用，全量一次后，就要每天采用增量抽取（2增量抽取）当数据源中有新数据加入或数据更新操作时，系统不会发出提醒，但可以识别出更新的数据，此时采用增量抽取，增量抽取只抽取自上次抽取以来，数据库中新增或修改的数据，在ETL中增量抽取使用更广泛；适合场景：1源数据数据量小2源数据不易发生变化3源数据规律性变化4目标数据量巨大（3更新提醒）当数据源中有新数据加入或数据更新操作时，系统会发出提醒，是最简单的一种抽取方式\n转换：抽取完数据后，要根据具体业务对数据进行转换操作，数据转换一般包括清洗和转换两部分，清洗掉数据集中重复、不完整的数据以及错误的数据\n加载：是将已按照业务需求清洗、转换、整理后的数据加载到各类数据仓库或数据库中，进而进行智能商业分析、数据挖掘等。（1全量加载）全表删除后在进行数据加载（2增量加载）目标表仅更新源表中变化的数据\n优势：1、相对于传统的ETL来说，ELT数据处理管道中无需单独的转换引擎，数据转换和数据消耗在同个地方。\n2、减少了数据预处理的时间开销，在实际应用中，下游各应用的目的各不相同，同一份数据可能有不同的应用，进而做不同的转换操作，面对这种情况，ETL需要多次对数据进行抽取、转换、加载，而ELT只需要进行一次数据的抽取加载，多次转换，从而实现一份数据的多次应用，大大降低了时间的开销。\n案例：电商用户收集用户信息进行商业分析，有两个核心目标：1、提高用户转化率  2、提高商品的精准营销效率  这两个项目都需要从同一数据源抽取数据，但是项目目标不同，所以需要的细节数据也就不同。\n对于ETL来说，相关人员需要从数据源抽取两次数据（一个项目各需要抽取一次），然后每个项目各需要进行一次ETL的完整流程；对于ELT来说，相关人员只需从数据源抽取一次数据，然后将数据加载到目的地(Hive、Hbase中)，最终转换成我们项目所需要的细节数据即可，无需重复的抽取和加载过程。"
    },
    {
        "title": "正则化。Lasso回归和岭回归",
        "content": "正则化：根据奥卡姆剃刀原理，在所有能解释数据的模型中，越简单的模型越靠谱。但是在实际问题中，为了拟合复杂的数据，不得不采用更复杂的模型，使用更复杂的模型通常会产生过拟合，而正则化就是防止过拟合的工具之一，通过限制参数过大或者过多来避免模型的复杂\nL1（Lasso Regression）、L2正则化（Ridge Regression）的目的都是为了防止过拟合，两者差别在于：岭回归中的L2正则项能够将一些特征变成很小的值，而Lasso回归中的L1正则项得到的特征是稀疏的。Lasso回归会趋向于减少特征的数量，相当于删除特征，类似于降维，而岭回归会把一些特征的权重调小，这些特征都是接近于0的，因此Lasso回归在特征选择的时候非常有用，而岭回归就是一种规则化而已。\n在所有特征中，如果只有少数特征起重要作用的话，选择Lasso回归比较合适，它能自动选择特征。而大部分特征能起到作用而且作用比较平均的话，选择岭回归更合适"
    },
    {
        "title": "spark mllib，rdd",
        "content": "数据对象：1Dataframe、2RDD、3Dataset(具有RDD和dataframe的优点，同时避免他们的缺点)。\n相同点：1、都有惰性机制，在进行转换操作时不会立即执行，只有遇到Active操作时才会执行2、都是spark的核心，只是弹性分布式数据集的不同体现3、都具有分区的概念4、都有相通的算子，比如map、filter等5、都会根据spark的内存情况自动做缓存计算即使数据量很大也不用担心内存的溢出，不用担心OOM-当请求的内存过大时，JVM无法满足而自杀\n不同点：1、RDD以person作为类型参数，但spark并不知道person的内部结构，而Dataframe提供了详细的结构信息，使得sparkSQL可以清楚的知道数据中包含了哪些列，以及列的名称和类型Dataframe提供了数据的结构信息，即schema2、RDD是分布式JAVA的对象集合，Dataframe是row对象的集合3、Dataframe除了提供比RDD更加丰富的算子以外，更重要的是提升执行效率、减少数据读取和执行计划的优化，比如filter下推、裁剪等。\nRDD操作：（优点：类型安全，面向对象；缺点：序列化和反序列化开销大，GC垃圾回收机制的性能开销，频繁的创建和销毁对象，势必增加GC）基本操作主要为Transformation算子（该算子是通过转换从一个或多个RDD生成新的RDD，该操作是惰性的，只有调用action算子时才发起job,典型算子如map、filter、flatMap,distinct等）和Action算子（当代码调用该类型算子的时候，立即启动job，典型算子包括：count、saveasTextfile、takeordered等）\nDataframe操作：（优点：自带schema信息，降低序列化和反序列化的开销；缺点：不是面向对象的，编译期不安全）"
    },
    {
        "title": "决策树",
        "content": "定义：决策树是一种分类算法，它通过对有标签的数据进行学习，学习数据的特征，得到一个模型，什么样的数据就打上什么样的标签\n算法思想：选择属性特征对训练集进行分类，使得各个子数据集有更好的分类，其中的关键点就是寻找分裂规则，因为它们决定了给定节点上的元组如何分裂\n决策树生成步骤：1、根据特征度是选择，从上至下递归地生成子节点，直到数据集不可分则停止决策树生长。2、剪枝:决策树容易过拟合，需要剪枝来缩小树的结构和规模(包括预剪枝和后萝枝)。3、先剪枝:通过提前停止树的构建而对树萝枝。4、后剪枝:由完全生长的树减去子树\n特征度量选择：ID3采用信息增益大的特征作为分裂属性；C4.5采用信息增益率大的属性分裂，同时避免了ID3信息增益偏向取值较多的属性的缺点，但是其实是偏向于取值较少的特征；CART使用基尼指数克服了C4.5计算复杂度的缺点，偏向取值较多的属性\n场景：ID3和C4.5都只能处理分类问题，CART可以用于分类和回归； ID3与C4.5是多叉树，速度慢，CART是二叉树，计算速度快；ID3没有剪枝策略，C4.5有预剪枝和后剪枝，CART采用CCP代价复杂度后剪枝；\n预剪枝：提前设置好树的深度；后剪枝：用完全生长的树减去子树，在测试集上定义损失函数C，目标是通过剪枝使得在测试集上的C值下降，就是说通过剪枝使在测试集上误差率降低\n后剪枝步骤：\n1.自底向上的遍历每一个非叶节点（除了根节点），将当前的非叶节点从树中减去，其下所有的叶节点合并成一个节点，代替原来被剪掉的节点。 \n2. 计算剪去节点前后的损失函数，如果剪去节点之后损失函数变小了，则说明该节点是可以剪去的，并将其剪去；如果发现损失函数并没有减少，说明该节点不可剪去，则将树还原成未剪去之前的状态。 \n3. 重复上述过程，直到所有的非叶节点（除了根节点）都被尝试了。"
    },
    {
        "title": "决策树-id3",
        "content": "核心思想：选择信息增益最大的属性进行分裂，求信息增益时要先求出信息熵和条件熵，信息增益=信息熵-条件熵 ，信息增益：信息的不确定性减少的程度      信息熵：信息的不纯度，信息熵越大，数据分布越混乱，也就是概率分布越均匀，信息熵也越大；\nID3算法建立在奥卡姆剃刀的思想上，越是小型的决策树越优于大的决策树；算法流程：1、初始化属性集合和数据集合  2、计算数据集合的信息熵，和所有属性的条件熵，然后得出信息增益，选择信息增益最大的属性作为当前决策树的分裂节点  3、更新数据集合合属性集合，也就是删除掉上一步使用的属性，并按照属性值来划分不同分支的数据集合   4、依次对每种取值情况下的子集重复第2步  5、若子集只包含单一属性，则为分支叶子节点，根据其属性值标记  6、完成所有属性集合的划分；\nID3优点：1、概念简单，计算复杂度不高，可解释强，易于理解\n2、数据的准备工作简单，能够同时处理数据型和常规型居性\n3、对中间值的缺失不敏感，比较适合处理有缺失属性值的样本，能够处理不相关特征\n4、可以对很多属性的数据集构造决策树，可扩展性强，可用于不熟悉的数据集，并从中提取一些属性规则\nID3缺点：1、没有剪枝策略，可能会产生过度匹配问题，决策树过深，容易导致过拟合，泛化能力差，因此希要简直\n2、信息增益会对取值较多的属性有所好，也就是作为分类属性\n应用场景：适用于特征取值字段不多的数据集，因为信息增益会对取值较多的属性偏好，更容易选择这种属性作为分类属性"
    },
    {
        "title": "决策树-C4.5算法",
        "content": "ID3算法容易选择那些取值较多的特征来划分，因为根据这些属性划分出的数据纯度较高，比如身份证的例子。而且ID3算法属性只能是离散的，当然属性值也可以是连续的数值型，但是需要对这些数据进行数据预处理，变为离散型的才可以用ID3算法，所以C4.5继承了ID3的优点，改进了它的缺点(信息增益会对取值较多的属性有所偏好，也就是作为分类属性)，并在此基础上做了改进的算法，能够处理属性是连续型的\n信息增益率=信息增益/属性a的一个固有值，当属性a\n改进优化的点：1、用信息增益率代替信息增益来选择特征，克眼了用信息增益选择特征时偏向取值较多的属性的不足\n2、能够完成对连续型数值属性的离散化处理\n3、能处理居性值缺失的情况\n4、在决策树构造完成之后剪枝\n"
    },
    {
        "title": "决策树-CART算法",
        "content": "分类回归树，在ID3的基础上，进行优化的决策树，CART既可以是分类树，也可以是回归树，当作为分类树时，采用基尼指数作为节点的分裂依据；当作为回归树时，采用最小方差作为节点的分裂依据。 CART只能用来建立二叉树\nCART在分类问题中采用基尼值来衡量节点的纯度，节点越不纯，基尼值越大，以二分类为例，如果节点的所有数据只有一个类别，则基尼值为0；此外，CART算法采用基于CCP(代价复杂度)的后剪枝方法"
    },
    {
        "title": "算法评估指标",
        "content": "在机器学习算法中，我们可以将算法分为分类算法、回归算法和聚类算法，当我们建立完模型以后，需要用一定的评估指标来判断模型的优劣\n回归算法评估指标（预测连续型数值问题）：最常用的评估指标是 MSE均方误差（mean squared error），均方误差是描述估计量与被估计量之间差异程度的一种评估指标，也就是预测值与实际值误差平方和的均值\n聚类算法评估：聚类的评价方式从大方向上分为两类，一种是分析外部信息，一种是分析内部信息。外部信息就是可看得见的直观信息，比如聚类完成后的类别号。内部信息就是聚类结束后通过一些模型生成这个聚类的相关信息，比如熵值、纯度这种数学评价指标，常见的聚类评估指标有：互信息法、兰德系数、轮毂系数等。\n轮毂系数，适用于实际类别信息未知的情况。对于单个样本来说，设a为其与类别内各样本的平均距离，设b为其与距离最近的类别内的样本的平均距离，轮毂系数公式：(b-a)/max(a,b)\n分类算法评估指标（预测离散型数值问题）：1、错误率，指预测错误的样本数占总样本数的比例，又被叫做汉明损失。表达式：(FP+FR)/P+R。2、精度，指预测正确的样本数占总样本数的比例，又叫做预测准确率。表达式：(TP+TR)/P+R。3、查准率，又叫做精确率，指在预测为正的样本数中真正正例的比例，表示预测是否分类为1中实际为0的误报率，表达式：TP/(TP+FP)。4、查全率，又叫做召回率，指在所有实际正例样本中预测为正的样本数的比例，表示漏掉了一该被分类为1的，却被分为0的漏报成分。5、f1-score，在理想状态中，模型的查准率和查全率越高越好，但是在现实状况下，查准率和查全率会出现一个升高，一个降低的情况。所以就需要一个能够综合考虑查准率和查全率的评估指标，因此引入F值，F值表达式为：(a^2+1)pr/a^2(p+r)，a为权重系数，当a=1时，F值便是F1值，代表查准率和查全率权重相等，是最常用的一种评估指标。表达式为：f1=2pr/p+r。6、PR曲线，是描述查准率和查全率变化的曲线，以查准率和查全率为纵、横坐标轴，根据学习器的预测结果对测试样本进行排序，将最可能是正例的样本排在前面，最不可能是正例的样本排在后面，按照此顺序，依次将每个样本当作正例进行预测，每次计算P值和R值。7、ROC曲线，与PR曲线类似，都是按照排序的顺序将样本当作正例进行预测，不同的是ROC曲线引进TPR、FPR的概念，FPR是假正例率，TPR是真正例率，ROC曲线以TPR(真正例率)为横轴、FPR(假正例率)为纵轴。ROC更加偏重于测试样本评估值的排序好坏。8、AUC面积，进行模型性能比较时，如果模型A的ROC曲线被模型B的ROC曲线完全包住，那么认为B模型的性能更好。若A和B的ROC曲线有交叉的地方，则比较两个模型ROC曲线与坐标轴围成的面积，AUC被定义为ROC曲线下的面积，面积越大,AUC值越大，模型分类的质量就越好。AUC为1时，所有正例排在负例前面，AUC为0时，所有负例排在正例前面\n备注1：查准率是宁愿漏掉，不可错杀。应用场景：一般应用于识别垃圾邮件的场景中。因为我们不希望很多的正常邮件被误杀，这样会造成严重的困扰。因此在这种场景下，查准率是一个很重要的指标\n2：查全率是宁愿错杀，不可漏掉。应用场景：一般用于金融风控领域，我们希望系统能够筛选出所有风险的行为或用户，然后进行人工鉴别，如果漏掉一个可能会造成灾难性后果。\n3：对于类别不平衡问题，ROC曲线的表现会比较稳定（不会受不均衡数据的影响），但如果我们希望看出模型在正类上的表现效果，还是用PR曲线更好\n4：ROC曲线由于兼顾正例与负例，适用于评估分类器的整体性能（通常是计算AUC，表示模型的排序性能）；PR曲线则完全聚焦于正例，因此如果我们主要关心的是正例，那么用PR曲线比较好"
    },
    {
        "title": "最优化方法-无约束-梯度下降",
        "content": "除了目标函数以外，对参与优化的各变量没有其他函数或变量约束，称之为无约束最优化问题；目标函数f(x)，当对其进行最小化时，也把它称作为代价函数、损失函数或误差函数\n求解方法1直接法:通常用于当目标函数表达式十分复杂或者写不出具体表达式时。通过数值计算，经过一系列迭代过程产生点列，在其中搜索最优点\n求解方法2解析法:根据无约束最优化问题的目标函数的解析式给出一种最优解的方法，主要有梯度下降、牛顿法、拟牛顿法、共轭梯度法和共轭方向法等\n概念：1、方向导数:函数沿任意方向的变化率，需要求得某一点在某一方向的导数即方向导数。2、梯度方向:函数在变量空间中的某一点沿着哪个方向有最大的变化率?最大方向导数方向，即梯度方向。3、梯度:函数在某一点的梯度是一个矢量，具有大小和方向。它的方向与取得最大方向导数的方向一致，而它的模为方向导数的最大值。4、正梯度向量指向上坡，负梯度向量指向下坡。我们在负梯度方向上移动可以减小f(x)，被称为最速下降法或梯度下降法\n一、批量梯度下降：每更新一次权重需要对所有的数据样本点进行遍历。在最小化损失函数的过程中，需要不断反复的更新权重使得误差函数减小；特点：每一次的参数更新都用到了所有的训练数据，因此批量梯度下降法会非常耗时，样本数据量越大，训练速度也变得越慢\n二、随机梯度下降：为了解决批量梯度下降训练速度过慢的问题。它是利用随机选取每个样本的损失函数对sita求偏导得到对应的梯度来更新sita；因为随机梯度下降是随机选择一个样本进行迭代更新一次，所以伴随的一个问题是噪音比批量梯度下降的多，使得随机梯度下降并不是每次都向着整体优化的方向\n三、小批量梯度下降：为了解决前两者的缺点，使得算法的训练过程较快，而且也要保证最终参数训练的准确率。它是通过增加每次送代个数来实现的；每次在一批数据上优化参数并不会比单个数据慢太多，但是每次使用一批数据可以大大减小收敛所需的迭代次数，同时可以使得收敛的结果更加接近批量梯度下降的效果"
    },
    {
        "title": "最优化方法-有约束",
        "content": "等式约束最优化：拉格朗日乘子法是解决等式约束最优化的问题的最常用方法，基本思想就是通过引入拉格朗日乘子来将含有n个变量和k个约束条件的约束优化问题转化为含有（n+k）个变量的无约束优化问题\n不等式约束最优化：大部分实际问题的约束都是不超过多少时间，不超过多少人力等等，因此对拉格朗日乘子法进行了扩展，增加了KKT条件，求解不等式约束的优化问题；使用拉格朗日函数对目标函数进行了处理，生成了一个新的目标函数。通过一些条件可以求出最优值的必要条件，这个条件就是KKT条件。经过拉格朗日函数处理之后的新目标函数，保证原目标函数与限制条件有交点，也就是必须要有解"
    },
    {
        "title": "异常值",
        "content": "定义：异常值是偏离整体样本的观察值，也是偏离正常范围的点\n影响：异常值会影响模型的精度，降低模型的准确性。增加了整体数据方差；异常值是随机分布的，可能会改变数据集的正态分布。增加因此异常值处理是数据预处理中重要的一步，异常检测场景：入侵检测、欺诈检测、安全监测等\n出现原因：1、数据输入错误，相关人员故意或者无意导致数据异常，比如客户年收入13万美元，数据登记为130万美元。2、数据测量，实验误差，比如测量仪器不精准导致数据异常。3、数据处理错误，比如ETL操作不当，发送数据异常。4、抽样错误，数据采集时包含了错误数据或无关数据。5、自然异常值，非人为因素导致数据异常，比如今年某月份的降水量远超前几年同月份降水量。\n异常值检测方法：1、散点图：将数据用散点图可视化出来，可以观测到异常值。2、基于分类模型的异常检测：根据现有数据建立模型，然后对新数据进行判断从而确定是否偏离，偏离则为异常值，比如贝叶斯模型，SVM模型等。3、3sita原则：若数据集服从期望为u，方差为σ^2的正态分布，异常值被定义为其值与平均值的偏差超过三倍标准差的值。4、箱型图分析：箱型图分为上界，下界，上四分位数，下四分位数，以及离群点   四分位数就是将所有数值按从小到大排列并分成四等份，处于三个分割点位置的数值就是四分位数。（上界:上限是非异常范围内的最大值；下界:下限是非异常范围内的最小值；上四分位数:数值排序后处于75%位置上的值；下四分位数:数值排序后处于25%位置上的值）\n异常值处理办法：1、删除异常值 -适用于异常值较少的情况。2、将异常值视为缺失值，按照缺失值的处理方法处理异常值。3、估算异常值，均值、中位数、众数填充异常值"
    },
    {
        "title": "训练集，测试集，验证集合",
        "content": "在训练过程中使用验证数据集来评估模型并更新模型超参数，训练结束后使用测试数据集评估训练好的模型的性能\n训练集：是用来构建机器学习模型，用于模型拟合的数据样本；测试集：用来评估训练好的模型的性能；验证集：辅助构建模型，用于构建过程中评估模型，为模型提供无偏估计，进而调整模型超参数和用于对模型的能力进行初步评估。通常用来在模型迭代训练时，用以验证当前模型的泛化能力（准确率，召回率等），以决定是否停止继续训练\n划分方法：（1留出法）直接将数据集划分为互斥的集合，一个用作训练集，一个用作测试集，且满足训练集U测试集=全集；训练集交测试集=空集  常见的划分为2/3-4/5的样本用作模型训练，剩下的用作测试。通常选择70%的数据作为训练集，30%的数据作为测试集；通常单次使用留出法得到的结果不够稳定可靠，一般采取分层抽样或者若干次随机划分作为留出法的评估结果\n（2自助法）给定包含M个样本的数据集，每次随机从中抽取一个样本，放入新的数据集中，然后从原始样本中有放回的抽取M次，就可以得到包含M个样本的数据集，平均情况下会有63.2%的原始样本出现在数据集中，而剩下的36.8%的原始样本不出现在数据集中，也称.632自助法\n（3交叉验证）将数据集划分为K个大小相同的互斥子集，同样保持数据分布的一致性，即采用分层抽样的方法获得这些子集。目的：在实际训练中，模型的结果通常对训练数据好，但是对训练数据以外的数据拟合程度较差，交叉验证的目的就是用作评价模型的泛化能力，从而进行模型选择\nK-折交叉验证：每次用K-1个子集的并集作为训练集，余下的那个子集作为测试集，这样就会产生K种数据集划分的情况，从而可以进行K次训练和测试，最终返回K次测试结果的均值\nK折交叉验证的K最常用的取值是10，此时为10折交叉验证，常用的还有5、20等\n股用于模型调优，找到使得模型泛化性能最优的超参数，在全部训练集上重新训练模型，并使用独立测试集对模型性能做出最终评价\n如果训练集相对较小，则增大K值：增大K的话，在每次选代过程中将会有更多的数据参与模型的训练，能够得到最小偏差，同时算法时间也会变长，且不同训练集间高度相似，会导致结果方差较高\n如果训练集相对较大，则减小K值：减小K的话，降低模型在数据集上重复拟合的时间和成本，在平均性能的基础上获得模型的准确评估"
    },
    {
        "title": "特征选择",
        "content": "为什么要进行特征选择：背景：现实中大数据挖掘任务，往往属性特征过多，而一个普遍存在的事实是，大数据集带来的关键信息之聚集在部分或少数特征上，因此需要从中选择出重要的特征使得后续建模过程只在一部分特征上建立，减少维数灾难出现的可能，同时去除不相关的特征，留下关键因素，降低学习的任务难度，更容易挖掘数据本身带有的规律，同时在特征选择的过程中，会对数据的特征有更充分的理解\n如何进行特征选择：当数据预处理完成后，需要选择有意义的特征进行模型训练，通常从三方面考虑：1、特征是否发散:如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本没有差异，这个特征对样本的区分作用不明显，区分度不高。2、待征之间的相关性:特征与待征之间的线性相关性，去除相关性高的特征。3、特征与目标之间的相关性:与目标相关性高的特征，应当优先选择\n三种常见特征选择方法对比：1、过去由于和特定的学习器无关，计算开销小，泛化能力强于后两种特征选择方法，因此，在实际应用中由于数据集很大，特征维度高，过港式特征选择应用的更广泛些，但它是通过一些统计指标对特征进行排字来选择，通常不能发现冗余。2、从模型性能的角度出发，包装法的性能要优于过考去，但时间开销较大。3、嵌入法中的L正则化方法对于特征理解和特征选择来说是非常强大的工具，它能够生成稀疏的模型，对于选择待征子集来说非常有用。4、嵌入法中的随机森林方法是当前比较主流的方法之一，它易于使用，一般不需要其他特征工程操作、调参等繁琐的步亲，有直接的工具包都提供平均不纯堂下降方法，它的两个主要问题：一是重要的特征有可能得分很低，二是这种方法对特征变量类别多的特征有利。"
    },
    {
        "title": "特征选择-Filter过滤法",
        "content": "按照特征发散性或者相关性对各个特征评分，设定阈值或待选择阈值的个数，选择特征；总的缺点是：若特征之间具有强关联，且非线性时，Filter方法不能避免选择的最优特征组合冗余。   Filter总结：利用不同的打分规则，对每个特征进行打分，相当于给每个特征赋予权重，按权重排序，对不达标的特征进行过滤\n1、方差选择法：方差越大的特征，对于分析目标影响越大，就越有用；如果方差较小，比如小于一，那么这个特征可能对算法的作用就比较小；如果某个特征方差为0，即所有的样本在这个特征上的取值都是一样的，那么对模型训练没有任何作用，可以直接舍弃（特征的方差越大越好）    ；实现方法：设定一个方差的阈值，当方差小于这个阈值的特征就会被删除；适用场景：只适用于连续变量。\n2、相关系数法（皮尔逊相关系数）：该方法衡量的是变量之间的线性相关性，两个变量之间的皮尔逊相关系数定义为两个变量之间的协方差与标准差的商，结果的取值区间为【-1，1】，-1表示完全的负相关，+1表示完全正相关，0表示没有线性相关    实现方法：R为相关性，P为显著性，首先看P值，P值用于判断R值，即相关系数有没有统计学意义，判断标准一般为0.05，当P值>0.05时，则相关性系数没有统计学意义，此时无论R值大小，都表明两者之间没有相关性；当P值<0.05，则表明两者之间有相关性，此时再看R值，R值越大相关性越大，正数则正相关，负数就是负相关   ；适用场景：适用于特征类型均为数值特征的情况；缺陷：只对线性关系敏感，如果特征与响应变量的关系是非线性的，即便两个变量具有一 一对应的关系，相关系数也可能会接近0。建议最好把数据可视化出来，以免得出错误的结论。\n3、卡方检验(𝜒^2检验)：它可以检验某个特征分布和输出值分布之间的相关性，𝝌^𝟐值描述了自变量与因变量之间的相关程度，𝜒^2值衡量实际值与理论值的差异程度，𝜒^2值越大，相关程度也就越大   实现方法：1、计算无关性假设(随机抽取一条实际值计算)   2、根据无关性假设生成新的理论值四格表   3、根据计算公式算出𝜒^2   4、计算该相依表的自由度，查询卡方分布的临界值表来判断𝜒^2值是否合理(需要用100%-卡方分布临界表的值才能得出相关性)\n4、互信息法：互信息表示两个变量是否有关系以及关系的强弱。互信息可以理解为，X引入导致Y的熵减小的量，从信息熵的角度分析特征和输出值之间的关系评分；互信息值越大，说明该特征和输出值之间的相关性越大，越需要保留；缺陷：它不属于度量方式，也没有办法归一化，在不同数据集上的结果无法做比较    2、对于连续变量通常需要先离散化，而互信息的结果对离散化的方式敏感\n"
    },
    {
        "title": "特征选择-wrapper包装法",
        "content": "根据目标函数(通常是预测效果评分)，每次选择若干特征，或者排除若干特征\n常用技术 -- 递归特征消除法RFE；\n使用一个基模型来进行多轮训练，每轮训练后，移除若干权值系数的特征，在基于新的特征集进行下一轮训练。  步骤：1、指定一个有N个特征的数据集 2、选择一个算法模型  3、指定保留特征的数量K(K<N)  4、第一轮对所有特征进行训练，算法会根据基模型的目标函数给出每个特征的评分或排名，将最小得分或排名的特征移除，这时候特征减少为N-1，对其进行第二轮训练，持续迭代，直到特征保留为K,这K个特征就是选择的特征\nRFE的稳定性很大程度上取决于在迭代时底层所采用的模型。例如，假如RFE采用的普通的回归，没有经过正则化的回归是不稳定的，那么RFE就是不稳定的；假如采用的是Ridge，而用Ridge正则化的回归是稳定的，那么RFE就是稳定的。"
    },
    {
        "title": "特征选择-Embedded嵌入法",
        "content": "先使用某些机器学习的算法或模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征\n（1基于L1正则化方法）：1、L1正则化将回归系数的L范数作为惩罚项加到损失函数上，由于正则化非0，这就使得那些弱的待征所对应的系数变为0，因此比L1正则化往往会使学习到的模型很保统(系数w经常为0)这个特性使得L1正则化成为一种很好的特征选择的方法。2、L1正则化像非正则化线性模型一样也是不稳走的，如果待征集合中具有相关联的持征，当数据发生轻微变化时，也有可能导致很大的差异。3、L1正则化能够生成稀疏的模型。4、L1正则化可以产生稀疏权值短阵，即产生一个稀疏模型，可以用于特征选择，也可以防止过拟合\n数据稀疏性说明:L1正则化本来就是为了降低过拟合风险，但是L1正则化的结果往往会得到稀疏数据，即L1方法选择后的数据拥有更多0分量，所以被当作特征选择的强大方法。稀疏数据对后续建模的好处:数据集表示的矩阵中有很多列与当前任务无关，通过特征选择去除这些列，如果数据備疏性比较突出，意味着去除了较多的无关列，模型训练过程实际上可以在较小的列上进行，降低学习任务的难度，计算和存储开销小，\n（2基于树模型方法）：基于树的预测模型能够用来计算特征的重要程度，因此能用来去除不相关的特征，随机森林具有准确率高，稳定性强、易于使用的优点，是目前最流行的机器学习算法之一，随机森林提供的特征选择方法：平均不纯度减少和平均精度下降。（一）、平均不纯度减少：1、随机森林由多个决策树构成，决策树的每一个节点都是关于某个特征的条件，目的时将数据集按照不同的取值一分为二。对于一个决策树森林来说，可以算出每个特征平均减少了多少不纯度，并把它平均减少的不纯度作为特征选择的值  2、若特征之间存在关联，一旦某个特征被选择后，其他特征的重要度就会急剧下降，而不纯度已经被选中的那个特征降下来，其他特征就很难在降低那么多不纯度。在理解数据时，容易错误的认为先被选中的特征是很重要的，而其余的特征是不重要的，但实际上这些特征对响应变量的作用可能非常接近；。（二）、平均精度下降：该方法直接度量每个特征对模型的精确率的影响，基本思路是打乱每个特征的特征值顺序，并且度量顺序变动对模型精确率的影响，因为对于不重要的变量，打乱其顺序对模型的准确率影响不会太大，对于重要的变量，打乱顺序就会降低模型的准确率；在特征选择上，需要注意：尽管在所有特征上进行了训练得到模型，然后才得到每个特征的重要性测试，这并不等于筛选掉某个或某几个重要特征后模型的性能就一定会下降很多，因为即便删掉某个特征后，其关联特征一样可以发挥作用，让模型性能不变"
    },
    {
        "title": "特征处理-特征缩放(数值归一化)",
        "content": "特征缩放可以提高模型的精度和模型的收敛速度，在实际业务中，当数据的量纲不同、数量级差距大时，会影响最终的模型，因此需要用到特征缩放\n1、标准化（常用）：概念:将训练集中的某一列特征缩放到均值为0，方差为1的状态；特点:标准化对数据进行规范化，去除数据的单位限制，将数据转换为无量纲的纯数值，便于不用单位的数据进行比较和加权，同时不改变数据的原始分布状态，标准化要求原始数据近似满足高斯分布，数据越接近高斯分布，标准化效果越佳；适用场景:数据存在异常值和较多的噪音值\n2、最小值-最大值归一化：概念:将训练集中某一列特征数值缩放到-1到1，或0到1之间；特点:受训练集中最大值和最小值影响大，存在数据集中最大值与最小值动态变化的可能；适用场景:数据较为稳定，不存在极端的最大值和最小值\n3、均值归一化：公式：(x-x均值)/max(x)-min(x)\n4、缩放成单位向量：公式：x/||x||\n5、基于树的方法是不需要特征归一化或标准化，比如随机森林，bagging 和 boosting等。基于参数的模型或基于距离的模型，需要特征归一化或标准化。\n"
    },
    {
        "title": "特征处理-数值离散化",
        "content": "概念:把无限空间中的有限个体映射到有限空间中去，提高算法的时空效率，简单讲就是在不改变数据相对大小的情况下，对数据进行相应缩小;离散化仅适用于只关注元素之间的大小关系而不关注元素数值本身的情况 在数据挖掘理论研究中，研究表明离散化数值也能在提高建模速度和提高模型精度上有显著作用；\n作用:离散化可以降低特征中的噪音节点，提升特征的表达能力但是离散化的过程都会带来一定的信息丢失\n（一）、数值变量分离散变量和连续变量\n连续变量中的有监督连续变量离散方法包括1R（把连续的区间分成小区间，然后根据类标签对区间内变量调整）、基于信息熵（自顶向下的方法，运用决策树理念进行离散化）、基于卡方（自底向上的方法，运用卡方检验的方法，自底向上合并数值进行有监督离散化，核心操作是merge）；\n连续变量中的无监督连续变量离散方法包括聚类划分（使用聚类算法将数据分为K类，需要制定K值的大小）；分箱-数据先排序（1等宽划分-把连续变量按照相同的区间间隔划分成几等份，也就是根据数值的最大值和最小值进行划分，分为N份，每份的数值间隔相同；2等频划分-把连续变量划分成几等份，保证每份数值个数相同）\n（二）、分类变量分有序分类变量和无序分类变量\n有序分类变量离散化方法包括Label-Encoding（有序分类变量数值之间存在一定的顺序关系，可直接使用划分后的数据进行数据建模。优点:解决了分类变量的编码问题。缺点:可解释性差）\n无序分类变量离散化方法包括独热编码（使用M位状态寄存器对M个状态进行编码，每个状态都有独立的寄存器位，这些特征互斥，所以在任意时候只有一位有效，也就是说这M位状态中只有一个状态位值为1，其他都是0，换句话说就是M个变量用M维表示，每个维度的数值为1或为0。；优点:解决了分类器不好处理分类变量的问题。；缺点:分类变量不宜过多，可能会造成稀疏矩阵）哑编码（哑编码和独热编码类似，唯一区别就是哑编码采用M-1位状态寄存器对M个状态进行编码。；优点:解决了分类器不好处理分类变量的问题。；缺点:分类变量不宜过多，可能会造成稀疏矩阵）"
    },
    {
        "title": "特征处理-特征编码",
        "content": "数据挖掘中，一些算法可以直接计算分类变量，比如决策树模型，但许多机器学习算法不能直接处理分类变量，他们的输入和输出都是数值型数据，因此把分类变量转换成数值型数据是必要的，可用独热编码和哑编码实现\n应用场景1、对逻辑回归中的连续变量做离散化处理，然后对离散特征进行独热编码或者哑编码，这样会使模型具有较强的非线性能力\n应用场景2、对于不能处理分类变量的模型，必须要先使用独热编码或哑编码，将变量转换成数值型；但若模型可以处理分类变量，那就无须转换数据，如树模型"
    },
    {
        "title": "用户画像",
        "content": "用户画像：即用户信息标签化，就是收集这个用户的各种行为和数据，从而分析得到这个用户的一些基本的信息和典型特征，最后形成一个人物原型，从中挖掘用户价值，从而提供业务推荐、精准营销等服务\n用户画像构建流程：1、收集用户相关数据：可以来源于网站上用户交易数据，用户日志数据，用户行为数据等；。\n2、数据预处理：对收集到的用户数据做一些预处理工作，包括检查数据是否完整，即数据是否含有缺失值，数据是否含有异常值以及数据是否存在不均衡现象，从而决定是否对缺失值填充，或者删除异常值，对不均衡数据进行重采样等数据清洗工作。最终将杂乱无章的数据转化为结构化数据，即我们后续机器学习模型可以识别的数据格式。；。\n3、用户行为数据建模：对于不携带标签的用户数据，也就是我们事先并不知道该用户属于哪一类别，或者说他和哪些用户行为数据比较相似，此时我们可以采用无监督学习中的聚类算法来对它们进行聚类，相似相近、关联性大的用户数据放在一起，不相似不相近，关联性不大的用户不放在一起，可以用这些聚类算法，针对不同形状的数据进行相应的聚类，此时聚类结束后，我们可以用相应的聚类评价指标来评价我们聚类模型性能的优劣，或者将聚类结果与专家给出的标准做对比，将聚类结果回归到真实商业逻辑上。对聚类后不同类别的用户进行打标签，从而得到带有标签的用户数据，对于携带标签的数据，我们接着选择机器学习中有监督学习的相关算法来对这些有标签的数据进行统计分析。\n4、举例：例如某银行推行一种信用卡，该银行的目标就是想知道哪些客户群体会办理信用卡，哪些客户群体不会办理信用卡，所以我们可以通过收集关于用户办理信用卡相关的数据，比如年龄、性别、婚姻状况、名下是否有房产、月收入年收入、之前是否办理过信用卡等等信息，通过聚类算法将用户分为三类群体，即会办理、不会办理以及未知三种客户群体，银行可据此向会办理信用卡的客户群体推送办卡相关业务以及活动福利，实现业务推荐和精准营销。最终我们可以通过现有的携带标签的用户数据，构建机器学习分类模型，例如LR、SVM等算法模型，然后通过该模型对未知用户进行预测，预测该用户是否会办理信用卡"
    },
    {
        "title": "无监督-聚类",
        "content": "无监督：是指在未加入标签的数据中，根据数据之间的属性特征和关联性对数据进行区分，相似相近、关联性大的数据放在一起，不相似不相近的、关联性不大的数据不放在一起；无监督本质：利用无标签的数据去学习数据的分布和数据与数据之间的关系；无监督算法包括如聚类算法、关联算法。\n分类：基于原型（K-means算法、K-means++算法、k-mediods算法）基于层次（HierarchicalClustring算法、Birch算法）基于密度（DBSCAN算法）\n（1）、K-means算法：思想：输入聚类个数K，已经包含n个数据样本的数据集，输出标准的K个聚类的算法，然后将n个数据样本划分为K个聚类，最终结果所满足：同一聚类中数据相似度较高，而不同聚类的数据相似度低；步骤：1、随机选取K个对象作为初始质心  2、计算样本到K个质心的欧氏距离，按就近原则将它们划分到距离最近的质心所对应的类中   3、计算各类别中所有样本对应的均值，将均值作为新的质心，计算目标函数  4、判断聚类中心或目标函数是否改变，若不变则输出，若改变，则返回2；；；；；\nK-means算法核心问题：K值如何选取：1、人工指定：多次选取K值，选择聚类效果最好的K值。2、均方根：假设我们有m个样本，该方法认为K=根号下m/2。3、枚举法：计算类内距离均值与类间距离均值之比，选择最小的K值，将所有K值进行二次聚类，选择两次聚类结果最相似的K值。4、手肘法：随着K值得增大，样本划分越来越精细，聚类得程度越来越高，误差平方和SSE便逐渐减小(误差平方和是所有样本的聚类误差，代表聚类效果的好坏，SSE越小越好)，当K小于真实聚类数时，随着K值的增大会大幅增加每个簇的聚合程度，SSE的下降幅度也会很大，当K等于真实聚类数时，随着K值的继续增大，聚合程度也会迅速减小，SSE的下降幅度便会骤减，然后随着K的继续增大而趋于平缓，所以K值和SSE的关系图就像一个手肘的图形，肘部对应的值便是数据真实聚类的个数\n（2）、K-means++：背景：为了解决K-means初始质心敏感的问题。；技术原理：不同于K-means算法是第一次随机选取K个样本作为初始质心，K-menas++是假设已经选取了p个初始质心，只有在选取第p+1个质心时，距离这p个质心越远的点会有更高的概率当选为第p+1个聚类中心(为了避免异常点的存在，第二个点的选择会从距离较远的几个点中通过加权选取第二个点)，只有在选取第一个聚类中心时是随机选取(p=1)，该方法的改进符合一般直觉：聚类中心之间距离的越远越好\n（3）、K-mediods：能够避免数据中异常值的影响；算法步骤：1、随机选取一组样本点作为中心点集  2、每个中心点对应一个簇 3、计算各样本点到各个中心点的距离，将样本点放入距离最近的中心点的类中。  4、计算各簇距簇内各样本点的距离绝对误差最小的点，作为新的聚类中心    5、如果新的聚类中心与原中心点相同，则过程结束，如果不同，则返回2\nK-mediods与k-means算法对比：1、算法流程基本一致。；2、质心的计算方式不同:k-means算法是将所有样本点对应的均值作为新的中心点，可能是样本中不存在的点 K-mediods是计算簇内每一个点到簇内其他点的距离之和，将绝对误差最小的点作为新的聚类中心，质心必须是某个样本点的值。；3、k-mediods可以避免数据中异常值带来的影响。；4、质心的计算复杂度更高:k-means直接将均值点作为新的聚类中心，而k-mediods需要计算簇内任意两点之间的距离，在对每个距离进行比较狭取新的质心，计算复杂度增加，速度变慢。；5、稳定性高，执行速度变慢:在具有异常值的小样本数据集中，k-mediods算法比k-means算法效果好，但是随着数据集规模的增加，k-mediods算法执行的速度会很慢，所以如果数据集本身不存在很多的异常值的话，就不用k-mediods代者k-means。；\n（4）、Hierarchical Clustering算法：思想:确保距离近的样本落在同一个族中\n步骤:1、每个样本点都作为一个簇，形成族的集合C 。2、将距离最近的两个簇合并，形成一个簇3、从C中去除这对簇。 4、最终形成层次树形的聚类结构树形图（判断两个簇之间的距离方法：1单链接 -- 不同两个簇之间最近的两个点的距离；2全链接-- 不同两个簇之间最远的两个点的距离；3均链接 -- 不同两个簇中所有点两两之间的平均距离）\n优点:可排除噪声点的干扰，但有可能和噪声点分为一簇 2、适合形状不规则，不要求聚类完全的情况 3、不必确定K值，可根聚类结果不同有不同的结果 4、原埋简单，易于理解\n缺点:计算量很大，耗费的存储空间相对于其他几种方法要高。 2、合并操作不能撤销 3、合并操作必须有一个合并限制比例，否则可能发生过度合并导致所有分类中心聚集，造成聚类失败\n适用场景:适合形状不规则，不要求聚类完全的情况\n（5）、Birch：使用聚类特征三元组表示一个簇的有关信息，而不用且体的一组点来表示该簇，通过构造满足分支因子和簇直径限制的聚类特征树来进行聚类三元组(数据点样本个数，数据点样本特征之和，数据点样本特征平方和)，分支因子:树的每个节点的样本个数，簇直径:一类点的距离范国\n算法步骤:1、扫描数据，建立聚类特征树 2.使用某种算法对聚类特征树的叶节点进行聚类\n优点:一次扫描就能进行很好的聚类\n缺点:要求是球形聚类，因为CF树存储的都是半径类的数据，都是球形才适合\n适用场景:因为Birch算法通过一次扫描就可以进行比较好的聚类，所以适用于大数据集，而且数据的分布呈凸型以及球形的情况，并且由于Birch算法需要提供正确的器类个数和簇直径限制，对不可视的高维数据不可行\n（6）、DBSCAN：一个聚类可以由其中任何核心对象唯一确定，该算法利用基于密度的概念，要求聚类空间中某一区域内的样本个数不小于某一给定闻值，该方法能够在具有噪声的空间数据库中发现任意形状的簇，可将密度足够大的相邻区域连接，能够有效处理异常数据，主要用于对空间教据的聚类。\n步骤:1、DBSCAN通过检查数据中每个样本的eps邻域来搜索簇，如果点p的eps邻域内包含的样本个数大于给定的闽值，那么就建立一个以点P为核心对象的簇。\n2、然后DBSCAN送代的聚集这些核心对象直接密度可达的对象，这个过程可能还会涉及密度可达簇的合并\n3、当没有新的样本点加入到簇中时，过程结束"
    },
    {
        "title": "数据属性",
        "content": "标称属性：标称，意味着与名称有关，标称属性的值是一些符号或事物的名称，每个值代表事物的某种类别、状态或编码。因此标称属性又被看做是分类的。标称属性的值不具有有意义的顺序，而且是不定量的。也就是说给定一个数据集，找出这种属性的均值没有意义;举例：头发颜色和婚姻状况，头发的颜色={黑色、棕色、红色、白色}；婚姻状况={单身、已婚、离异、丧偶}；其次还有职业，具有教师、程序员、农民等\n二元属性:二元属性是一种标称属性，只有两种状态，用来描述事物的两种状态，用值0或1来体现，0表示该属性不出现，1表示出现。二元属性又称布尔属性，如果两种状态对应的是True和False;对称的二元属性(两种状态具有相等的价值，携带相同的权重，例如性别，具有男女两种状态)非对称的二元属性(其状态的结果不是同等重要的，例如艾滋病化验结果的阳性和阴性，用1对最重要的结果(HIV阳性)编码，而另一个用0编码(HIV阴性))\n序数属性:其可能的值之间具有有意义的序或秩评定，但是相继值之间的差是未知的，也就是对应的值有先后顺序。序数属性可以把数量值的值域划分成有限个有序类别。（如0-很不满意，1-不满意，2-满意，3-很满意），把数值属性离散化得到。可以用众数和中位数表示序数属性的中心趋势，但不能定义均值.;举例：成绩={优、良、中、差}；    drink_size表示饮料杯的大小：大、小、中。\n数值属性:是定量的，即它是可以度量的量，用整数或实数值表示;(1)区间标度属性:用相等的单位尺度度量，区间标度属性的值有序，可以评估值之间的差，但不能评估倍数，没有固有的零点(也就是说0°C并不是指现在没有温度)。举例：摄氏温度、华氏温度，日历日期。不能说2020年是1010年的两倍，两者的差有意义，但是比值没有意义；(2)比率标度属性:具有固有零点的数值属性。比率标度属性的值有序，可以评估值之间的差，也可以评估倍数.举例：开氏温度、重量、高度、速度、货币量。拿货币来说，可以说200元比100元多100元，也可以说200元是100元的两倍\n离散属性：具有有限或无限可数个值，可以用或不用整数表示。例如，属性customer_ID是无限可数的。顾客数量是无限增长的，但事实上实际的值集合是可数的（可以建立这些值与整数集合的一一对应）举例：邮编、省份数目\n连续属性:如果属性不是离散的，则它是连续的。属性值为实数，一般用浮点变量表示。\n总结：（1）标称、二元、序数属性都是定性的，他们只描述对象的特征，而不给出实际大小和数值；（2）标称、二元属性的中心趋势可以用众数度量。序数属性的中心趋势可以用它的众数和中位数度量，但不能定义均值（3）所有属性都能用中心趋势来表述。标称、二元属性用众数度量；序数属性用众数、中位数度量。均值是数值属性的中心趋势描述"
    },
    {
        "title": "降维-PCA，奇异值、主成份分析",
        "content": "思想：降维就是将事物的特征进行压缩和筛选，将原始高维空间中的数据映射到低维空间中，该项任务比较抽象，如果没有特定领域的知识，很难事先决定采用哪些数据。比如在人脸识别任务中，如果直接采用图片的原始像素信息，那么数据的维度是非常大的，所以此时就要用到降维的方法，对图片信息进行处理，选出区分度最大的像素组合\n1、SVD（奇异值分解）：1、奇异值分解可以适用于任意矩阵的一种分解方法 2、奇异值分解可以发现数据中隐藏的特征来建立矩阵行列之间的关系 3、奇异值分解能够发现矩阵中的几余，并提供用于消除它的格式\n优点:原理简单，仅涉及简单的矩阵线性变换知识。可以有效处理数据噪音，矩阵处理过程中得到的三个短阵也具有物理意义\n缺点:分解出的矩阵可解释性差，计算量大\n应用场景:广泛的用来求解线性最小平方、最小二乘问题，低秩通近问题，数据压缩问题，应用于推荐系统(找到用户没有评分的物品，经过SVD压缩后低维空间中，计算未评分物品与其他物品的相似性，得到一个预测打分，再对这些物品评分从高到低排序，返回前N个物品推荐给用户)\n2、PCA（主成份分析法）：思想:寻找表示数据分布的最优子空间(降维，去掉线性相关性)，就是将n维特征映射到k维上(k<n)，k维是全新的正交特征(k维特征称为主成份，是重新构造出来的k维特征，而不是简单的从n维特征中去除n-k维特征)，PCA目的是在高维数据中寻找最大方差的方向，然后将原始数据映射到维数小的新空间中。数学原理:根据协方差矩阵选取前s个最大特征值所对应的特征向量构建映射矩阵，进行降维\n优点:1、方差衡量的无监督学习，不受样本标签的限制 2、各主成份之间正交，可消除原始数据中各特征之间的影响 3、计算方法简单，主要运算是奇异值分解，容易在计算机上实现\n缺点:主成份解释某含义具有一定的模糊性，不如原始样本特征解释性强 2、方差小的非主成份也可能含有重要信息，因降维丢弃后可能对后续数据处理产生影响\n算法流程：1、对所有样本构成的矩阵X去中心化2、求X的协方差矩阵C   3、利用特征值分解，求出协方差矩阵C的特征值和特征向量      4、取前s个最大特征值对应的特征向量构成变换矩阵W    5、用原数据集和变换矩阵W相乘，得到降维后的新数据集（矩阵）\n追问:详细介绍PCA：在PCA中，数据从原始坐标系转换到新的坐标系中，转换坐标系时，选择数据方差最大的方向作为坐标轴的方向，（方差最大的方向给出了数据最重要的信息），第一个坐标轴的方向是方差最大的方向，第二个新坐标轴的方向是与第一个新坐标轴正交且方差次大的方向，重复过程N次，N是数据原始维度，通过这种方式可以获得新的坐标系，其中大部分方差包含在前几个坐标轴中，而后几个坐标轴中方差基本为0，这时可以忽略后面的坐标轴，只保留前面几个包含大部分方差的坐标轴；通过计算协方差矩阵，可以得到协方差矩阵的特征值和特征向量，选取前几个最大特征值所对应的特征向量组成变换矩阵，就可以将矩阵转换到新的坐标系中，实现数据的降维\n3、LDA（线性判别分析）：思想:寻找可分性判据最大的子空间，即投影后类内间距最小，类间距离最大 2、用到了fisher的思想，即寻找一个向量，可以使得类内散度最小，类间散度最大，其实也就是选取特征向量构建映射矩阵，然后对数据进行处理，该方法能使投影后样本的类间散步矩阵最大，类内散步矩阵最小\n优点:降维过程中可以使用类别的先验经验 2、LDA在样本分类信息依赖均值而不是方差的时候，比PCA之类的算法更优\n缺点:LDA不适合对非高斯分布的样本进行降维，PCA也有这个问题 2、LDA在样本分类信息依赖方差而不是均值的时候，降维效果不好 3、LDA可能过度拟合数据 4、LDA降维最多降到类别数k-1的维数，如果我们降维的维度大于k-1，则不能使用LDA\n4、LLE（局部线性嵌入）：相比较于传统的PCA、LDA这种只关注样本方差的降维方法，LLE在保持降维效果的同时也保留了样本局部的线性特征，因为其保留了局部的线性特征，所以常被应用在高维数据可视化、图像识别等领域\n5、LDA VS PCA：共同点：1、都属于线性方法。2、在降维时都采用矩阵分解的方法。3、假设数据符合正态分布。；不同点：1、LDA是有监督的，不能保证投影到新空间的坐标系是正交的，选择分类性能最好的投影方向。2、LDA可以用于降维也可以用于分类。3、LDA降维保留个数与其对应的类别数有关，与数据本身维度无关\n奇异值分解定义：将矩阵分解为奇异向量和奇异值，可以将矩阵A分解为三个矩阵的乘积，即A=UDV^t，其中矩阵U和V为正交矩阵，U的列向量为左奇异向量，V的列向量为右奇异向量，D矩阵的对角线上的值为矩阵A的奇异值，D矩阵为对角矩阵，奇异值按大小进行排序，对角线之外的值为0\n奇异值分解应用：1、在机器学习和数据挖掘领域，奇异值的应用很广泛，比如应用在用于降维的PCA(主成份分析法)和LDA(线性判别分析法)，数据压缩（以图像压缩为代表）算法，还有做搜索引擎语义层次检索的LSI。；2、求矩阵伪逆：奇异值分解可以被应用于求矩阵的伪逆，若矩阵M的奇异值分解为M=UDV^t，那么M的伪逆为M=VD^+U^t，其中D+为D的伪逆，并将其主对角线上非零元素求倒数在转置得到的。求伪逆通常可以用来求解线性最小平方、最小二乘法问题。；3、矩阵近似值：奇异值分解在统计中的主要应用为PCA（主成分分析），一种数据分析的方法，用来发现大量数据中的隐含模式，可以用于模式识别，数据压缩等方面.PCA方法的作用是将数据集映射到低维空间中的坐标系中，数据集的特征值(可以用奇异值来表征)按照重要性排列，降维的过程其实就是舍弃不重要的特征向量，而剩下的特征向量组成的空间就是降维后的空间。；4、平行奇异值：用于频率选择性的衰落信道的分解"
    },
    {
        "title": "数据挖掘流程",
        "content": "数据挖掘是通过对大量的数据进行分析，以发现和提取隐含在其中的具有价值的信息和知识的过程。跨行业数据挖掘标准流程，是当今数据挖掘业界通用流行的标准之一，它强调数据挖掘技术在商业中的应用，是用以管理并指导Data Miner有效、准确开展数据挖掘工作获得最佳挖掘成果的一系列工作步骤的规范标准\n1、商业理解：从商业角度理解项目的目标和要求，然后把理解转化为数据挖问题的定义和一个旨在实现目标的初步计划。确定业务目标：分析项目背录，从业务角度分析项目的需求和目标，确定业务角度成功标准。；项目可行性分析：分析现有资源、条件和限制，风险估计、成本和效益估计。；确定数据控掘目标：明确数据挖狂的目标和成功标隹。；提出项目计划：对整个项目做出计划，初步估计用到的工具和技术。\n2、数据理解：开始于原始数据的收集，然后是熟悉数据，表明数据质量问题，探索数据进而对数据初步理解，发掘有趣的子集以形成对隐藏信息的假设。收集原始数据：收集项目所涉及到的数据，如有必要，将数据传入数据处理工具中，并做一些初步数据集成的工作，生成相应报告。；描述数据：对数据做一些大致描述，例如属性数、记录数等，生成报告。；探索数据：对数据做简单的统计分析，例如关键属性的分布。；检查数据质量：包括数据是否完整、是否有错误，是否会有缺失值等\n3、数据准备：从原始未加工的数据构造最终数据集的过程(这些数据指将要嵌入建模工具中的数据)，数据准备任务可能要实施多次，并且没有任何规定的顺序。这些任务包括表格、属性和记录的选择以及按照建模工具要求，对数据进行转换、清洗等。；数据选择：根据数据挖掘目标和数据质量选择合适数据，包括表格选择、属性选择和记是选择。；数据清洁：对选择出的教据进行清洁，提高数据质量，例去除噪音、填充缺失值等。；数据创建：在原始数据上生成新的记录或属性。；数据合并：利用表连接等方式将几个数据集合并在一起。；教据格式化：将数据转化为数据挖掘处理的格式。\n4、建立模型：选择和应用各种建模技术，同时对他们的参数进行调整，以达到最优值。选择建模技术 -- 确定数据挖症算法模型和参数；测试方案设计：设计某测试模型的质是和有效性机制模型训练：在准备好的数据集上运行数据挖掘算法，得出一个或多个模型模型测试评估：根据测试方案，从数据挖狂技术角度确定数据挖掘目标是否成功\n5、模型评估：更为彻底的评估模型和检查建立模型的各个步骤，从而确保它真正的达到了商业目标。结果评估：从商业角度评估得到的模型，甚至实际试用该模型测试其效果；过程回顾：回顾项目的所有流程，确定每一个阶段都没有错误；确定下一步工作：根据结果评估和过程回顾得出结论，确定部晋该控掘模型还是从某个阶段重新开始\n6、模型实施：实施计划 ：在业务运作中部署模型做出计划；监控和维护计划：如何监控模型在实际业务中的使用情况，如何维护该模型；作出最终报告 ：项目总结，项目经验和项目结果；项目回顾：回顾项目的实施过程，总结经验教训;对数据挖握的运行效果做一个预测"
    },
    {
        "title": "数据采集抽样方法",
        "content": "简单随机抽样：先将调查总体进行编号，然后根据抽签法或者随机数字表抽取部分所观察到的数据组成样本数据，分为有放回抽样和无放回抽样。常被应用于压缩数据量以减少费用和时间开销\n系统抽样：又被称为等距抽样，首先设定抽样间距为n，然后从前n个数据样本中抽取初始数据，然后按照顺序每隔n个单位抽取一个数据组成样本数据\n分层抽样：将总体数据按照特征划分为若干层次或类型，然后从各个类型和层次的数据中采用简单随机抽样或者系统抽样抽取子样本，最终将这些子样本组合为总体数据样本，常被应用于离网预警模型和金融欺诈模型等严重有偏的数据\n整群抽样：将总体数据按照属性拆分为互不相交、互不重复的群，这些群中的数据尽可能具有不同属性，尽量能代表总体数据的信息，然后以群为单位进行抽样"
    },
    {
        "title": "数据清洗",
        "content": "定义：数据清洗是指通过删除、转换、组合等方法或策略清洗数据中的异常样本，为数据建模提供优质数据的过程；场景：不均衡数据处理、缺失值处理、异常值处理\n1.不均衡数据处理：类别数据不平衡是分类任务中出现的经典问题，一般在数据清洗环节进行处理，不均衡简单来讲就是数据集中一个类别的数据远超其他类别数据的数据量，比如在1000条用户数据中，男性数据占950条，女性数据只占50条；处理办法1重采样数据（过采样：对少的一类进行重复选择，欠采样：对多的一类进行少量随机选择）2.K-fold交叉验证3.一分类4.组合不同的重采样数据集（核心原理：建立N个模型。    假设稀有样本有100个，然后从丰富样本中抽取1000(100*10)个数据，将这1000个分为N份，分别和100个稀有数据合并建立模型）\n2.异常值处理：异常值指偏离正常范围的值，不是错误值，异常值出现频率较低，但又会对实际项目分析造成影响；检测方式：异常值一般通过箱型图或分布图来判断；处理办法：采取盖帽法或者数据离散化   2、删除异常值、使用与异常值较少的时候  3、将异常值视为缺失值，按照缺失值的处理方法处理   4、估算异常值，mean/mode/median\n3.缺失值处理：数据缺失产生原因：1、人为疏忽、机器故障等客观原因造成数据缺失   2、人为故意隐瞒部分数据，比如在数据表中有意将一列属性视为空值，此时缺失值可以被看作为特殊的特征值   3、数数据本身不存在，例如银行在做用户信息收集时，对于学生群体来说，薪资这一列就不存在，所以在数据集中显示为空值   4、系统实时性要求较高  5、历史局限性导致数据收集不完整；；；.缺失值处理方法包括1.删除（适用场景：数据量大，缺失值少的数据集，完全随机缺失时可以直接使用删除操作）2.填充。3.不处理（补齐得缺失值毕竟不是原始数据，所以不一定符合客观事实，数据填充在一定程度上改变了数据的原始分布，也不排除加入噪音点的可能，因此对于一些无法容忍缺失值的模型可以进行填充，但有些模型本身可以容忍一定的数据缺失，此时选用不处理的方式，比如XGboost模型）\n缺失值填充方法包括如下4种：（1）.数值填充：众数填充-以类别数据量较多的类别填充，适用于数据倾斜时。；均值、中位数填充：以所有非缺失值的Mean或Median填充缺失值(广义插补)   2、分类别计算非缺失值的Mean或Median填充各类别的缺失值(相似填充)；均值填充：适用于数据中没有极端值的场景；中位数填充：适用于数据中有奇异值的场景\n（2）KNN：通过KNN算法将所有样本进行划分，通过计算欧氏距离，选取与缺失数据样本最近的K个样本，然后通过投票法或者K个值加权平均来估计该缺失样本的缺失数据；优点：不需要为含有缺失值的每个属性都建立预测模型  2、一个属性有多个缺失值时也可以很好解决   3、缺失值处理时把数据结构之间的相关性考虑在内；缺点：面对大数据集，KNN时间开销大  2、K值得选择是关键，过大过小都会影响结果。\n（3）回归：把数据中不含缺失值得部分当作训练集，建立回归模型，将此回归模型用来预测缺失值；适用场景：只适用缺失值是连续得情况；优缺点：预测填充理论上比值填充效果好，但如果缺失值与其他变量没有关系，那么预测出得缺失值没有意义\n（4）变量映射：把变量映射到高维空间中，优点：可以保留数据得完整性，无需考虑缺失值，缺点：1、计算开销增加  2、可能会出现稀疏矩阵，影响模型质量。"
    }]
    results = []
    for entry in json_data:
        if key.lower() in entry['title'].lower():
            results.append({'title': entry['title'], 'content': entry['content']})
    for i in results:
        print(i)
def lunshu_by_t(key):
    json_data=[
    {
        "title": "关联规则",
        "content": "反映了一个事物与其他事物之间相互依存性和关联性，如果两个事物或多个事物之间存在关联规则，那么其中的一个事物就可以由其他事物来预测到。关联规则必须在频繁项集中诞生且满足一定的置信度阈值\n整体步骤：1、生成频繁项集和生成规则2、找到强关联规则3、找出所有满足强关联规则的项集\n涉及的概念：项集（项的集合，包含k项的集合称为k项集）频繁项集（满足最小支持度阈值的项集）支持度（如果有两个不相交的非空集合X、Y(物品集)，N为数据记录总数，X集合和Y集合共同出现的次数占记录总数的比例：support(X->Y)=|X交Y|/N）置信度（集合X和集合Y共同出现的次数占集合X出现次数的比例，confidence(X->Y)=|X交Y|/|X|）提升度（表示置信度与Y总体发生的概率之比，lift(X->Y)=confidence(X->Y)/P(Y)）强关联规则（满足最小支持度阈值和最小置信度阈值的规则被称为强关联规则）"
    },
    {
        "title": "关联规则-Apriori算法",
        "content": "工作原理：1、首先扫描整个数据集，然后产生一个大的候选项集，计算每个候选项的次数，然后基于预先设定的最小支持度阈值，找出频繁一项集集合2、然后基于频繁一项集和原数据集找到频繁二项集3、同样的办法直到生成频繁N项集，其中已不可在生成满足最小支持度的N+1项集，也就是极大频繁项集4、根据Apriori定律1：频繁项集的子集一定是频繁项集，所以到此就找到了所有的频繁项集，然后又可以根据Apriori定律2：非频繁项集的超集一定是非频繁项集来帮助算法构建频繁项集树，加快收敛速度5、然后为每一个频繁项集创建一颗置信树（并不只为极大频繁项集创建置信树，还要为极大频繁项集的所有子集都创建置信树）6、最后可以得出数据之间的关联规则，且该关联规则满足支持度和置信度的阈值\n优点：1、使用先验原理，大大提高了频繁项集逐层产生的效率    2、简单易理解，数据集要求低\n缺点：1、每一步产生的候选项集时循环产生的组合过多，没有排除不该参与组合的元素    2、每次计算项集的支持度时，都需要将数据库中的记录全部扫描一遍，如果是一个大型数据库的话，这种扫描会大大增加计算机I/O的开销，而这种代价是伴随着数据库记录的增加呈几何级增长的，因此人们开始追求更好的算法\n应用：推荐系统：用关联算法做协同过滤，Apriori不适于非重复项集数元素较多的案例，建议分析的商品种类为10类左右。"
    },
    {
        "title": "关联规则-FP-growth算法",
        "content": "定义：该算法建立在Apriori算法概念之上，不同之处是它采用了更高级的数据结构FP-tree减少数据扫描次数，只需要扫描两次数据库，相比于Apriori减少了I/O操作，克服了Apriori算法需要多次扫描数据库的问题\n为了减少I/O次数，FP-growth引入了一些数据结构来临时存储数据，数据结构包括三部分：1、一个项头表里面记录了所有的频繁一项集出现的次数，并且按照次数降序排序  2、FP-Tree将原始数据集映射到了内存中的一棵FP树  3、节点链表，所有项头表的频繁一项集都是一个节点链表的头，它依次指向FP树中的该频繁一项集出现的位置。这样做主要是方便项头表和FP-tree之间的联系查找和更新，也好理解。\n算法流程：1、扫描数据，得到所有频繁一项集的计数，然后删除低于支持度阈值的项集，将频繁一项集放入项头表，并按支持度降序排列。2、扫描数据，将读到的原始数据剔除非频繁一项集，并将每一条再按支持度降序排列  3、读入排序后的数据集，逐条插入FP树，插入时按照排序后的顺序插入FP树中，排序靠前的是祖先节点，靠后的是子孙节点，如果有共用的祖先，则对应的共用祖先节点计数加1，插入后如果有新的节点出现，则项头表对应的节点会通过节点链表连接上新节点，直到所有数据都插入到FP树上，FP树建立完成。  4、从项头表的底部依次向上找到项头表对应的条件模式基。从条件模式基递归挖掘得到项头表项的频繁项集 5、如果不限制频繁项集的项数，则返回步骤4所有的频繁项集，否则只返回满足项数要求的频繁项集\n优点：优点：FP-growth一般快于Apriori，因为只扫描两次数据库\n缺点：缺点：1、FP-growth实现比较困难，在某些数据集上性能可能会下降   2、适用数据类型：离散型数据"
    },
    {
        "title": "随机森林",
        "content": "概述：在机器学习中，直接建立一个高性能的分类器是很困难的，但是如果构建一系列性能较差的弱分类器，再将这些弱分类器集成起来，也许就能得到一个性能较高的分类器。通常根据训练集的不同，会训练得到不同的基分类器，这时可以通过训练集的不同来构造不同的基分类器，最终把他们集成起来，形成一个组合分类器。  组合分类器是一个复合模型，由多个基分类器组合而成，基分类器通过投票，组合分类器基于投票的结果进行预测。组合分类器往往比基分类器更加准确，常用的组合方法：装袋、提升、随机森林\n构建分类器的过程一般有两种集成方法1、利用训练集的不同子集训练得到不同的基分类器2、利用同一个训练集的不同属性子集构建不同的基分类器\n随机森林是bagging的一个扩展变体，它是以决策树为基学习器构建bagging的基础上，进一步在决策树的训练过程中引入了随机属性选择。具体步骤：1、从样本集中用自助法选出n个样本组成子集  2、从所有属性n中随机选择K个属性，选择最佳分裂属性(ID3、C4.5、CART)作为节点建立决策树，每棵树都最大程度生长，不进行剪枝 3、重复以上两步m次，建立m棵决策树  4、这m个决策树形成随机森林，在分类问题中通过投票决定输出属于哪一类，在回归问题中输出所有决策树输出的平均值\n优点：1、准确率高。2、各个初级学习器可以并行计算。3、能处理高维持征的数据样本，不需要降维。4、能够评估各个特征在分类问题中的重要性。5、因为采用随机选择特征，对部分特征缺失不数感。6、由于采用有放回抽样，训练出来的模型方差小,泛化能力强\n缺点：1、取值较多的特征会对RF的决策树产生更大影响有可能影响模型的效果。2、bagging保证了预测准确率，但损失了解释率。3、在某些噪音比较大的特征上，RF还是容易陷入过拟合"
    },
    {
        "title": "Bagging",
        "content": "对训练集进行有放回的抽取训练样例，从而为每一个基学习器都构建一个与训练集相当大小但各不相同的训练集，从而训练出不同的基学习器。Bagging是并行计算来训练每个弱学习器，且每个弱学习器的权重相等，且训练集权重也相等\n算法流程:1、从大小为N的原始数据集中独立随机地抽取m个数据(m<=n),形成一个自助数据集 2、重复第一步K次，产生K个独立的自助数据集。3、利用K个数据集训练出K个最优模型(K次可以并行进行) 4、分类问题中的分警结果根据K个模型的结果投票决定，回归问题:对K个模型的值求平均得到结果\n特点：1、通过降低基学习器的方差改善了泛化误差。2、由于每一个样本被选中的概率相同，所以装袋并不侧重于训练数据集中的任何实例，因为对于噪声数据，装袋不太受过分拟合的影响。3、由于是多个决策树组成，所以装袋提升了准确率的同时损失了解释性，哪个变是起到重要作用未知"
    },
    {
        "title": "boosting",
        "content": "主要作用和bagging类似，都是将昔干基分类器整合为一个分类器的方法，boosting是个顺序的过程，每个后续模里都会尝试纠正先前模型的错误，后续的模型依赖之前的模型\n算法步骤:1、首先给每一个训练样例赋予相同的权重 2、然后训练第一个基分类器并用它对训练集进行测试，对于那些分类错误的测试样本提高权重(实际算法中是降低分类正确的样本的权重)3、随后用调整后的带权训练集训练第二个基分类器 4、最后重复这个过程直到最后得到一个足够好的学习器\n提升是一个选代的过程，用于自适应地改变训练样本的分布，使得基分类警聚焦在那些很难分的样本上，不像bagging，提升给每一个训练样本赋予一个权值，而且可以在每一轮提升过程结束时自动地调整权值"
    },
    {
        "title": "adaboost",
        "content": "自适应在于前一个基分类器分错的样本会得到加权，加权后的全体样本再次被用来训练下一个基分类器，采用选代的思想，继承boosting，每次选代只训练一个弱学习器，训练好的弱学习器将参与下一次选代。\n步骤:1、初始化训练数据的权值分布，如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值:1/N 2、通过训练集训练得到弱分类器，将分类正确的样本降低权重，同时提升分类错误样本的权重，将权重更新后的训练集用于训练下一个分类器，如此选代下去 3、将这些得到的弱分类器泪合成强分类器，加大那些分类误差率小的分类器的权重，使其在最终的分类函数中起较大的决定作用，降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的作用，总之就是误差率低的弱分类器在最终分类器中权重较大，起较大的决定作用，否则较小。\n优点：1、很好地利用了弱分类器进行级联。2、提供的是框架，可以使用各种方法构建弱分类器，很灵活。3、不容易发生过拟合。4、具有很高的精度。5、相对于bagging和随机森林，adaboost充分考虑了每个分类器的权重\n缺点：对异常样本敏感，异常样本可能会在迭代过程中获得较高的权重值，最终影响模型效果。"
    },
    {
        "title": "XGboost",
        "content": "（1）xgboost本质上还是GBDT,但是把速和效率做到了极致，不同于传统的GBDT，只利用了一阶导数信息，xGboost对损失函数做了二阶求导泰勒展开，并在目标函数之外加入了正则项整体求最优解，用以权衡目标函数的下降和模型的复杂程度，避免过拟合，传统GBDTI以CART作为基学习器，XGboost还支持线性分类器，也就是xgboost相当于带L1和L2正则化项的逻辑回归(分类问题)或者线性回归(回归问题)\n（2）表勒展开的一次项和二次项系数不依赖于损失函数，所以xgboost支持自定义损失函数\n（3）XGboost在损失区数里加入正则项，用于控制模型复杂度，正则项降低了模型的复杂度，使学习出来的模型更加简单，防止过拟合，这点优于GBDT\n（4）xGboost还借鉴了随机森林算法，支持列抽样不仅能够降低过拟合还能减少计算，优于GBDT\n（5）Xgboost进行完一次选代后，会将叶子节点的权重乘上一个缩减系数，相当于学习速率\n(XxGboost中的eta)，主要是为了削弱每棵树的影响，让后面有更大的学习空间，实际应用中一般把eta设置的小一点(小学习率可使得后面的学习更加仔细)，选代次数设置的大一点\n（6）应用场景：预测用户行为喜好，商品推荐"
    },
    {
        "title": "梯度提升树（GBDT）",
        "content": "（1）梯度提升决策树，在GBDT中基学习器都是分类回归树，也就是CART，且使用的都是CART中回树。\n（2）对于初级学习器的结果来说一般都是低方差，高混差，因此GBDT的训练过程就是通过降低差来不断提高精度\n（3）GBDT的核心是在于累加所有树的结果作为最终结果，例如对年龄的累加，10岁加5岁，分类树的结果显然是累加不了的，所以GBDT中都是回归树，不是分类树，尽管GBDT调整后也可用于分类，但是不代表GBDT的树是分类树\n（4）GBDT的核心在于每一颗树学习的是之前所有树的结论和残差，这个残差就是一个加预测值后能得到真实值的累加是\n（5）主要使用到的损失函数有:0-1损失函数，对数损失区数，平方损失函数等，目标是:希望损失函数能够不断减小，还有希望损失函数能够尽快减小。\n（6）应用场景：常用于大数据挖掘竟赛；可用于几乎所有的回归问题，也可用于二分类但不适合多分类问题；广告推荐；排序问题"
    },
    {
        "title": "ELT，ETL",
        "content": "ELT，概念：即extract  load  transform(数据抽取，加载，转换)\n抽取：（1全量抽取）当数据源中有新数据加入或发生数据更新操作时，系统不会发出提醒，此时采用全量抽取，类似于数据迁移或复制，将数据源中的数据原封不动的从数据库中抽取出来，转换成自己的ETL工具可以识别的格式，一般在系统初始化时使用，全量一次后，就要每天采用增量抽取（2增量抽取）当数据源中有新数据加入或数据更新操作时，系统不会发出提醒，但可以识别出更新的数据，此时采用增量抽取，增量抽取只抽取自上次抽取以来，数据库中新增或修改的数据，在ETL中增量抽取使用更广泛；适合场景：1源数据数据量小2源数据不易发生变化3源数据规律性变化4目标数据量巨大（3更新提醒）当数据源中有新数据加入或数据更新操作时，系统会发出提醒，是最简单的一种抽取方式\n转换：抽取完数据后，要根据具体业务对数据进行转换操作，数据转换一般包括清洗和转换两部分，清洗掉数据集中重复、不完整的数据以及错误的数据\n加载：是将已按照业务需求清洗、转换、整理后的数据加载到各类数据仓库或数据库中，进而进行智能商业分析、数据挖掘等。（1全量加载）全表删除后在进行数据加载（2增量加载）目标表仅更新源表中变化的数据\n优势：1、相对于传统的ETL来说，ELT数据处理管道中无需单独的转换引擎，数据转换和数据消耗在同个地方。\n2、减少了数据预处理的时间开销，在实际应用中，下游各应用的目的各不相同，同一份数据可能有不同的应用，进而做不同的转换操作，面对这种情况，ETL需要多次对数据进行抽取、转换、加载，而ELT只需要进行一次数据的抽取加载，多次转换，从而实现一份数据的多次应用，大大降低了时间的开销。\n案例：电商用户收集用户信息进行商业分析，有两个核心目标：1、提高用户转化率  2、提高商品的精准营销效率  这两个项目都需要从同一数据源抽取数据，但是项目目标不同，所以需要的细节数据也就不同。\n对于ETL来说，相关人员需要从数据源抽取两次数据（一个项目各需要抽取一次），然后每个项目各需要进行一次ETL的完整流程；对于ELT来说，相关人员只需从数据源抽取一次数据，然后将数据加载到目的地(Hive、Hbase中)，最终转换成我们项目所需要的细节数据即可，无需重复的抽取和加载过程。"
    },
    {
        "title": "正则化。Lasso回归和岭回归",
        "content": "正则化：根据奥卡姆剃刀原理，在所有能解释数据的模型中，越简单的模型越靠谱。但是在实际问题中，为了拟合复杂的数据，不得不采用更复杂的模型，使用更复杂的模型通常会产生过拟合，而正则化就是防止过拟合的工具之一，通过限制参数过大或者过多来避免模型的复杂\nL1（Lasso Regression）、L2正则化（Ridge Regression）的目的都是为了防止过拟合，两者差别在于：岭回归中的L2正则项能够将一些特征变成很小的值，而Lasso回归中的L1正则项得到的特征是稀疏的。Lasso回归会趋向于减少特征的数量，相当于删除特征，类似于降维，而岭回归会把一些特征的权重调小，这些特征都是接近于0的，因此Lasso回归在特征选择的时候非常有用，而岭回归就是一种规则化而已。\n在所有特征中，如果只有少数特征起重要作用的话，选择Lasso回归比较合适，它能自动选择特征。而大部分特征能起到作用而且作用比较平均的话，选择岭回归更合适"
    },
    {
        "title": "spark mllib，rdd",
        "content": "数据对象：1Dataframe、2RDD、3Dataset(具有RDD和dataframe的优点，同时避免他们的缺点)。\n相同点：1、都有惰性机制，在进行转换操作时不会立即执行，只有遇到Active操作时才会执行2、都是spark的核心，只是弹性分布式数据集的不同体现3、都具有分区的概念4、都有相通的算子，比如map、filter等5、都会根据spark的内存情况自动做缓存计算即使数据量很大也不用担心内存的溢出，不用担心OOM-当请求的内存过大时，JVM无法满足而自杀\n不同点：1、RDD以person作为类型参数，但spark并不知道person的内部结构，而Dataframe提供了详细的结构信息，使得sparkSQL可以清楚的知道数据中包含了哪些列，以及列的名称和类型Dataframe提供了数据的结构信息，即schema2、RDD是分布式JAVA的对象集合，Dataframe是row对象的集合3、Dataframe除了提供比RDD更加丰富的算子以外，更重要的是提升执行效率、减少数据读取和执行计划的优化，比如filter下推、裁剪等。\nRDD操作：（优点：类型安全，面向对象；缺点：序列化和反序列化开销大，GC垃圾回收机制的性能开销，频繁的创建和销毁对象，势必增加GC）基本操作主要为Transformation算子（该算子是通过转换从一个或多个RDD生成新的RDD，该操作是惰性的，只有调用action算子时才发起job,典型算子如map、filter、flatMap,distinct等）和Action算子（当代码调用该类型算子的时候，立即启动job，典型算子包括：count、saveasTextfile、takeordered等）\nDataframe操作：（优点：自带schema信息，降低序列化和反序列化的开销；缺点：不是面向对象的，编译期不安全）"
    },
    {
        "title": "决策树",
        "content": "定义：决策树是一种分类算法，它通过对有标签的数据进行学习，学习数据的特征，得到一个模型，什么样的数据就打上什么样的标签\n算法思想：选择属性特征对训练集进行分类，使得各个子数据集有更好的分类，其中的关键点就是寻找分裂规则，因为它们决定了给定节点上的元组如何分裂\n决策树生成步骤：1、根据特征度是选择，从上至下递归地生成子节点，直到数据集不可分则停止决策树生长。2、剪枝:决策树容易过拟合，需要剪枝来缩小树的结构和规模(包括预剪枝和后萝枝)。3、先剪枝:通过提前停止树的构建而对树萝枝。4、后剪枝:由完全生长的树减去子树\n特征度量选择：ID3采用信息增益大的特征作为分裂属性；C4.5采用信息增益率大的属性分裂，同时避免了ID3信息增益偏向取值较多的属性的缺点，但是其实是偏向于取值较少的特征；CART使用基尼指数克服了C4.5计算复杂度的缺点，偏向取值较多的属性\n场景：ID3和C4.5都只能处理分类问题，CART可以用于分类和回归； ID3与C4.5是多叉树，速度慢，CART是二叉树，计算速度快；ID3没有剪枝策略，C4.5有预剪枝和后剪枝，CART采用CCP代价复杂度后剪枝；\n预剪枝：提前设置好树的深度；后剪枝：用完全生长的树减去子树，在测试集上定义损失函数C，目标是通过剪枝使得在测试集上的C值下降，就是说通过剪枝使在测试集上误差率降低\n后剪枝步骤：\n1.自底向上的遍历每一个非叶节点（除了根节点），将当前的非叶节点从树中减去，其下所有的叶节点合并成一个节点，代替原来被剪掉的节点。 \n2. 计算剪去节点前后的损失函数，如果剪去节点之后损失函数变小了，则说明该节点是可以剪去的，并将其剪去；如果发现损失函数并没有减少，说明该节点不可剪去，则将树还原成未剪去之前的状态。 \n3. 重复上述过程，直到所有的非叶节点（除了根节点）都被尝试了。"
    },
    {
        "title": "决策树-id3",
        "content": "核心思想：选择信息增益最大的属性进行分裂，求信息增益时要先求出信息熵和条件熵，信息增益=信息熵-条件熵 ，信息增益：信息的不确定性减少的程度      信息熵：信息的不纯度，信息熵越大，数据分布越混乱，也就是概率分布越均匀，信息熵也越大；\nID3算法建立在奥卡姆剃刀的思想上，越是小型的决策树越优于大的决策树；算法流程：1、初始化属性集合和数据集合  2、计算数据集合的信息熵，和所有属性的条件熵，然后得出信息增益，选择信息增益最大的属性作为当前决策树的分裂节点  3、更新数据集合合属性集合，也就是删除掉上一步使用的属性，并按照属性值来划分不同分支的数据集合   4、依次对每种取值情况下的子集重复第2步  5、若子集只包含单一属性，则为分支叶子节点，根据其属性值标记  6、完成所有属性集合的划分；\nID3优点：1、概念简单，计算复杂度不高，可解释强，易于理解\n2、数据的准备工作简单，能够同时处理数据型和常规型居性\n3、对中间值的缺失不敏感，比较适合处理有缺失属性值的样本，能够处理不相关特征\n4、可以对很多属性的数据集构造决策树，可扩展性强，可用于不熟悉的数据集，并从中提取一些属性规则\nID3缺点：1、没有剪枝策略，可能会产生过度匹配问题，决策树过深，容易导致过拟合，泛化能力差，因此希要简直\n2、信息增益会对取值较多的属性有所好，也就是作为分类属性\n应用场景：适用于特征取值字段不多的数据集，因为信息增益会对取值较多的属性偏好，更容易选择这种属性作为分类属性"
    },
    {
        "title": "决策树-C4.5算法",
        "content": "ID3算法容易选择那些取值较多的特征来划分，因为根据这些属性划分出的数据纯度较高，比如身份证的例子。而且ID3算法属性只能是离散的，当然属性值也可以是连续的数值型，但是需要对这些数据进行数据预处理，变为离散型的才可以用ID3算法，所以C4.5继承了ID3的优点，改进了它的缺点(信息增益会对取值较多的属性有所偏好，也就是作为分类属性)，并在此基础上做了改进的算法，能够处理属性是连续型的\n信息增益率=信息增益/属性a的一个固有值，当属性a\n改进优化的点：1、用信息增益率代替信息增益来选择特征，克眼了用信息增益选择特征时偏向取值较多的属性的不足\n2、能够完成对连续型数值属性的离散化处理\n3、能处理居性值缺失的情况\n4、在决策树构造完成之后剪枝\n"
    },
    {
        "title": "决策树-CART算法",
        "content": "分类回归树，在ID3的基础上，进行优化的决策树，CART既可以是分类树，也可以是回归树，当作为分类树时，采用基尼指数作为节点的分裂依据；当作为回归树时，采用最小方差作为节点的分裂依据。 CART只能用来建立二叉树\nCART在分类问题中采用基尼值来衡量节点的纯度，节点越不纯，基尼值越大，以二分类为例，如果节点的所有数据只有一个类别，则基尼值为0；此外，CART算法采用基于CCP(代价复杂度)的后剪枝方法"
    },
    {
        "title": "算法评估指标",
        "content": "在机器学习算法中，我们可以将算法分为分类算法、回归算法和聚类算法，当我们建立完模型以后，需要用一定的评估指标来判断模型的优劣\n回归算法评估指标（预测连续型数值问题）：最常用的评估指标是 MSE均方误差（mean squared error），均方误差是描述估计量与被估计量之间差异程度的一种评估指标，也就是预测值与实际值误差平方和的均值\n聚类算法评估：聚类的评价方式从大方向上分为两类，一种是分析外部信息，一种是分析内部信息。外部信息就是可看得见的直观信息，比如聚类完成后的类别号。内部信息就是聚类结束后通过一些模型生成这个聚类的相关信息，比如熵值、纯度这种数学评价指标，常见的聚类评估指标有：互信息法、兰德系数、轮毂系数等。\n轮毂系数，适用于实际类别信息未知的情况。对于单个样本来说，设a为其与类别内各样本的平均距离，设b为其与距离最近的类别内的样本的平均距离，轮毂系数公式：(b-a)/max(a,b)\n分类算法评估指标（预测离散型数值问题）：1、错误率，指预测错误的样本数占总样本数的比例，又被叫做汉明损失。表达式：(FP+FR)/P+R。2、精度，指预测正确的样本数占总样本数的比例，又叫做预测准确率。表达式：(TP+TR)/P+R。3、查准率，又叫做精确率，指在预测为正的样本数中真正正例的比例，表示预测是否分类为1中实际为0的误报率，表达式：TP/(TP+FP)。4、查全率，又叫做召回率，指在所有实际正例样本中预测为正的样本数的比例，表示漏掉了一该被分类为1的，却被分为0的漏报成分。5、f1-score，在理想状态中，模型的查准率和查全率越高越好，但是在现实状况下，查准率和查全率会出现一个升高，一个降低的情况。所以就需要一个能够综合考虑查准率和查全率的评估指标，因此引入F值，F值表达式为：(a^2+1)pr/a^2(p+r)，a为权重系数，当a=1时，F值便是F1值，代表查准率和查全率权重相等，是最常用的一种评估指标。表达式为：f1=2pr/p+r。6、PR曲线，是描述查准率和查全率变化的曲线，以查准率和查全率为纵、横坐标轴，根据学习器的预测结果对测试样本进行排序，将最可能是正例的样本排在前面，最不可能是正例的样本排在后面，按照此顺序，依次将每个样本当作正例进行预测，每次计算P值和R值。7、ROC曲线，与PR曲线类似，都是按照排序的顺序将样本当作正例进行预测，不同的是ROC曲线引进TPR、FPR的概念，FPR是假正例率，TPR是真正例率，ROC曲线以TPR(真正例率)为横轴、FPR(假正例率)为纵轴。ROC更加偏重于测试样本评估值的排序好坏。8、AUC面积，进行模型性能比较时，如果模型A的ROC曲线被模型B的ROC曲线完全包住，那么认为B模型的性能更好。若A和B的ROC曲线有交叉的地方，则比较两个模型ROC曲线与坐标轴围成的面积，AUC被定义为ROC曲线下的面积，面积越大,AUC值越大，模型分类的质量就越好。AUC为1时，所有正例排在负例前面，AUC为0时，所有负例排在正例前面\n备注1：查准率是宁愿漏掉，不可错杀。应用场景：一般应用于识别垃圾邮件的场景中。因为我们不希望很多的正常邮件被误杀，这样会造成严重的困扰。因此在这种场景下，查准率是一个很重要的指标\n2：查全率是宁愿错杀，不可漏掉。应用场景：一般用于金融风控领域，我们希望系统能够筛选出所有风险的行为或用户，然后进行人工鉴别，如果漏掉一个可能会造成灾难性后果。\n3：对于类别不平衡问题，ROC曲线的表现会比较稳定（不会受不均衡数据的影响），但如果我们希望看出模型在正类上的表现效果，还是用PR曲线更好\n4：ROC曲线由于兼顾正例与负例，适用于评估分类器的整体性能（通常是计算AUC，表示模型的排序性能）；PR曲线则完全聚焦于正例，因此如果我们主要关心的是正例，那么用PR曲线比较好"
    },
    {
        "title": "最优化方法-无约束-梯度下降",
        "content": "除了目标函数以外，对参与优化的各变量没有其他函数或变量约束，称之为无约束最优化问题；目标函数f(x)，当对其进行最小化时，也把它称作为代价函数、损失函数或误差函数\n求解方法1直接法:通常用于当目标函数表达式十分复杂或者写不出具体表达式时。通过数值计算，经过一系列迭代过程产生点列，在其中搜索最优点\n求解方法2解析法:根据无约束最优化问题的目标函数的解析式给出一种最优解的方法，主要有梯度下降、牛顿法、拟牛顿法、共轭梯度法和共轭方向法等\n概念：1、方向导数:函数沿任意方向的变化率，需要求得某一点在某一方向的导数即方向导数。2、梯度方向:函数在变量空间中的某一点沿着哪个方向有最大的变化率?最大方向导数方向，即梯度方向。3、梯度:函数在某一点的梯度是一个矢量，具有大小和方向。它的方向与取得最大方向导数的方向一致，而它的模为方向导数的最大值。4、正梯度向量指向上坡，负梯度向量指向下坡。我们在负梯度方向上移动可以减小f(x)，被称为最速下降法或梯度下降法\n一、批量梯度下降：每更新一次权重需要对所有的数据样本点进行遍历。在最小化损失函数的过程中，需要不断反复的更新权重使得误差函数减小；特点：每一次的参数更新都用到了所有的训练数据，因此批量梯度下降法会非常耗时，样本数据量越大，训练速度也变得越慢\n二、随机梯度下降：为了解决批量梯度下降训练速度过慢的问题。它是利用随机选取每个样本的损失函数对sita求偏导得到对应的梯度来更新sita；因为随机梯度下降是随机选择一个样本进行迭代更新一次，所以伴随的一个问题是噪音比批量梯度下降的多，使得随机梯度下降并不是每次都向着整体优化的方向\n三、小批量梯度下降：为了解决前两者的缺点，使得算法的训练过程较快，而且也要保证最终参数训练的准确率。它是通过增加每次送代个数来实现的；每次在一批数据上优化参数并不会比单个数据慢太多，但是每次使用一批数据可以大大减小收敛所需的迭代次数，同时可以使得收敛的结果更加接近批量梯度下降的效果"
    },
    {
        "title": "最优化方法-有约束",
        "content": "等式约束最优化：拉格朗日乘子法是解决等式约束最优化的问题的最常用方法，基本思想就是通过引入拉格朗日乘子来将含有n个变量和k个约束条件的约束优化问题转化为含有（n+k）个变量的无约束优化问题\n不等式约束最优化：大部分实际问题的约束都是不超过多少时间，不超过多少人力等等，因此对拉格朗日乘子法进行了扩展，增加了KKT条件，求解不等式约束的优化问题；使用拉格朗日函数对目标函数进行了处理，生成了一个新的目标函数。通过一些条件可以求出最优值的必要条件，这个条件就是KKT条件。经过拉格朗日函数处理之后的新目标函数，保证原目标函数与限制条件有交点，也就是必须要有解"
    },
    {
        "title": "异常值",
        "content": "定义：异常值是偏离整体样本的观察值，也是偏离正常范围的点\n影响：异常值会影响模型的精度，降低模型的准确性。增加了整体数据方差；异常值是随机分布的，可能会改变数据集的正态分布。增加因此异常值处理是数据预处理中重要的一步，异常检测场景：入侵检测、欺诈检测、安全监测等\n出现原因：1、数据输入错误，相关人员故意或者无意导致数据异常，比如客户年收入13万美元，数据登记为130万美元。2、数据测量，实验误差，比如测量仪器不精准导致数据异常。3、数据处理错误，比如ETL操作不当，发送数据异常。4、抽样错误，数据采集时包含了错误数据或无关数据。5、自然异常值，非人为因素导致数据异常，比如今年某月份的降水量远超前几年同月份降水量。\n异常值检测方法：1、散点图：将数据用散点图可视化出来，可以观测到异常值。2、基于分类模型的异常检测：根据现有数据建立模型，然后对新数据进行判断从而确定是否偏离，偏离则为异常值，比如贝叶斯模型，SVM模型等。3、3sita原则：若数据集服从期望为u，方差为σ^2的正态分布，异常值被定义为其值与平均值的偏差超过三倍标准差的值。4、箱型图分析：箱型图分为上界，下界，上四分位数，下四分位数，以及离群点   四分位数就是将所有数值按从小到大排列并分成四等份，处于三个分割点位置的数值就是四分位数。（上界:上限是非异常范围内的最大值；下界:下限是非异常范围内的最小值；上四分位数:数值排序后处于75%位置上的值；下四分位数:数值排序后处于25%位置上的值）\n异常值处理办法：1、删除异常值 -适用于异常值较少的情况。2、将异常值视为缺失值，按照缺失值的处理方法处理异常值。3、估算异常值，均值、中位数、众数填充异常值"
    },
    {
        "title": "训练集，测试集，验证集合",
        "content": "在训练过程中使用验证数据集来评估模型并更新模型超参数，训练结束后使用测试数据集评估训练好的模型的性能\n训练集：是用来构建机器学习模型，用于模型拟合的数据样本；测试集：用来评估训练好的模型的性能；验证集：辅助构建模型，用于构建过程中评估模型，为模型提供无偏估计，进而调整模型超参数和用于对模型的能力进行初步评估。通常用来在模型迭代训练时，用以验证当前模型的泛化能力（准确率，召回率等），以决定是否停止继续训练\n划分方法：（1留出法）直接将数据集划分为互斥的集合，一个用作训练集，一个用作测试集，且满足训练集U测试集=全集；训练集交测试集=空集  常见的划分为2/3-4/5的样本用作模型训练，剩下的用作测试。通常选择70%的数据作为训练集，30%的数据作为测试集；通常单次使用留出法得到的结果不够稳定可靠，一般采取分层抽样或者若干次随机划分作为留出法的评估结果\n（2自助法）给定包含M个样本的数据集，每次随机从中抽取一个样本，放入新的数据集中，然后从原始样本中有放回的抽取M次，就可以得到包含M个样本的数据集，平均情况下会有63.2%的原始样本出现在数据集中，而剩下的36.8%的原始样本不出现在数据集中，也称.632自助法\n（3交叉验证）将数据集划分为K个大小相同的互斥子集，同样保持数据分布的一致性，即采用分层抽样的方法获得这些子集。目的：在实际训练中，模型的结果通常对训练数据好，但是对训练数据以外的数据拟合程度较差，交叉验证的目的就是用作评价模型的泛化能力，从而进行模型选择\nK-折交叉验证：每次用K-1个子集的并集作为训练集，余下的那个子集作为测试集，这样就会产生K种数据集划分的情况，从而可以进行K次训练和测试，最终返回K次测试结果的均值\nK折交叉验证的K最常用的取值是10，此时为10折交叉验证，常用的还有5、20等\n股用于模型调优，找到使得模型泛化性能最优的超参数，在全部训练集上重新训练模型，并使用独立测试集对模型性能做出最终评价\n如果训练集相对较小，则增大K值：增大K的话，在每次选代过程中将会有更多的数据参与模型的训练，能够得到最小偏差，同时算法时间也会变长，且不同训练集间高度相似，会导致结果方差较高\n如果训练集相对较大，则减小K值：减小K的话，降低模型在数据集上重复拟合的时间和成本，在平均性能的基础上获得模型的准确评估"
    },
    {
        "title": "特征选择",
        "content": "为什么要进行特征选择：背景：现实中大数据挖掘任务，往往属性特征过多，而一个普遍存在的事实是，大数据集带来的关键信息之聚集在部分或少数特征上，因此需要从中选择出重要的特征使得后续建模过程只在一部分特征上建立，减少维数灾难出现的可能，同时去除不相关的特征，留下关键因素，降低学习的任务难度，更容易挖掘数据本身带有的规律，同时在特征选择的过程中，会对数据的特征有更充分的理解\n如何进行特征选择：当数据预处理完成后，需要选择有意义的特征进行模型训练，通常从三方面考虑：1、特征是否发散:如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本没有差异，这个特征对样本的区分作用不明显，区分度不高。2、待征之间的相关性:特征与待征之间的线性相关性，去除相关性高的特征。3、特征与目标之间的相关性:与目标相关性高的特征，应当优先选择\n三种常见特征选择方法对比：1、过去由于和特定的学习器无关，计算开销小，泛化能力强于后两种特征选择方法，因此，在实际应用中由于数据集很大，特征维度高，过港式特征选择应用的更广泛些，但它是通过一些统计指标对特征进行排字来选择，通常不能发现冗余。2、从模型性能的角度出发，包装法的性能要优于过考去，但时间开销较大。3、嵌入法中的L正则化方法对于特征理解和特征选择来说是非常强大的工具，它能够生成稀疏的模型，对于选择待征子集来说非常有用。4、嵌入法中的随机森林方法是当前比较主流的方法之一，它易于使用，一般不需要其他特征工程操作、调参等繁琐的步亲，有直接的工具包都提供平均不纯堂下降方法，它的两个主要问题：一是重要的特征有可能得分很低，二是这种方法对特征变量类别多的特征有利。"
    },
    {
        "title": "特征选择-Filter过滤法",
        "content": "按照特征发散性或者相关性对各个特征评分，设定阈值或待选择阈值的个数，选择特征；总的缺点是：若特征之间具有强关联，且非线性时，Filter方法不能避免选择的最优特征组合冗余。   Filter总结：利用不同的打分规则，对每个特征进行打分，相当于给每个特征赋予权重，按权重排序，对不达标的特征进行过滤\n1、方差选择法：方差越大的特征，对于分析目标影响越大，就越有用；如果方差较小，比如小于一，那么这个特征可能对算法的作用就比较小；如果某个特征方差为0，即所有的样本在这个特征上的取值都是一样的，那么对模型训练没有任何作用，可以直接舍弃（特征的方差越大越好）    ；实现方法：设定一个方差的阈值，当方差小于这个阈值的特征就会被删除；适用场景：只适用于连续变量。\n2、相关系数法（皮尔逊相关系数）：该方法衡量的是变量之间的线性相关性，两个变量之间的皮尔逊相关系数定义为两个变量之间的协方差与标准差的商，结果的取值区间为【-1，1】，-1表示完全的负相关，+1表示完全正相关，0表示没有线性相关    实现方法：R为相关性，P为显著性，首先看P值，P值用于判断R值，即相关系数有没有统计学意义，判断标准一般为0.05，当P值>0.05时，则相关性系数没有统计学意义，此时无论R值大小，都表明两者之间没有相关性；当P值<0.05，则表明两者之间有相关性，此时再看R值，R值越大相关性越大，正数则正相关，负数就是负相关   ；适用场景：适用于特征类型均为数值特征的情况；缺陷：只对线性关系敏感，如果特征与响应变量的关系是非线性的，即便两个变量具有一 一对应的关系，相关系数也可能会接近0。建议最好把数据可视化出来，以免得出错误的结论。\n3、卡方检验(𝜒^2检验)：它可以检验某个特征分布和输出值分布之间的相关性，𝝌^𝟐值描述了自变量与因变量之间的相关程度，𝜒^2值衡量实际值与理论值的差异程度，𝜒^2值越大，相关程度也就越大   实现方法：1、计算无关性假设(随机抽取一条实际值计算)   2、根据无关性假设生成新的理论值四格表   3、根据计算公式算出𝜒^2   4、计算该相依表的自由度，查询卡方分布的临界值表来判断𝜒^2值是否合理(需要用100%-卡方分布临界表的值才能得出相关性)\n4、互信息法：互信息表示两个变量是否有关系以及关系的强弱。互信息可以理解为，X引入导致Y的熵减小的量，从信息熵的角度分析特征和输出值之间的关系评分；互信息值越大，说明该特征和输出值之间的相关性越大，越需要保留；缺陷：它不属于度量方式，也没有办法归一化，在不同数据集上的结果无法做比较    2、对于连续变量通常需要先离散化，而互信息的结果对离散化的方式敏感\n"
    },
    {
        "title": "特征选择-wrapper包装法",
        "content": "根据目标函数(通常是预测效果评分)，每次选择若干特征，或者排除若干特征\n常用技术 -- 递归特征消除法RFE；\n使用一个基模型来进行多轮训练，每轮训练后，移除若干权值系数的特征，在基于新的特征集进行下一轮训练。  步骤：1、指定一个有N个特征的数据集 2、选择一个算法模型  3、指定保留特征的数量K(K<N)  4、第一轮对所有特征进行训练，算法会根据基模型的目标函数给出每个特征的评分或排名，将最小得分或排名的特征移除，这时候特征减少为N-1，对其进行第二轮训练，持续迭代，直到特征保留为K,这K个特征就是选择的特征\nRFE的稳定性很大程度上取决于在迭代时底层所采用的模型。例如，假如RFE采用的普通的回归，没有经过正则化的回归是不稳定的，那么RFE就是不稳定的；假如采用的是Ridge，而用Ridge正则化的回归是稳定的，那么RFE就是稳定的。"
    },
    {
        "title": "特征选择-Embedded嵌入法",
        "content": "先使用某些机器学习的算法或模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征\n（1基于L1正则化方法）：1、L1正则化将回归系数的L范数作为惩罚项加到损失函数上，由于正则化非0，这就使得那些弱的待征所对应的系数变为0，因此比L1正则化往往会使学习到的模型很保统(系数w经常为0)这个特性使得L1正则化成为一种很好的特征选择的方法。2、L1正则化像非正则化线性模型一样也是不稳走的，如果待征集合中具有相关联的持征，当数据发生轻微变化时，也有可能导致很大的差异。3、L1正则化能够生成稀疏的模型。4、L1正则化可以产生稀疏权值短阵，即产生一个稀疏模型，可以用于特征选择，也可以防止过拟合\n数据稀疏性说明:L1正则化本来就是为了降低过拟合风险，但是L1正则化的结果往往会得到稀疏数据，即L1方法选择后的数据拥有更多0分量，所以被当作特征选择的强大方法。稀疏数据对后续建模的好处:数据集表示的矩阵中有很多列与当前任务无关，通过特征选择去除这些列，如果数据備疏性比较突出，意味着去除了较多的无关列，模型训练过程实际上可以在较小的列上进行，降低学习任务的难度，计算和存储开销小，\n（2基于树模型方法）：基于树的预测模型能够用来计算特征的重要程度，因此能用来去除不相关的特征，随机森林具有准确率高，稳定性强、易于使用的优点，是目前最流行的机器学习算法之一，随机森林提供的特征选择方法：平均不纯度减少和平均精度下降。（一）、平均不纯度减少：1、随机森林由多个决策树构成，决策树的每一个节点都是关于某个特征的条件，目的时将数据集按照不同的取值一分为二。对于一个决策树森林来说，可以算出每个特征平均减少了多少不纯度，并把它平均减少的不纯度作为特征选择的值  2、若特征之间存在关联，一旦某个特征被选择后，其他特征的重要度就会急剧下降，而不纯度已经被选中的那个特征降下来，其他特征就很难在降低那么多不纯度。在理解数据时，容易错误的认为先被选中的特征是很重要的，而其余的特征是不重要的，但实际上这些特征对响应变量的作用可能非常接近；。（二）、平均精度下降：该方法直接度量每个特征对模型的精确率的影响，基本思路是打乱每个特征的特征值顺序，并且度量顺序变动对模型精确率的影响，因为对于不重要的变量，打乱其顺序对模型的准确率影响不会太大，对于重要的变量，打乱顺序就会降低模型的准确率；在特征选择上，需要注意：尽管在所有特征上进行了训练得到模型，然后才得到每个特征的重要性测试，这并不等于筛选掉某个或某几个重要特征后模型的性能就一定会下降很多，因为即便删掉某个特征后，其关联特征一样可以发挥作用，让模型性能不变"
    },
    {
        "title": "特征处理-特征缩放(数值归一化)",
        "content": "特征缩放可以提高模型的精度和模型的收敛速度，在实际业务中，当数据的量纲不同、数量级差距大时，会影响最终的模型，因此需要用到特征缩放\n1、标准化（常用）：概念:将训练集中的某一列特征缩放到均值为0，方差为1的状态；特点:标准化对数据进行规范化，去除数据的单位限制，将数据转换为无量纲的纯数值，便于不用单位的数据进行比较和加权，同时不改变数据的原始分布状态，标准化要求原始数据近似满足高斯分布，数据越接近高斯分布，标准化效果越佳；适用场景:数据存在异常值和较多的噪音值\n2、最小值-最大值归一化：概念:将训练集中某一列特征数值缩放到-1到1，或0到1之间；特点:受训练集中最大值和最小值影响大，存在数据集中最大值与最小值动态变化的可能；适用场景:数据较为稳定，不存在极端的最大值和最小值\n3、均值归一化：公式：(x-x均值)/max(x)-min(x)\n4、缩放成单位向量：公式：x/||x||\n5、基于树的方法是不需要特征归一化或标准化，比如随机森林，bagging 和 boosting等。基于参数的模型或基于距离的模型，需要特征归一化或标准化。\n"
    },
    {
        "title": "特征处理-数值离散化",
        "content": "概念:把无限空间中的有限个体映射到有限空间中去，提高算法的时空效率，简单讲就是在不改变数据相对大小的情况下，对数据进行相应缩小;离散化仅适用于只关注元素之间的大小关系而不关注元素数值本身的情况 在数据挖掘理论研究中，研究表明离散化数值也能在提高建模速度和提高模型精度上有显著作用；\n作用:离散化可以降低特征中的噪音节点，提升特征的表达能力但是离散化的过程都会带来一定的信息丢失\n（一）、数值变量分离散变量和连续变量\n连续变量中的有监督连续变量离散方法包括1R（把连续的区间分成小区间，然后根据类标签对区间内变量调整）、基于信息熵（自顶向下的方法，运用决策树理念进行离散化）、基于卡方（自底向上的方法，运用卡方检验的方法，自底向上合并数值进行有监督离散化，核心操作是merge）；\n连续变量中的无监督连续变量离散方法包括聚类划分（使用聚类算法将数据分为K类，需要制定K值的大小）；分箱-数据先排序（1等宽划分-把连续变量按照相同的区间间隔划分成几等份，也就是根据数值的最大值和最小值进行划分，分为N份，每份的数值间隔相同；2等频划分-把连续变量划分成几等份，保证每份数值个数相同）\n（二）、分类变量分有序分类变量和无序分类变量\n有序分类变量离散化方法包括Label-Encoding（有序分类变量数值之间存在一定的顺序关系，可直接使用划分后的数据进行数据建模。优点:解决了分类变量的编码问题。缺点:可解释性差）\n无序分类变量离散化方法包括独热编码（使用M位状态寄存器对M个状态进行编码，每个状态都有独立的寄存器位，这些特征互斥，所以在任意时候只有一位有效，也就是说这M位状态中只有一个状态位值为1，其他都是0，换句话说就是M个变量用M维表示，每个维度的数值为1或为0。；优点:解决了分类器不好处理分类变量的问题。；缺点:分类变量不宜过多，可能会造成稀疏矩阵）哑编码（哑编码和独热编码类似，唯一区别就是哑编码采用M-1位状态寄存器对M个状态进行编码。；优点:解决了分类器不好处理分类变量的问题。；缺点:分类变量不宜过多，可能会造成稀疏矩阵）"
    },
    {
        "title": "特征处理-特征编码",
        "content": "数据挖掘中，一些算法可以直接计算分类变量，比如决策树模型，但许多机器学习算法不能直接处理分类变量，他们的输入和输出都是数值型数据，因此把分类变量转换成数值型数据是必要的，可用独热编码和哑编码实现\n应用场景1、对逻辑回归中的连续变量做离散化处理，然后对离散特征进行独热编码或者哑编码，这样会使模型具有较强的非线性能力\n应用场景2、对于不能处理分类变量的模型，必须要先使用独热编码或哑编码，将变量转换成数值型；但若模型可以处理分类变量，那就无须转换数据，如树模型"
    },
    {
        "title": "用户画像",
        "content": "用户画像：即用户信息标签化，就是收集这个用户的各种行为和数据，从而分析得到这个用户的一些基本的信息和典型特征，最后形成一个人物原型，从中挖掘用户价值，从而提供业务推荐、精准营销等服务\n用户画像构建流程：1、收集用户相关数据：可以来源于网站上用户交易数据，用户日志数据，用户行为数据等；。\n2、数据预处理：对收集到的用户数据做一些预处理工作，包括检查数据是否完整，即数据是否含有缺失值，数据是否含有异常值以及数据是否存在不均衡现象，从而决定是否对缺失值填充，或者删除异常值，对不均衡数据进行重采样等数据清洗工作。最终将杂乱无章的数据转化为结构化数据，即我们后续机器学习模型可以识别的数据格式。；。\n3、用户行为数据建模：对于不携带标签的用户数据，也就是我们事先并不知道该用户属于哪一类别，或者说他和哪些用户行为数据比较相似，此时我们可以采用无监督学习中的聚类算法来对它们进行聚类，相似相近、关联性大的用户数据放在一起，不相似不相近，关联性不大的用户不放在一起，可以用这些聚类算法，针对不同形状的数据进行相应的聚类，此时聚类结束后，我们可以用相应的聚类评价指标来评价我们聚类模型性能的优劣，或者将聚类结果与专家给出的标准做对比，将聚类结果回归到真实商业逻辑上。对聚类后不同类别的用户进行打标签，从而得到带有标签的用户数据，对于携带标签的数据，我们接着选择机器学习中有监督学习的相关算法来对这些有标签的数据进行统计分析。\n4、举例：例如某银行推行一种信用卡，该银行的目标就是想知道哪些客户群体会办理信用卡，哪些客户群体不会办理信用卡，所以我们可以通过收集关于用户办理信用卡相关的数据，比如年龄、性别、婚姻状况、名下是否有房产、月收入年收入、之前是否办理过信用卡等等信息，通过聚类算法将用户分为三类群体，即会办理、不会办理以及未知三种客户群体，银行可据此向会办理信用卡的客户群体推送办卡相关业务以及活动福利，实现业务推荐和精准营销。最终我们可以通过现有的携带标签的用户数据，构建机器学习分类模型，例如LR、SVM等算法模型，然后通过该模型对未知用户进行预测，预测该用户是否会办理信用卡"
    },
    {
        "title": "无监督-聚类",
        "content": "无监督：是指在未加入标签的数据中，根据数据之间的属性特征和关联性对数据进行区分，相似相近、关联性大的数据放在一起，不相似不相近的、关联性不大的数据不放在一起；无监督本质：利用无标签的数据去学习数据的分布和数据与数据之间的关系；无监督算法包括如聚类算法、关联算法。\n分类：基于原型（K-means算法、K-means++算法、k-mediods算法）基于层次（HierarchicalClustring算法、Birch算法）基于密度（DBSCAN算法）\n（1）、K-means算法：思想：输入聚类个数K，已经包含n个数据样本的数据集，输出标准的K个聚类的算法，然后将n个数据样本划分为K个聚类，最终结果所满足：同一聚类中数据相似度较高，而不同聚类的数据相似度低；步骤：1、随机选取K个对象作为初始质心  2、计算样本到K个质心的欧氏距离，按就近原则将它们划分到距离最近的质心所对应的类中   3、计算各类别中所有样本对应的均值，将均值作为新的质心，计算目标函数  4、判断聚类中心或目标函数是否改变，若不变则输出，若改变，则返回2；；；；；\nK-means算法核心问题：K值如何选取：1、人工指定：多次选取K值，选择聚类效果最好的K值。2、均方根：假设我们有m个样本，该方法认为K=根号下m/2。3、枚举法：计算类内距离均值与类间距离均值之比，选择最小的K值，将所有K值进行二次聚类，选择两次聚类结果最相似的K值。4、手肘法：随着K值得增大，样本划分越来越精细，聚类得程度越来越高，误差平方和SSE便逐渐减小(误差平方和是所有样本的聚类误差，代表聚类效果的好坏，SSE越小越好)，当K小于真实聚类数时，随着K值的增大会大幅增加每个簇的聚合程度，SSE的下降幅度也会很大，当K等于真实聚类数时，随着K值的继续增大，聚合程度也会迅速减小，SSE的下降幅度便会骤减，然后随着K的继续增大而趋于平缓，所以K值和SSE的关系图就像一个手肘的图形，肘部对应的值便是数据真实聚类的个数\n（2）、K-means++：背景：为了解决K-means初始质心敏感的问题。；技术原理：不同于K-means算法是第一次随机选取K个样本作为初始质心，K-menas++是假设已经选取了p个初始质心，只有在选取第p+1个质心时，距离这p个质心越远的点会有更高的概率当选为第p+1个聚类中心(为了避免异常点的存在，第二个点的选择会从距离较远的几个点中通过加权选取第二个点)，只有在选取第一个聚类中心时是随机选取(p=1)，该方法的改进符合一般直觉：聚类中心之间距离的越远越好\n（3）、K-mediods：能够避免数据中异常值的影响；算法步骤：1、随机选取一组样本点作为中心点集  2、每个中心点对应一个簇 3、计算各样本点到各个中心点的距离，将样本点放入距离最近的中心点的类中。  4、计算各簇距簇内各样本点的距离绝对误差最小的点，作为新的聚类中心    5、如果新的聚类中心与原中心点相同，则过程结束，如果不同，则返回2\nK-mediods与k-means算法对比：1、算法流程基本一致。；2、质心的计算方式不同:k-means算法是将所有样本点对应的均值作为新的中心点，可能是样本中不存在的点 K-mediods是计算簇内每一个点到簇内其他点的距离之和，将绝对误差最小的点作为新的聚类中心，质心必须是某个样本点的值。；3、k-mediods可以避免数据中异常值带来的影响。；4、质心的计算复杂度更高:k-means直接将均值点作为新的聚类中心，而k-mediods需要计算簇内任意两点之间的距离，在对每个距离进行比较狭取新的质心，计算复杂度增加，速度变慢。；5、稳定性高，执行速度变慢:在具有异常值的小样本数据集中，k-mediods算法比k-means算法效果好，但是随着数据集规模的增加，k-mediods算法执行的速度会很慢，所以如果数据集本身不存在很多的异常值的话，就不用k-mediods代者k-means。；\n（4）、Hierarchical Clustering算法：思想:确保距离近的样本落在同一个族中\n步骤:1、每个样本点都作为一个簇，形成族的集合C 。2、将距离最近的两个簇合并，形成一个簇3、从C中去除这对簇。 4、最终形成层次树形的聚类结构树形图（判断两个簇之间的距离方法：1单链接 -- 不同两个簇之间最近的两个点的距离；2全链接-- 不同两个簇之间最远的两个点的距离；3均链接 -- 不同两个簇中所有点两两之间的平均距离）\n优点:可排除噪声点的干扰，但有可能和噪声点分为一簇 2、适合形状不规则，不要求聚类完全的情况 3、不必确定K值，可根聚类结果不同有不同的结果 4、原埋简单，易于理解\n缺点:计算量很大，耗费的存储空间相对于其他几种方法要高。 2、合并操作不能撤销 3、合并操作必须有一个合并限制比例，否则可能发生过度合并导致所有分类中心聚集，造成聚类失败\n适用场景:适合形状不规则，不要求聚类完全的情况\n（5）、Birch：使用聚类特征三元组表示一个簇的有关信息，而不用且体的一组点来表示该簇，通过构造满足分支因子和簇直径限制的聚类特征树来进行聚类三元组(数据点样本个数，数据点样本特征之和，数据点样本特征平方和)，分支因子:树的每个节点的样本个数，簇直径:一类点的距离范国\n算法步骤:1、扫描数据，建立聚类特征树 2.使用某种算法对聚类特征树的叶节点进行聚类\n优点:一次扫描就能进行很好的聚类\n缺点:要求是球形聚类，因为CF树存储的都是半径类的数据，都是球形才适合\n适用场景:因为Birch算法通过一次扫描就可以进行比较好的聚类，所以适用于大数据集，而且数据的分布呈凸型以及球形的情况，并且由于Birch算法需要提供正确的器类个数和簇直径限制，对不可视的高维数据不可行\n（6）、DBSCAN：一个聚类可以由其中任何核心对象唯一确定，该算法利用基于密度的概念，要求聚类空间中某一区域内的样本个数不小于某一给定闻值，该方法能够在具有噪声的空间数据库中发现任意形状的簇，可将密度足够大的相邻区域连接，能够有效处理异常数据，主要用于对空间教据的聚类。\n步骤:1、DBSCAN通过检查数据中每个样本的eps邻域来搜索簇，如果点p的eps邻域内包含的样本个数大于给定的闽值，那么就建立一个以点P为核心对象的簇。\n2、然后DBSCAN送代的聚集这些核心对象直接密度可达的对象，这个过程可能还会涉及密度可达簇的合并\n3、当没有新的样本点加入到簇中时，过程结束"
    },
    {
        "title": "数据属性",
        "content": "标称属性：标称，意味着与名称有关，标称属性的值是一些符号或事物的名称，每个值代表事物的某种类别、状态或编码。因此标称属性又被看做是分类的。标称属性的值不具有有意义的顺序，而且是不定量的。也就是说给定一个数据集，找出这种属性的均值没有意义;举例：头发颜色和婚姻状况，头发的颜色={黑色、棕色、红色、白色}；婚姻状况={单身、已婚、离异、丧偶}；其次还有职业，具有教师、程序员、农民等\n二元属性:二元属性是一种标称属性，只有两种状态，用来描述事物的两种状态，用值0或1来体现，0表示该属性不出现，1表示出现。二元属性又称布尔属性，如果两种状态对应的是True和False;对称的二元属性(两种状态具有相等的价值，携带相同的权重，例如性别，具有男女两种状态)非对称的二元属性(其状态的结果不是同等重要的，例如艾滋病化验结果的阳性和阴性，用1对最重要的结果(HIV阳性)编码，而另一个用0编码(HIV阴性))\n序数属性:其可能的值之间具有有意义的序或秩评定，但是相继值之间的差是未知的，也就是对应的值有先后顺序。序数属性可以把数量值的值域划分成有限个有序类别。（如0-很不满意，1-不满意，2-满意，3-很满意），把数值属性离散化得到。可以用众数和中位数表示序数属性的中心趋势，但不能定义均值.;举例：成绩={优、良、中、差}；    drink_size表示饮料杯的大小：大、小、中。\n数值属性:是定量的，即它是可以度量的量，用整数或实数值表示;(1)区间标度属性:用相等的单位尺度度量，区间标度属性的值有序，可以评估值之间的差，但不能评估倍数，没有固有的零点(也就是说0°C并不是指现在没有温度)。举例：摄氏温度、华氏温度，日历日期。不能说2020年是1010年的两倍，两者的差有意义，但是比值没有意义；(2)比率标度属性:具有固有零点的数值属性。比率标度属性的值有序，可以评估值之间的差，也可以评估倍数.举例：开氏温度、重量、高度、速度、货币量。拿货币来说，可以说200元比100元多100元，也可以说200元是100元的两倍\n离散属性：具有有限或无限可数个值，可以用或不用整数表示。例如，属性customer_ID是无限可数的。顾客数量是无限增长的，但事实上实际的值集合是可数的（可以建立这些值与整数集合的一一对应）举例：邮编、省份数目\n连续属性:如果属性不是离散的，则它是连续的。属性值为实数，一般用浮点变量表示。\n总结：（1）标称、二元、序数属性都是定性的，他们只描述对象的特征，而不给出实际大小和数值；（2）标称、二元属性的中心趋势可以用众数度量。序数属性的中心趋势可以用它的众数和中位数度量，但不能定义均值（3）所有属性都能用中心趋势来表述。标称、二元属性用众数度量；序数属性用众数、中位数度量。均值是数值属性的中心趋势描述"
    },
    {
        "title": "降维-PCA，奇异值、主成份分析",
        "content": "思想：降维就是将事物的特征进行压缩和筛选，将原始高维空间中的数据映射到低维空间中，该项任务比较抽象，如果没有特定领域的知识，很难事先决定采用哪些数据。比如在人脸识别任务中，如果直接采用图片的原始像素信息，那么数据的维度是非常大的，所以此时就要用到降维的方法，对图片信息进行处理，选出区分度最大的像素组合\n1、SVD（奇异值分解）：1、奇异值分解可以适用于任意矩阵的一种分解方法 2、奇异值分解可以发现数据中隐藏的特征来建立矩阵行列之间的关系 3、奇异值分解能够发现矩阵中的几余，并提供用于消除它的格式\n优点:原理简单，仅涉及简单的矩阵线性变换知识。可以有效处理数据噪音，矩阵处理过程中得到的三个短阵也具有物理意义\n缺点:分解出的矩阵可解释性差，计算量大\n应用场景:广泛的用来求解线性最小平方、最小二乘问题，低秩通近问题，数据压缩问题，应用于推荐系统(找到用户没有评分的物品，经过SVD压缩后低维空间中，计算未评分物品与其他物品的相似性，得到一个预测打分，再对这些物品评分从高到低排序，返回前N个物品推荐给用户)\n2、PCA（主成份分析法）：思想:寻找表示数据分布的最优子空间(降维，去掉线性相关性)，就是将n维特征映射到k维上(k<n)，k维是全新的正交特征(k维特征称为主成份，是重新构造出来的k维特征，而不是简单的从n维特征中去除n-k维特征)，PCA目的是在高维数据中寻找最大方差的方向，然后将原始数据映射到维数小的新空间中。数学原理:根据协方差矩阵选取前s个最大特征值所对应的特征向量构建映射矩阵，进行降维\n优点:1、方差衡量的无监督学习，不受样本标签的限制 2、各主成份之间正交，可消除原始数据中各特征之间的影响 3、计算方法简单，主要运算是奇异值分解，容易在计算机上实现\n缺点:主成份解释某含义具有一定的模糊性，不如原始样本特征解释性强 2、方差小的非主成份也可能含有重要信息，因降维丢弃后可能对后续数据处理产生影响\n算法流程：1、对所有样本构成的矩阵X去中心化2、求X的协方差矩阵C   3、利用特征值分解，求出协方差矩阵C的特征值和特征向量      4、取前s个最大特征值对应的特征向量构成变换矩阵W    5、用原数据集和变换矩阵W相乘，得到降维后的新数据集（矩阵）\n追问:详细介绍PCA：在PCA中，数据从原始坐标系转换到新的坐标系中，转换坐标系时，选择数据方差最大的方向作为坐标轴的方向，（方差最大的方向给出了数据最重要的信息），第一个坐标轴的方向是方差最大的方向，第二个新坐标轴的方向是与第一个新坐标轴正交且方差次大的方向，重复过程N次，N是数据原始维度，通过这种方式可以获得新的坐标系，其中大部分方差包含在前几个坐标轴中，而后几个坐标轴中方差基本为0，这时可以忽略后面的坐标轴，只保留前面几个包含大部分方差的坐标轴；通过计算协方差矩阵，可以得到协方差矩阵的特征值和特征向量，选取前几个最大特征值所对应的特征向量组成变换矩阵，就可以将矩阵转换到新的坐标系中，实现数据的降维\n3、LDA（线性判别分析）：思想:寻找可分性判据最大的子空间，即投影后类内间距最小，类间距离最大 2、用到了fisher的思想，即寻找一个向量，可以使得类内散度最小，类间散度最大，其实也就是选取特征向量构建映射矩阵，然后对数据进行处理，该方法能使投影后样本的类间散步矩阵最大，类内散步矩阵最小\n优点:降维过程中可以使用类别的先验经验 2、LDA在样本分类信息依赖均值而不是方差的时候，比PCA之类的算法更优\n缺点:LDA不适合对非高斯分布的样本进行降维，PCA也有这个问题 2、LDA在样本分类信息依赖方差而不是均值的时候，降维效果不好 3、LDA可能过度拟合数据 4、LDA降维最多降到类别数k-1的维数，如果我们降维的维度大于k-1，则不能使用LDA\n4、LLE（局部线性嵌入）：相比较于传统的PCA、LDA这种只关注样本方差的降维方法，LLE在保持降维效果的同时也保留了样本局部的线性特征，因为其保留了局部的线性特征，所以常被应用在高维数据可视化、图像识别等领域\n5、LDA VS PCA：共同点：1、都属于线性方法。2、在降维时都采用矩阵分解的方法。3、假设数据符合正态分布。；不同点：1、LDA是有监督的，不能保证投影到新空间的坐标系是正交的，选择分类性能最好的投影方向。2、LDA可以用于降维也可以用于分类。3、LDA降维保留个数与其对应的类别数有关，与数据本身维度无关\n奇异值分解定义：将矩阵分解为奇异向量和奇异值，可以将矩阵A分解为三个矩阵的乘积，即A=UDV^t，其中矩阵U和V为正交矩阵，U的列向量为左奇异向量，V的列向量为右奇异向量，D矩阵的对角线上的值为矩阵A的奇异值，D矩阵为对角矩阵，奇异值按大小进行排序，对角线之外的值为0\n奇异值分解应用：1、在机器学习和数据挖掘领域，奇异值的应用很广泛，比如应用在用于降维的PCA(主成份分析法)和LDA(线性判别分析法)，数据压缩（以图像压缩为代表）算法，还有做搜索引擎语义层次检索的LSI。；2、求矩阵伪逆：奇异值分解可以被应用于求矩阵的伪逆，若矩阵M的奇异值分解为M=UDV^t，那么M的伪逆为M=VD^+U^t，其中D+为D的伪逆，并将其主对角线上非零元素求倒数在转置得到的。求伪逆通常可以用来求解线性最小平方、最小二乘法问题。；3、矩阵近似值：奇异值分解在统计中的主要应用为PCA（主成分分析），一种数据分析的方法，用来发现大量数据中的隐含模式，可以用于模式识别，数据压缩等方面.PCA方法的作用是将数据集映射到低维空间中的坐标系中，数据集的特征值(可以用奇异值来表征)按照重要性排列，降维的过程其实就是舍弃不重要的特征向量，而剩下的特征向量组成的空间就是降维后的空间。；4、平行奇异值：用于频率选择性的衰落信道的分解"
    },
    {
        "title": "数据挖掘流程",
        "content": "数据挖掘是通过对大量的数据进行分析，以发现和提取隐含在其中的具有价值的信息和知识的过程。跨行业数据挖掘标准流程，是当今数据挖掘业界通用流行的标准之一，它强调数据挖掘技术在商业中的应用，是用以管理并指导Data Miner有效、准确开展数据挖掘工作获得最佳挖掘成果的一系列工作步骤的规范标准\n1、商业理解：从商业角度理解项目的目标和要求，然后把理解转化为数据挖问题的定义和一个旨在实现目标的初步计划。确定业务目标：分析项目背录，从业务角度分析项目的需求和目标，确定业务角度成功标准。；项目可行性分析：分析现有资源、条件和限制，风险估计、成本和效益估计。；确定数据控掘目标：明确数据挖狂的目标和成功标隹。；提出项目计划：对整个项目做出计划，初步估计用到的工具和技术。\n2、数据理解：开始于原始数据的收集，然后是熟悉数据，表明数据质量问题，探索数据进而对数据初步理解，发掘有趣的子集以形成对隐藏信息的假设。收集原始数据：收集项目所涉及到的数据，如有必要，将数据传入数据处理工具中，并做一些初步数据集成的工作，生成相应报告。；描述数据：对数据做一些大致描述，例如属性数、记录数等，生成报告。；探索数据：对数据做简单的统计分析，例如关键属性的分布。；检查数据质量：包括数据是否完整、是否有错误，是否会有缺失值等\n3、数据准备：从原始未加工的数据构造最终数据集的过程(这些数据指将要嵌入建模工具中的数据)，数据准备任务可能要实施多次，并且没有任何规定的顺序。这些任务包括表格、属性和记录的选择以及按照建模工具要求，对数据进行转换、清洗等。；数据选择：根据数据挖掘目标和数据质量选择合适数据，包括表格选择、属性选择和记是选择。；数据清洁：对选择出的教据进行清洁，提高数据质量，例去除噪音、填充缺失值等。；数据创建：在原始数据上生成新的记录或属性。；数据合并：利用表连接等方式将几个数据集合并在一起。；教据格式化：将数据转化为数据挖掘处理的格式。\n4、建立模型：选择和应用各种建模技术，同时对他们的参数进行调整，以达到最优值。选择建模技术 -- 确定数据挖症算法模型和参数；测试方案设计：设计某测试模型的质是和有效性机制模型训练：在准备好的数据集上运行数据挖掘算法，得出一个或多个模型模型测试评估：根据测试方案，从数据挖狂技术角度确定数据挖掘目标是否成功\n5、模型评估：更为彻底的评估模型和检查建立模型的各个步骤，从而确保它真正的达到了商业目标。结果评估：从商业角度评估得到的模型，甚至实际试用该模型测试其效果；过程回顾：回顾项目的所有流程，确定每一个阶段都没有错误；确定下一步工作：根据结果评估和过程回顾得出结论，确定部晋该控掘模型还是从某个阶段重新开始\n6、模型实施：实施计划 ：在业务运作中部署模型做出计划；监控和维护计划：如何监控模型在实际业务中的使用情况，如何维护该模型；作出最终报告 ：项目总结，项目经验和项目结果；项目回顾：回顾项目的实施过程，总结经验教训;对数据挖握的运行效果做一个预测"
    },
    {
        "title": "数据采集抽样方法",
        "content": "简单随机抽样：先将调查总体进行编号，然后根据抽签法或者随机数字表抽取部分所观察到的数据组成样本数据，分为有放回抽样和无放回抽样。常被应用于压缩数据量以减少费用和时间开销\n系统抽样：又被称为等距抽样，首先设定抽样间距为n，然后从前n个数据样本中抽取初始数据，然后按照顺序每隔n个单位抽取一个数据组成样本数据\n分层抽样：将总体数据按照特征划分为若干层次或类型，然后从各个类型和层次的数据中采用简单随机抽样或者系统抽样抽取子样本，最终将这些子样本组合为总体数据样本，常被应用于离网预警模型和金融欺诈模型等严重有偏的数据\n整群抽样：将总体数据按照属性拆分为互不相交、互不重复的群，这些群中的数据尽可能具有不同属性，尽量能代表总体数据的信息，然后以群为单位进行抽样"
    },
    {
        "title": "数据清洗",
        "content": "定义：数据清洗是指通过删除、转换、组合等方法或策略清洗数据中的异常样本，为数据建模提供优质数据的过程；场景：不均衡数据处理、缺失值处理、异常值处理\n1.不均衡数据处理：类别数据不平衡是分类任务中出现的经典问题，一般在数据清洗环节进行处理，不均衡简单来讲就是数据集中一个类别的数据远超其他类别数据的数据量，比如在1000条用户数据中，男性数据占950条，女性数据只占50条；处理办法1重采样数据（过采样：对少的一类进行重复选择，欠采样：对多的一类进行少量随机选择）2.K-fold交叉验证3.一分类4.组合不同的重采样数据集（核心原理：建立N个模型。    假设稀有样本有100个，然后从丰富样本中抽取1000(100*10)个数据，将这1000个分为N份，分别和100个稀有数据合并建立模型）\n2.异常值处理：异常值指偏离正常范围的值，不是错误值，异常值出现频率较低，但又会对实际项目分析造成影响；检测方式：异常值一般通过箱型图或分布图来判断；处理办法：采取盖帽法或者数据离散化   2、删除异常值、使用与异常值较少的时候  3、将异常值视为缺失值，按照缺失值的处理方法处理   4、估算异常值，mean/mode/median\n3.缺失值处理：数据缺失产生原因：1、人为疏忽、机器故障等客观原因造成数据缺失   2、人为故意隐瞒部分数据，比如在数据表中有意将一列属性视为空值，此时缺失值可以被看作为特殊的特征值   3、数数据本身不存在，例如银行在做用户信息收集时，对于学生群体来说，薪资这一列就不存在，所以在数据集中显示为空值   4、系统实时性要求较高  5、历史局限性导致数据收集不完整；；；.缺失值处理方法包括1.删除（适用场景：数据量大，缺失值少的数据集，完全随机缺失时可以直接使用删除操作）2.填充。3.不处理（补齐得缺失值毕竟不是原始数据，所以不一定符合客观事实，数据填充在一定程度上改变了数据的原始分布，也不排除加入噪音点的可能，因此对于一些无法容忍缺失值的模型可以进行填充，但有些模型本身可以容忍一定的数据缺失，此时选用不处理的方式，比如XGboost模型）\n缺失值填充方法包括如下4种：（1）.数值填充：众数填充-以类别数据量较多的类别填充，适用于数据倾斜时。；均值、中位数填充：以所有非缺失值的Mean或Median填充缺失值(广义插补)   2、分类别计算非缺失值的Mean或Median填充各类别的缺失值(相似填充)；均值填充：适用于数据中没有极端值的场景；中位数填充：适用于数据中有奇异值的场景\n（2）KNN：通过KNN算法将所有样本进行划分，通过计算欧氏距离，选取与缺失数据样本最近的K个样本，然后通过投票法或者K个值加权平均来估计该缺失样本的缺失数据；优点：不需要为含有缺失值的每个属性都建立预测模型  2、一个属性有多个缺失值时也可以很好解决   3、缺失值处理时把数据结构之间的相关性考虑在内；缺点：面对大数据集，KNN时间开销大  2、K值得选择是关键，过大过小都会影响结果。\n（3）回归：把数据中不含缺失值得部分当作训练集，建立回归模型，将此回归模型用来预测缺失值；适用场景：只适用缺失值是连续得情况；优缺点：预测填充理论上比值填充效果好，但如果缺失值与其他变量没有关系，那么预测出得缺失值没有意义\n（4）变量映射：把变量映射到高维空间中，优点：可以保留数据得完整性，无需考虑缺失值，缺点：1、计算开销增加  2、可能会出现稀疏矩阵，影响模型质量。"
    }]
    results = []
    for entry in json_data:
        if key.lower() in entry['content'].lower():
            results.append({'title': entry['title'], 'content': entry['content']})
    for i in results:
        print(i)
