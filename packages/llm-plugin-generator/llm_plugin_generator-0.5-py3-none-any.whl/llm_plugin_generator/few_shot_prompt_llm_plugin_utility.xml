<?xml version='1.0' encoding='utf-8'?>
<few_shot_prompt>
<plugin name="llm-cluster">
<readme>
  # llm-cluster
  
  [![PyPI](https://img.shields.io/pypi/v/llm-cluster.svg)](https://pypi.org/project/llm-cluster/)
  [![Changelog](https://img.shields.io/github/v/release/simonw/llm-cluster?include_prereleases&amp;label=changelog)](https://github.com/simonw/llm-cluster/releases)
  [![Tests](https://github.com/simonw/llm-cluster/workflows/Test/badge.svg)](https://github.com/simonw/llm-cluster/actions?query=workflow%3ATest)
  [![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](https://github.com/simonw/llm-cluster/blob/main/LICENSE)
  
  [LLM](https://llm.datasette.io/) plugin for clustering embeddings
  
  Background on this project: [Clustering with llm-cluster](https://simonwillison.net/2023/Sep/4/llm-embeddings/#llm-cluster).
  
  ## Installation
  
  Install this plugin in the same environment as LLM.
  ```bash
  llm install llm-cluster
  ```
  
  ## Usage
  
  The plugin adds a new command, `llm cluster`. This command takes the name of an [embedding collection](https://llm.datasette.io/en/stable/embeddings/cli.html#storing-embeddings-in-sqlite) and the number of clusters to return.
  
  First, use [paginate-json](https://github.com/simonw/paginate-json) and [jq](https://stedolan.github.io/jq/) to populate a collection. I this case we are embedding the title and body of every issue in the [llm repository](https://github.com/simonw/llm), and storing the result in a `issues.db` database:
  ```bash
  paginate-json 'https://api.github.com/repos/simonw/llm/issues?state=all&amp;filter=all' \
    | jq '[.[] | {id: .id, title: .title}]' \
    | llm embed-multi llm-issues - \
      --database issues.db --store
  ```
  The `--store` flag causes the content to be stored in the database along with the embedding vectors.
  
  Now we can cluster those embeddings into 10 groups:
  ```bash
  llm cluster llm-issues 10 \
    -d issues.db
  ```
  If you omit the `-d` option the default embeddings database will be used.
  
  The output should look something like this (truncated):
  ```json
  [
    {
      "id": "2",
      "items": [
        {
          "id": "1650662628",
          "content": "Initial design"
        },
        {
          "id": "1650682379",
          "content": "Log prompts and responses to SQLite"
        }
      ]
    },
    {
      "id": "4",
      "items": [
        {
          "id": "1650760699",
          "content": "llm web command - launches a web server"
        },
        {
          "id": "1759659476",
          "content": "`llm models` command"
        },
        {
          "id": "1784156919",
          "content": "`llm.get_model(alias)` helper"
        }
      ]
    },
    {
      "id": "7",
      "items": [
        {
          "id": "1650765575",
          "content": "--code mode for outputting code"
        },
        {
          "id": "1659086298",
          "content": "Accept PROMPT from --stdin"
        },
        {
          "id": "1714651657",
          "content": "Accept input from standard in"
        }
      ]
    }
  ]
  ```
  The content displayed is truncated to 100 characters. Pass `--truncate 0` to disable truncation, or `--truncate X` to truncate to X characters.
  
  ## Generating summaries for each cluster
  
  The `--summary` flag will cause the plugin to generate a summary for each cluster, by passing the content of the items (truncated according to the `--truncate` option) through a prompt to a Large Language Model.
  
  This feature is still experimental. You should experiment with custom prompts to improve the quality of your summaries.
  
  Since this can run a large amount of text through a LLM this can be expensive, depending on which model you are using.
  
  This feature only works for embeddings that have had their associated content stored in the database using the `--store` flag.
  
  You can use it like this:
  
  ```bash
  llm cluster llm-issues 10 \
    -d issues.db \
    --summary
  ```
  This uses the default prompt and the default model.
  
  Partial example output:
  ```json
  [
    {
      "id": "5",
      "items": [
        {
          "id": "1650682379",
          "content": "Log prompts and responses to SQLite"
        },
        {
          "id": "1650757081",
          "content": "Command for browsing captured logs"
        }
      ],
      "summary": "Log Management and Interactive Prompt Tracking"
    },
    {
      "id": "6",
      "items": [
        {
          "id": "1650771320",
          "content": "Mechanism for continuing an existing conversation"
        },
        {
          "id": "1740090291",
          "content": "-c option for continuing a chat (using new chat_id column)"
        },
        {
          "id": "1784122278",
          "content": "Figure out truncation strategy for continue conversation mode"
        }
      ],
      "summary": "Continuing Conversation Mechanism and Management"
    }
  ]
  ```
  
  To use a different model, e.g. GPT-4, pass the `--model` option:
  ```bash
  llm cluster llm-issues 10 \
    -d issues.db \
    --summary \
    --model gpt-4
  ```
  The default prompt used is:
  
  &gt; Short, concise title for this cluster of related documents.
  
  To use a custom prompt, pass `--prompt`:
  
  ```bash
  llm cluster llm-issues 10 \
    -d issues.db \
    --summary \
    --model gpt-4 \
    --prompt 'Summarize this in a short line in the style of a bored, angry panda'
  ```
  A `"summary"` key will be added to each cluster, containing the generated summary.
  
  ## Development
  
  To set up this plugin locally, first checkout the code. Then create a new virtual environment:
  ```bash
  cd llm-cluster
  python3 -m venv venv
  source venv/bin/activate
  ```
  Now install the dependencies and test dependencies:
  ```bash
  pip install -e '.[test]'
  ```
  To run the tests:
  ```bash
  pytest
  ```
  
</readme>
<python_file name="llm_cluster.py">
  import click
  import json
  import llm
  import numpy as np
  import sklearn.cluster
  import sqlite_utils
  import textwrap
  
  DEFAULT_SUMMARY_PROMPT = """
  Short, concise title for this cluster of related documents.
  """.strip()
  
  
  @llm.hookimpl
  def register_commands(cli):
      @cli.command()
      @click.argument("collection")
      @click.argument("n", type=int)
      @click.option(
          "--truncate",
          type=int,
          default=100,
          help="Truncate content to this many characters - 0 for no truncation",
      )
      @click.option(
          "-d",
          "--database",
          type=click.Path(
              file_okay=True, allow_dash=False, dir_okay=False, writable=True
          ),
          envvar="LLM_EMBEDDINGS_DB",
          help="SQLite database file containing embeddings",
      )
      @click.option(
          "--summary", is_flag=True, help="Generate summary title for each cluster"
      )
      @click.option("-m", "--model", help="LLM model to use for the summary")
      @click.option("--prompt", help="Custom prompt to use for the summary")
      def cluster(collection, n, truncate, database, summary, model, prompt):
          """
          Generate clusters from embeddings in a collection
  
          Example usage, to create 10 clusters:
  
          \b
              llm cluster my_collection 10
  
          Outputs a JSON array of {"id": "cluster_id", "items": [list of items]}
  
          Pass --summary to generate a summary for each cluster, using the default
          language model or the model you specify with --model.
          """
          from llm.cli import get_default_model, get_key
  
          clustering_model = sklearn.cluster.MiniBatchKMeans(n_clusters=n, n_init="auto")
          if database:
              db = sqlite_utils.Database(database)
          else:
              db = sqlite_utils.Database(llm.user_dir() / "embeddings.db")
          rows = [
              (row[0], llm.decode(row[1]), row[2])
              for row in db.execute(
                  """
              select id, embedding, content from embeddings
              where collection_id = (
                  select id from collections where name = ?
              )
          """,
                  [collection],
              ).fetchall()
          ]
          to_cluster = np.array([item[1] for item in rows])
          clustering_model.fit(to_cluster)
          assignments = clustering_model.labels_
  
          def truncate_text(text):
              if not text:
                  return None
              if truncate &gt; 0:
                  return text[:truncate]
              else:
                  return text
  
          # Each one corresponds to an ID
          clusters = {}
          for (id, _, content), cluster in zip(rows, assignments):
              clusters.setdefault(str(cluster), []).append(
                  {"id": str(id), "content": truncate_text(content)}
              )
          # Re-arrange into a list
          output_clusters = [{"id": k, "items": v} for k, v in clusters.items()]
  
          # Do we need to generate summaries?
          if summary:
              model = llm.get_model(model or get_default_model())
              if model.needs_key:
                  model.key = get_key("", model.needs_key, model.key_env_var)
              prompt = prompt or DEFAULT_SUMMARY_PROMPT
              click.echo("[")
              for cluster, is_last in zip(
                  output_clusters, [False] * (len(output_clusters) - 1) + [True]
              ):
                  click.echo("  {")
                  click.echo('    "id": {},'.format(json.dumps(cluster["id"])))
                  click.echo(
                      '    "items": '
                      + textwrap.indent(
                          json.dumps(cluster["items"], indent=2), "    "
                      ).lstrip()
                      + ","
                  )
                  prompt_content = "\n".join(
                      [item["content"] for item in cluster["items"] if item["content"]]
                  )
                  if prompt_content.strip():
                      summary = model.prompt(
                          prompt_content,
                          system=prompt,
                      ).text()
                  else:
                      summary = None
                  click.echo('    "summary": {}'.format(json.dumps(summary)))
                  click.echo("  }" + ("," if not is_last else ""))
              click.echo("]")
          else:
              click.echo(json.dumps(output_clusters, indent=4))
  
</python_file>
</plugin>
<plugin name="llm-cmd">
<readme>
  # llm-cmd
  
  [![PyPI](https://img.shields.io/pypi/v/llm-cmd.svg)](https://pypi.org/project/llm-cmd/)
  [![Changelog](https://img.shields.io/github/v/release/simonw/llm-cmd?include_prereleases&amp;label=changelog)](https://github.com/simonw/llm-cmd/releases)
  [![Tests](https://github.com/simonw/llm-cmd/actions/workflows/test.yml/badge.svg)](https://github.com/simonw/llm-cmd/actions/workflows/test.yml)
  [![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](https://github.com/simonw/llm-cmd/blob/main/LICENSE)
  
  Use LLM to generate and execute commands in your shell
  
  ## Installation
  
  Install this plugin in the same environment as [LLM](https://llm.datasette.io/).
  ```bash
  llm install llm-cmd
  ```
  ## Usage
  
  This command could be **very dangerous**. Do not use this unless you are confident you understand what it does and are sure you could spot if it is likely to do something dangerous.
  
  Run `llm cmd` like this:
  
  ```bash
  llm cmd undo last git commit
  ```
  It will use your [default model](https://llm.datasette.io/en/stable/setup.html#setting-a-custom-default-model) to generate the corresponding shell command.
  
  This will then be displayed in your terminal ready for you to edit it, or hit `&lt;enter&gt;` to execute the prompt.
  
  If the command doesnt't look right, hit `Ctrl+C` to cancel.
  
  ## The system prompt
  
  This is the prompt used by this tool:
  
  &gt; Return only the command to be executed as a raw string, no string delimiters
  wrapping it, no yapping, no markdown, no fenced code blocks, what you return
  will be passed to subprocess.check_output() directly.
  &gt;
  &gt; For example, if the user asks: undo last git commit
  &gt;
  &gt; You return only: git reset --soft HEAD~1
  
  ## Development
  
  To set up this plugin locally, first checkout the code. Then create a new virtual environment:
  ```bash
  cd llm-cmd
  python3 -m venv venv
  source venv/bin/activate
  ```
  Now install the dependencies and test dependencies:
  ```bash
  llm install -e '.[test]'
  ```
  To run the tests:
  ```bash
  pytest
  ```
  
</readme>
<python_file name="llm_cmd.py">
  import click
  import llm
  import subprocess
  from prompt_toolkit import PromptSession
  from prompt_toolkit.lexers import PygmentsLexer
  from prompt_toolkit.patch_stdout import patch_stdout
  from pygments.lexers.shell import BashLexer
  
  SYSTEM_PROMPT = """
  Return only the command to be executed as a raw string, no string delimiters
  wrapping it, no yapping, no markdown, no fenced code blocks, what you return
  will be passed to subprocess.check_output() directly.
  For example, if the user asks: undo last git commit
  You return only: git reset --soft HEAD~1
  """.strip()
  
  @llm.hookimpl
  def register_commands(cli):
      @cli.command()
      @click.argument("args", nargs=-1)
      @click.option("-m", "--model", default=None, help="Specify the model to use")
      @click.option("-s", "--system", help="Custom system prompt")
      @click.option("--key", help="API key to use")
      def cmd(args, model, system, key):
          """Generate and execute commands in your shell"""
          from llm.cli import get_default_model
          prompt = " ".join(args)
          model_id = model or get_default_model()
          model_obj = llm.get_model(model_id)
          if model_obj.needs_key:
              model_obj.key = llm.get_key(key, model_obj.needs_key, model_obj.key_env_var)
          result = model_obj.prompt(prompt, system=system or SYSTEM_PROMPT)
          interactive_exec(str(result))
  
  def interactive_exec(command):
      session = PromptSession(lexer=PygmentsLexer(BashLexer))
      with patch_stdout():
          if '\n' in command:
              print("Multiline command - Meta-Enter or Esc Enter to execute")
              edited_command = session.prompt("&gt; ", default=command, multiline=True)
          else:
              edited_command = session.prompt("&gt; ", default=command)
      try:
          output = subprocess.check_output(
              edited_command, shell=True, stderr=subprocess.STDOUT
          )
          print(output.decode())
      except subprocess.CalledProcessError as e:
          print(f"Command failed with error (exit status {e.returncode}): {e.output.decode()}")
</python_file>
</plugin>
<plugin name="llm-jq">
<readme>
  # llm-jq
  
  [![PyPI](https://img.shields.io/pypi/v/llm-jq.svg)](https://pypi.org/project/llm-jq/)
  [![Changelog](https://img.shields.io/github/v/release/simonw/llm-jq?include_prereleases&amp;label=changelog)](https://github.com/simonw/llm-jq/releases)
  [![Tests](https://github.com/simonw/llm-jq/actions/workflows/test.yml/badge.svg)](https://github.com/simonw/llm-jq/actions/workflows/test.yml)
  [![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](https://github.com/simonw/llm-jq/blob/main/LICENSE)
  
  Write and execute jq programs with the help of LLM
  
  See [Run a prompt to generate and execute jq programs using llm-jq](https://simonwillison.net/2024/Oct/27/llm-jq/) for background on this project.
  
  ## Installation
  
  Install this plugin in the same environment as [LLM](https://llm.datasette.io/).
  ```bash
  llm install llm-jq
  ```
  ## Usage
  
  Pipe JSON directly into `llm jq` and describe the result you would like:
  
  ```bash
  curl -s https://api.github.com/repos/simonw/datasette/issues | \
    llm jq 'count by user.login, top 3'
  ```
  Output:
  ```json
  [
    {
      "login": "simonw",
      "count": 11
    },
    {
      "login": "king7532",
      "count": 5
    },
    {
      "login": "dependabot[bot]",
      "count": 2
    }
  ]
  ```
  ```
  group_by(.user.login) | map({login: .[0].user.login, count: length}) | sort_by(-.count) | .[0:3]
  ```
  The JSON is printed to standard output, the jq program is printed to standard error.
  
  Options:
  
  - `-s/--silent`: Do not print the jq program to standard error
  - `-o/--output`: Output just the jq program, do not run it
  - `-v/--verbose`: Show the prompt sent to the model and the response
  - `-m/--model X`: Use a model other than the configured LLM default model
  - `-l/--length X`: Use a length of the input other than 1024 as the example
  
  By default, the first 1024 bytes of JSON will be sent to the model as an example along with your description. You can use `-l` to send more or less example data.
  
  ## Development
  
  To set up this plugin locally, first checkout the code. Then create a new virtual environment:
  ```bash
  cd llm-jq
  python -m venv venv
  source venv/bin/activate
  ```
  Now install the dependencies and test dependencies:
  ```bash
  llm install -e '.[test]'
  ```
  To run the tests:
  ```bash
  python -m pytest
  ```
  
</readme>
<python_file name="llm_jq.py">
  import click
  import llm
  import subprocess
  import sys
  import os
  
  
  SYSTEM_PROMPT = """
  Based on the example JSON snippet and the desired query, write a jq program
  
  Return only the jq program to be executed as a raw string, no string delimiters
  wrapping it, no yapping, no markdown, no fenced code blocks, what you return
  will be passed to subprocess.check_output('jq', [...]) directly.
  For example, if the user asks: extract the name of the first person
  You return only: .people[0].name
  """.strip()
  
  
  @llm.hookimpl
  def register_commands(cli):
      @cli.command()
      @click.argument("description")
      @click.option("model_id", "-m", "--model", help="Model to use")
      @click.option("-l", "--length", help="Example length to use", default=1024)
      @click.option("-o", "--output", help="Just show the jq program", is_flag=True)
      @click.option("-s", "--silent", help="Don't output jq program", is_flag=True)
      @click.option(
          "-v", "--verbose", help="Verbose output of prompt and response", is_flag=True
      )
      def jq(description, model_id, length, output, silent, verbose):
          """
          Pipe JSON data into this tool and provide a description of a
          jq program you want to run against that data.
  
          Example usage:
  
          \b
            cat data.json | llm jq "Just the first and last names"
          """
          model = llm.get_model(model_id)
  
          is_pipe = not sys.stdin.isatty()
          if is_pipe:
              example = sys.stdin.buffer.read(length)
          else:
              example = ""
  
          prompt = description
          if example:
              prompt += "\n\nExample JSON snippet:\n" + example.decode()
  
          if verbose:
              click.echo(
                  click.style(f"System:\n{SYSTEM_PROMPT}", fg="yellow", bold=True),
                  err=True,
              )
              click.echo(
                  click.style(f"Prompt:\n{prompt}", fg="green", bold=True),
                  err=True,
              )
  
          program = (
              model.prompt(
                  prompt,
                  system=SYSTEM_PROMPT,
              )
              .text()
              .strip()
          )
  
          if verbose:
              click.echo(
                  click.style(f"Response:\n{program}", fg="green", bold=True),
                  err=True,
              )
  
          if output or not is_pipe:
              click.echo(program)
              return
  
          # Run jq
          process = subprocess.Popen(
              ["jq", program],
              stdin=subprocess.PIPE,
              stdout=subprocess.PIPE,
              stderr=subprocess.PIPE,
          )
  
          try:
              if example:
                  process.stdin.write(example)
  
              # Stream the rest of stdin to jq, 8k at a time
              if is_pipe:
                  while True:
                      chunk = sys.stdin.buffer.read(8192)
                      if not chunk:
                          break
                      process.stdin.write(chunk)
                  process.stdin.close()
  
              # Stream stdout
              while True:
                  chunk = process.stdout.read(8192)
                  if not chunk:
                      break
                  sys.stdout.buffer.write(chunk)
                  sys.stdout.buffer.flush()
  
              # After stdout is done, read and forward any stderr
              stderr_data = process.stderr.read()
              if stderr_data:
                  sys.stderr.buffer.write(stderr_data)
                  sys.stderr.buffer.flush()
  
              # Wait for process to complete and get exit code
              return_code = process.wait()
  
              # Output the program at the end
              if not silent and not verbose:
                  click.echo(
                      click.style(f"{program}", fg="blue", bold=True),
                      err=True,
                  )
  
              sys.exit(return_code)
  
          except BrokenPipeError:
              # Handle case where output pipe is closed
              devnull = os.open(os.devnull, os.O_WRONLY)
              os.dup2(devnull, sys.stdout.fileno())
              sys.exit(1)
          except KeyboardInterrupt:
              # Handle Ctrl+C gracefully
              process.terminate()
              process.wait()
              sys.exit(130)
          finally:
              # Ensure process resources are cleaned up
              process.stdout.close()
              process.stderr.close()
  
</python_file>
</plugin>
<plugin name="llm-whisper-api">
<readme>
  # llm-whisper-api
  
  [![PyPI](https://img.shields.io/pypi/v/llm-whisper-api.svg)](https://pypi.org/project/llm-whisper-api/)
  [![Changelog](https://img.shields.io/github/v/release/simonw/llm-whisper-api?include_prereleases&amp;label=changelog)](https://github.com/simonw/llm-whisper-api/releases)
  [![Tests](https://github.com/simonw/llm-whisper-api/actions/workflows/test.yml/badge.svg)](https://github.com/simonw/llm-whisper-api/actions/workflows/test.yml)
  [![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](https://github.com/simonw/llm-whisper-api/blob/main/LICENSE)
  
  Run transcriptions using the OpenAI Whisper API
  
  ## Installation
  
  Install this plugin in the same environment as [LLM](https://llm.datasette.io/).
  ```bash
  llm install llm-whisper-api
  ```
  ## Usage
  
  The plugin adds a new command, `llm whisper-api`. Use it like this:
  
  ```bash
  llm whisper-api audio.mp3
  ```
  The transcribed audio will be output directly to standard output as plain text.
  
  The plugin will use the OpenAI API key you have already configured using:
  ```bash
  llm keys set openai
  # Paste key here
  ```
  You can also pass an explicit API key using `--key` like this:
  
  ```bash
  llm whisper-api audio.mp3 --key $OPENAI_API_KEY
  ```
  
  You can pipe data to the tool if you specify `-` as a filename:
  
  ```bash
  curl -s 'https://static.simonwillison.net/static/2024/russian-pelican-in-spanish.mp3' \
    | llm whisper-api -
  ```
  
  ## Development
  
  To set up this plugin locally, first checkout the code. Then create a new virtual environment:
  ```bash
  cd llm-whisper-api
  python -m venv venv
  source venv/bin/activate
  ```
  Now install the dependencies and test dependencies:
  ```bash
  llm install -e '.[test]'
  ```
  To run the tests:
  ```bash
  python -m pytest
  ```
  
</readme>
<python_file name="llm_whisper_api.py">
  import click
  import httpx
  import io
  import llm
  
  
  @llm.hookimpl
  def register_commands(cli):
      @cli.command()
      @click.argument("audio_file", type=click.File("rb"))
      @click.option("api_key", "--key", help="API key to use")
      def whisper_api(audio_file, api_key):
          """
          Run transcriptions using the OpenAI Whisper API
  
          Usage:
  
          \b
              llm whisper-api audio.mp3 &gt; output.txt
              cat audio.mp3 | llm whisper-api - &gt; output.txt
          """
          # Read the entire content into memory first
          audio_content = audio_file.read()
          audio_file.close()
  
          key = llm.get_key(api_key, "openai")
          if not key:
              raise click.ClickException("OpenAI API key is required")
          try:
              click.echo(transcribe(audio_content, key))
          except httpx.HTTPError as ex:
              raise click.ClickException(str(ex))
  
  
  def transcribe(audio_content: bytes, api_key: str) -&gt; str:
      """
      Transcribe audio content using OpenAI's Whisper API.
  
      Args:
          audio_content (bytes): The audio content as bytes
          api_key (str): OpenAI API key
  
      Returns:
          str: The transcribed text
  
      Raises:
          httpx.RequestError: If the API request fails
      """
      url = "https://api.openai.com/v1/audio/transcriptions"
      headers = {"Authorization": f"Bearer {api_key}"}
  
      audio_file = io.BytesIO(audio_content)
      audio_file.name = "audio.mp3"  # OpenAI API requires a filename, or 400 error
  
      files = {"file": audio_file}
      data = {"model": "whisper-1", "response_format": "text"}
  
      with httpx.Client() as client:
          response = client.post(url, headers=headers, files=files, data=data)
          response.raise_for_status()
          return response.text.strip()
  
</python_file>
</plugin>
</few_shot_prompt>