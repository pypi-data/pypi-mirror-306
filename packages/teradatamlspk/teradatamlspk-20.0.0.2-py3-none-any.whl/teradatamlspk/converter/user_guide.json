{
  "not_supported": {
    "collect_set": "Not yet available for Teradata Vantage.",
    "udtf" : "Not yet available. Convert teradatamlspk DataFrame to teradataml DataFrame using teradatamlspk_df.toTeradataml() and then use map_row or map_partition.",
    "ArrayType" : "Not applicable for Teradata. If user is planning to use ArrayType for Vector UDT, then they do not need this type as Teradata accepts multiple columns for map_row and map_partition.",
    "Vectors" : "Not applicable for Teradata. All ML functions accepts multiple columns so pass multiple columns to ML function instead of converting multiple columns to a Vectors.",
    "Vector" : "Not applicable for Teradata. All ML functions accepts multiple columns so pass multiple columns to ML function instead of converting multiple columns to a Vector.",
    "DenseVector" : "Not applicable for Teradata. All ML functions accepts multiple columns so pass multiple columns to ML function instead of converting multiple columns to a Vector.",
    "VectorUDT" : "Not applicable for Teradata. All ML functions accepts multiple columns so pass multiple columns to ML function instead of converting multiple columns to a Vector.",
    "IndexToString" : "Not yet available.",
    "SparkFiles" : "Not applicable for Teradata Vantage. User should load the corresponding file to Vantage as BLOB DataType. Also, if user is trying to use this file in a Python application, Teradata recommends to look at Script/Apply in teradataml. One can convert teradatamlspk DataFrame to teradataml DataFrame using toTeradataml() API.",
    "RDD": "Not applicable for Teradata Vantage. All DataFrame operations can be done using teradatamlspk DataFrame instead of RDD.",
    "_create_row": "Not applicable for Teradata Vantage. If the data is existed outside of Vantage, load the data in to Vantage using Data Transfer functions available in teradataml.",
    "Pipeline": "Not yet available in Teradata Vantage. Convert the Pipeline call to make use of pipeline function scikit-learn using Teradata Open source ML functions. Refer user guide for the example on how to accomplish this.",
    "PipelineModel": "Not yet available in Teradata Vantage. Convert the Pipeline call to make use of pipeline function scikit-learn using Teradata Open source ML functions. Refer user guide for the example on how to accomplish this.",
    "BinaryType": "Not applicable for Teradata Vantage.",
    "aes_encrypt": "Not available for Teradata Vantage.",
    "aes_decrypt": "Not available for Teradata Vantage.",
    "Observation": "Not yet available in Teradata Vantage.",
    "PandasCogroupedOps": "Not yet available.",
    "UDFRegistration": "Not yet available Teradata Vantage.",
    "UDTFRegistration": "Not yet available Teradata Vantage.",
    "UserDefinedTableFunction": "Not yet available Teradata Vantage.",
    "DataStreamReader": "Not available for Teradata Vantage.",
    "DataStreamWriter": "Not available for Teradata Vantage.",
    "StreamingQuery": "Not available for Teradata Vantage.",
    "StreamingQueryManager": "Not available for Teradata Vantage.",
    "StreamingQueryListener": "Not available for Teradata Vantage.",
    "Broadcast": "Not available for Teradata Vantage.",
    "Accumulator": "Not available for Teradata Vantage.",
    "AccumulatorParam": "Not available for Teradata Vantage.",
    "StorageLevel": "Not applicable for Teradata Vantage.",
    "TaskContext": "Not applicable for Teradata Vantage.",
    "RDDBarrier": "Not applicable for Teradata Vantage.",
    "BarrierTaskContext": "Not applicable for Teradata Vantage.",
    "BarrierTaskInfo": "Not applicable for Teradata Vantage.",
    "InheritableThread": "Not applicable for Teradata Vantage.",
    "VersionUtils": "Not applicable for Teradata Vantage.",
    "ResourceInformation": "Not applicable for Teradata Vantage.",
    "ResourceProfile": "Not applicable for Teradata Vantage.",
    "ResourceProfileBuilder": "Not applicable for Teradata Vantage.",
    "ExecutorResourceRequest": "Not applicable for Teradata Vantage.",
    "ExecutorResourceRequests": "Not applicable for Teradata Vantage.",
    "TaskResourceRequest": "Not applicable for Teradata Vantage.",
    "TaskResourceRequests": "Not applicable for Teradata Vantage.",
    "readStream": "Not applicable for Teradata Vantage.",
    "streams": "Not applicable for Teradata Vantage.",
    "addArtifact": "Not applicable for Teradata Vantage. Look at teradataml Script or Apply to use open source python modules for processing the data.",
    "addArtifacts": "Not applicable for Teradata Vantage. Look at teradataml Script or Apply to use open source python modules for processing the data.",
    "copyFromLocalToFs": "Look at teradataml User Environment Service API's. It has API's to upload the file to cloud storage and make use of that file in processing the data.",
    "client": "Not applicable for Teradata Vantage.",
    "interruptAll": "Not yet available. Alternatively, users can contact DBA to cancel/stop the running sessions. ",
    "interruptTag": "Not yet available. Alternatively, users can contact DBA to cancel/stop the running sessions. ",
    "interruptOperation": "Not yet available. Alternatively, users can contact DBA to cancel/stop the running sessions. ",
    "addTag": "Not applicable for Teradata Vantage. Users should identify the individual SQL queries and ask DBA to cancel/stop them. ",
    "removeTag": "Not applicable for Teradata Vantage. Users should identify the individual SQL queries and ask DBA to cancel/stop them. ",
    "getTags": "Not applicable for Teradata Vantage. Users should identify the individual SQL queries and ask DBA to cancel/stop them. ",
    "clearTags": "Not applicable for Teradata Vantage. Users should identify the individual SQL queries and ask DBA to cancel/stop them. ",
    "jdbc": "Not applicable for Teradata Vantage. Pull the data inside Vantage and create a DataFrame using TeradataSession.createDataFrame()",
    "RegressionMetrics": "RegressionMetrics uses RDD for evaluating the metrics. User should use RegressionEvaluator as it runs on DataFrame instead of RDD.",
    "accumulator": "Not applicable for Teradata Vantage.",
    "addFile": "Not applicable for Teradata Vantage. ",
    "addPyFile": "Not applicable for Teradata Vantage. ",
    "binaryFiles": "Not applicable for Teradata Vantage. ",
    "binaryRecords": "Not applicable for Teradata Vantage. ",
    "cancelAllJobs": "Not applicable for Teradata Vantage. ",
    "cancelJobGroup": "Not applicable for Teradata Vantage. ",
    "dump_profiles": "Not applicable for Teradata Vantage. ",
    "emptyRDD": "Not applicable for Teradata Vantage. ",
    "getCheckpointDir": "Not applicable for Teradata Vantage. ",
    "getLocalProperty": "Not applicable for Teradata Vantage. ",
    "hadoopFile": "Not applicable for Teradata Vantage. ",
    "hadoopRDD": "Not applicable for Teradata Vantage. ",
    "newAPIHadoopFile": "Not applicable for Teradata Vantage. ",
    "newAPIHadoopRDD": "Not applicable for Teradata Vantage. ",
    "parallelize": "Not applicable for Teradata Vantage. ",
    "pickleFile": "Not applicable for Teradata Vantage. ",
    "runJob": "Not applicable for Teradata Vantage. ",
    "sequenceFile": "Not applicable for Teradata Vantage. ",
    "setCheckpointDir": "Not applicable for Teradata Vantage. ",
    "setJobDescription": "Not applicable for Teradata Vantage. ",
    "setJobGroup": "Not applicable for Teradata Vantage. ",
    "setSystemProperty": "Not applicable for Teradata Vantage. ",
    "show_profiles": "Not applicable for Teradata Vantage. ",
    "statusTracker": "Not applicable for Teradata Vantage. ",
    "textFile": "Not applicable for Teradata Vantage. ",
    "union": "Not applicable for Teradata Vantage, if it's a TeradataContext API or union is performed on RDD.",
    "wholeTextFiles": "Not applicable for Teradata Vantage. ",
    "range": "SparkSession.range() and SparkContext.range() is not applicable for Teradata Vantage. Load the data in to Vantage table and then create DataFrame on it.",
    "approxQuantile": "Not yet available. User should write the functionality for this API using teradataml Script or teradataml Apply.",
    "dropDuplicatesWithinWatermark": "Not available for Teradata Vantage. ",
    "freqItems": "Not yet available. User can look create a query using function 'XMLAGG' and create a DataFrame on the query. ",
    "mapInPandas": "Not yet available. User should look at teradataml Script or Apply and get similar output.",
    "mapInArrow": "Not yet available. User should look at teradataml Script or Apply and get similar output.",
    "observe": "Not applicable to Teradata Vantage. ",
    "offset": "Not yet available. User should use sample function as an alternative. ",
    "toJSON": "Not applicable to Teradata Vantage. Do not convert any object to RDD. user should use the API's defined on DataFrame instead of RDD's.",
    "to_pandas_on_spark": "Not yet available. Teradata recommends the user to use regular DataFrame API's instead of pandas style API's.",
    "withMetadata": "Not yet available. ",
    "withWatermark": "Not applicable to Teradata Vantage.",
    "writeStream": "Not applicable to Teradata Vantage.",
    "pandas_api": "Not yet available. Teradata recommends the user to use regular DataFrame API's instead of pandas style API's.",
    "rlike": "Not available. Teradata recommends user to convert the functionality to use 'like' API.",
    "dropFields": "Not applicable for Teradata Vantage.",
    "getItem": "Not applicable for Teradata Vantage.",
    "getField": "Not applicable for Teradata Vantage.",
    "withField": "Not applicable for Teradata Vantage.",
    "input_file_name": "Not available. However, if file is stored in a cloud storage, then TeradataSession.read returns a DataFrame with first column has file location. ",
    "named_struct": "Not applicable for Teradata Vantage. ",
    "factorial": "Not yet available. User can use teradataml DataFrame.map_row. ",
    "shiftrightunsigned": "Not yet available. User can use teradataml DataFrame.map_row. ",
    "make_interval": "Not yet available. ",
    "session_window": "Not yet available. ",
    "window_time": "Not yet available. ",
    "array": "Array columns are not supported in teradatamlspk. ",
    "array_contains": "Not applicable for Teradata Vantage. ",
    "arrays_overlap": "Not applicable for Teradata Vantage. ",
    "array_join": "Not applicable for Teradata Vantage. ",
    "create_map": "Not applicable for Teradata Vantage. ",
    "slice": "teradatamlspk columns does not store a vector/list of values. Hence, slice on Column or column name is not applicable.",
    "array_position": "Not applicable for Teradata Vantage. ",
    "element_at": "Not applicable for Teradata Vantage. ",
    "array_append": "Not applicable for Teradata Vantage. ",
    "array_size": "Not applicable for Teradata Vantage. ",
    "array_sort": "Not applicable for Teradata Vantage. ",
    "array_insert": "Not applicable for Teradata Vantage. ",
    "array_remove": "Not applicable for Teradata Vantage. ",
    "array_prepend": "Not applicable for Teradata Vantage. ",
    "array_distinct": "Not applicable for Teradata Vantage. ",
    "array_intersect": "Not applicable for Teradata Vantage. ",
    "array_union": "Not applicable for Teradata Vantage. ",
    "array_except": "Not applicable for Teradata Vantage. ",
    "array_compact": "Not applicable for Teradata Vantage. ",
    "exists": "exists function on Column or column name is not applicable for Teradata Vantage. ",
    "forall": "Not applicable for Teradata Vantage. ",
    "zip_with": "Not applicable for Teradata Vantage. ",
    "transform_keys": "Not applicable for Teradata Vantage. ",
    "transform_values": "Not applicable for Teradata Vantage. ",
    "map_filter": "Not applicable for Teradata Vantage. ",
    "map_from_arrays": "Not applicable for Teradata Vantage. ",
    "map_zip_with": "Not applicable for Teradata Vantage. ",
    "explode": "Not applicable for Teradata Vantage. ",
    "explode_outer": "Not applicable for Teradata Vantage. ",
    "posexplode": "Not applicable for Teradata Vantage. ",
    "posexplode_outer": "Not applicable for Teradata Vantage. ",
    "inline": "Not applicable for Teradata Vantage. ",
    "inline_outer": "Not applicable for Teradata Vantage. ",
    "get": "teradatamlspk columns does not store a vector/list of values. Hence, get on Column or column name is not applicable.",
    "get_json_object": "Not yet available. ",
    "json_tuple": "Not applicable for Teradata Vantage. ",
    "from_json": "Not applicable for Teradata Vantage. ",
    "schema_of_json": "Not applicable for Teradata Vantage. ",
    "to_json": "Not applicable for Teradata Vantage. ",
    "json_array_length": "Not applicable for Teradata Vantage. ",
    "json_object_keys": "Not applicable for Teradata Vantage. ",
    "size": "Not applicable for Teradata Vantage. ",
    "cardinality": "Not applicable for Teradata Vantage. ",
    "sort_array": "Not applicable for Teradata Vantage. ",
    "array_max": "Not applicable for Teradata Vantage. ",
    "array_min": "Not applicable for Teradata Vantage. ",
    "shuffle": "Not applicable for Teradata Vantage. ",
    "flatten": "teradatamlspk columns does not store a vector/list of values. Hence, flatten on Column or column name is not applicable.",
    "sequence": "Not applicable for Teradata Vantage. ",
    "array_repeat": "Not applicable for Teradata Vantage. ",
    "map_contains_key": "Not applicable for Teradata Vantage. ",
    "map_keys": "Not applicable for Teradata Vantage. ",
    "map_values": "Not applicable for Teradata Vantage. ",
    "map_entries": "Not applicable for Teradata Vantage. ",
    "map_from_entries": "Not applicable for Teradata Vantage. ",
    "arrays_zip": "Not applicable for Teradata Vantage. ",
    "map_concat": "Not applicable for Teradata Vantage. ",
    "schema_of_csv": "Not applicable for Teradata Vantage. ",
    "try_element_at": "Not applicable for Teradata Vantage. ",
    "find_in_set": "Not applicable for Teradata Vantage. ",
    "split": "teradatamlspk columns does not store a vector/list of values. Hence, split on Column or column name is not applicable.",
    "split_part": "Not applicable for Teradata Vantage. ",
    "substring_index": "Not applicable for Teradata Vantage. ",
    "sentences": "Not applicable for Teradata Vantage. ",
    "bit_count": "Not applicable for Teradata Vantage. ",
    "bit_get": "Not applicable for Teradata Vantage. ",
    "getbit": "Not applicable for Teradata Vantage. ",
    "call_function": "Not applicable for Teradata Vantage. ",
    "pandas_udf": "Not applicable for Teradata Vantage. ",
    "xpath": "Not applicable for Teradata Vantage. ",
    "xpath_boolean": "Not applicable for Teradata Vantage. ",
    "xpath_double": "Not applicable for Teradata Vantage. ",
    "xpath_float": "Not applicable for Teradata Vantage. ",
    "xpath_long": "Not applicable for Teradata Vantage. ",
    "xpath_number": "Not applicable for Teradata Vantage. ",
    "xpath_short": "Not applicable for Teradata Vantage. ",
    "xpath_string": "Not applicable for Teradata Vantage. ",
    "BucketedRandomProjectionLSH": "Not yet available. User's can use open source module along with teradataml Script or Apply to achieve the functionality.",
    "ChiSqSelector": "Not yet available.",
    "DCT": "Not yet available. User's can use 'dct' in scipy module along with teradataml 'Script' or 'Apply'.",
    "FeatureHasher": "Not yet available. Use sklearn FeatureHasher along with teradataml 'Script' or 'Apply'.",
    "HashingTF": "Not yet available. Use sklearn HashingVectorizer along with teradataml 'Script' or 'Apply'.",
    "IDF": "Not yet available. Use sklearn TfidfTransformer/ TfidfVectorizer along with teradataml 'Script' or 'Apply'.",
    "MinHashLSH": "Not yet available. Use datasketch 'MinHashLSH' along with teradataml 'Script' or 'Apply'.",
    "NGram": "Not yet available. Use sklearn 'CountVectorizer' with 'ngram_range' argument using teradataml 'Script' or 'Apply'.",
    "QuantileDiscretizer": "Not yet available. Use KBinsDiscretizer from sklearn along with teradataml 'Script' or 'Apply'.",
    "StopWordsRemover": "Not yet available. Use 'stopwords' from nltk along with teradataml 'Script' or 'Apply'.",
    "StringIndexer": "Not yet available. Use 'LabelEncoder' and 'OrdinalEncoder' from sklearn along with teradataml 'Script' or 'Apply'.",
    "Tokenizer": "Not yet available. Use 'CountVectorizer' from sklearn along with teradataml 'Script' or 'Apply'.",
    "Word2Vec": "Not yet available. Use 'Word2Vec' from gensim.models along with teradataml 'Script' or 'Apply'.",
    "FMClassifier": "Not yet available. Use 'pylibfm' from pyfm along with teradataml 'Script' or 'Apply'.",
    "BisectingKMeans": "Not yet available. Use 'BisectingKMeans' from sklearn along with teradataml 'Script' or 'Apply'.",
    "LDA": "Not yet available. Use 'LatentDirichletAllocation' from sklearn along with teradataml 'Script' or 'Apply'.",
    "PowerIterationClustering": "Not yet available. ",
    "ALS": "Not yet available. Use 'AlternatingLeastSquares' from implicit package along with teradataml 'Script' or 'Apply'.",
    "AFTSurvivalRegression": "Not yet available. Use 'WeibullAFT' from sksurv package along with teradataml 'Script' or 'Apply'.",
    "GeneralizedLinearRegression": "Not yet available. Use 'GLM' from statsmodels along with teradataml 'Script' or 'Apply'.",
    "FMRegressor": "Not yet available. Use 'pylibfm' from pyfm along with teradataml 'Script' or 'Apply'.",
    "KolmogorovSmirnovTest": "Not yet available. Use 'ks_2samp' from scipy along with teradataml 'Script' or 'Apply'.",
    "MultivariateGaussian": "Not yet available. Use 'multivariate_normal' from scipy along with teradataml 'Script' or 'Apply'.",
    "ParamGridBuilder": "Not yet available. Use 'GridSearchCV' from sklearn along with teradataml 'Script' or 'Apply'.",
    "CrossValidator": "Not yet available. Use 'GridSearchCV' from sklearn along with teradataml 'Script' or 'Apply'.",
    "TrainValidationSplit": "Not yet available. Use 'GridSearchCV' from sklearn along with teradataml 'Script' or 'Apply'.",
    "BinaryClassificationEvaluator": "Not yet available. Use ClassificationEvaluator.",
    "MultilabelClassificationEvaluator": "Not yet available. ",
    "RankingEvaluator": "Not yet available. Use 'label_ranking_average_precision_score' from sklearn along with teradataml 'Script' or 'Apply'.",
    "FPGrowth": "Not yet available. Use 'pyfpgrowth' from mlxtend along with teradataml 'Script' or 'Apply'.",
    "PrefixSpan": "Not yet available. Use 'PrefixSpan' from prefixspan along with teradataml 'Script' or 'Apply'.",
    "ImageSchema": "Not yet available. Use 'OpenCV' along with teradataml 'Script' or 'Apply'.",
    "TorchDistributor": "Not yet available. ",
    "DeepspeedTorchDistributor": "Not yet available. ",
    "writeTo.overwrite": "Not yet Supported",
    "writeTo.overwritePartitions": "Not yet Supported",
    "writeTo.using": "Not yet Supported",
    "writeTo.tableProperty": "Not yet Supported",
    "write.bucketBy": "Not yet Supported",
    "write.partitionBy": "Not yet Supported",
    "read.orc": "Not yet Supported",
    "_jsparkSession": "API is not applicable to Vantage. Use API's on TeradataSession as most of API's on _jsparkSession are supported on TeradataSession object"
  },
  "partially_supported": {
    "reverse": "DataFrameColumn.reverse is supported on string type columns and not supported on array type columns.",
    "parse_url": "Does not support key parameter.",
    "to_char": "Format specifiers for PySpark is different from Teradata if format specifiers lies in a Column. Make sure to update the format specifiers in the corresponding column accordingly.",
    "to_varchar": "Format specifiers for PySpark is different from Teradata if format specifiers lies in a Column. Make sure to update the format specifiers in the corresponding column accordingly.",
    "make_timestamp": "time zone values in column supports only teradataml timezone strings",
    "make_timestamp_ltz": "time zone values in column supports only teradataml timezone strings",
    "from_utc_timestamp": "time zone values in column supports only teradataml timezone strings",
    "to_utc_timestamp": "time zone values in column supports only teradataml timezone strings",
    "writeTo.partitionedBy": "'bucket' function is not supported. 'primary_index' is required to be used in \"options\". Placeholders are kept for this in the converted file",
    "read.csv": ["read.csv can read the file from local file system or from cloud file system.",
                 "If file is in cloud file system, user should pass parameters required for teradataml ReadNOS. Placeholders are kept for this in the converted file. User can replace those placeholders with actual values.",
                 "If file is in local file system, function uses teradataml read_csv. Users have to remove Placeholders in the converted file. teradataml read_csv arguments are accepted as options. Header is mandatory. pyspark2teradataml will not infer the schema automatically and so schema is mandatory."],
    "read.json": ["read.json can read the file from local file system or from cloud file system.",
                  "If file is in cloud file system, user should pass parameters required for teradataml ReadNOS. Placeholders are kept for this in the converted file. User can replace those placeholders with actual values.",
                  "If file is in local file system, teradatamlspk use pandas to read the file. Users have to remove Placeholders in the converted file. User can pass the parameters which are accepted by pandas.read_json to either 'option' or 'options'."],
    "read.parquet": ["read.parquet can read the file from local file system or from cloud file system.",
                     "If file is in cloud file system, user should pass parameters required for teradataml ReadNOS. Placeholders are kept for this in the converted file. User can replace those placeholders with actual values.",
                     "If file is in local file system, teradatamlspk use pandas to read the file. Users have to remove Placeholders in the converted file. User can pass the parameters which are accepted by pandas.read_parquet to either 'option' or 'options'."],
    "read.format": "Supported format are csv, json and parquet.",
    "write.csv": ["write.csv can write the file in local file system or in cloud file system.",
                  "If file to write in cloud file system, user should pass parameters required for teradataml WriteNOS. Placeholders are kept for this in the converted file. User can replace those placeholders with actual values.",
                  "If file to write in local file system, teradatamlspk use pandas to write the file. Users have to remove Placeholders in the converted file. User can pass the parameters which are accepted by DataFrame.to_csv of pandas to either 'option' or 'options'."],
    "write.parquet": ["write.parquet can write the file in local file system or in cloud file system.",
                      "If file to write in cloud file system, user should pass parameters required for teradataml WriteNOS. Placeholders are kept for this in the converted file. User can replace those placeholders with actual values.",
                      "If file to write in local file system, teradatamlspk use pandas to write the file. Users have to remove Placeholders in the converted file. User can pass the parameters which are accepted by DataFrame.to_parquet of pandas to either 'option' or 'options'."],
    "write.format": "Supported format are csv, json, parquet and orc.",
    "write.json": ["write.json can write the file in local file system.",
                   "If file to write in local file system, teradatamlspk use pandas to write the file. Users have to remove Placeholders in the converted file. User can pass the parameters which are accepted by DataFrame.to_json of pandas to either 'option' or 'options'."],
    "write.orc": ["write.orc can write the file in local file system.",
                  "If file to write in local file system, teradatamlspk use pandas to write the file. Users have to remove Placeholders in the converted file. User can pass the parameters which are accepted by DataFrame.to_orc of pandas to either 'option' or 'options'."],
    "StandardScaler": ["The DataFrame accepted by StandardScaler.fit() method should have a mandatory column 'id' which should have distinct values. Create a column from function monotonically_increasing_id. Visit user guide for example.",
                       "Argument 'outputCol' is not significant. 'transform()' returns all the columns which includes scaled columns. Look at the user guide for more details. "],
    "MinMaxScaler": "Argument 'outputCol' is not significant. Look at the user guide for more details. ",
    "MaxAbsScaler": "Argument 'outputCol' is not significant. Look at the user guide for more details. ",
    "VarianceThresholdSelector": "Argument 'outputCol' is not significant. Look at the user guide for more details. ",
    "OneHotEncoder": "Argument 'outputCol' is not significant. Look at the user guide for more details. ",
    "UnivariateFeatureSelector": "Argument 'outputCol' is not significant. Look at the user guide for more details. ",
    "PACKAGE_EXTENSIONS": "Not applicable for Teradata Vantage.",
    "resources": "Not yet available.",
    "newSession": "pyspark2teradataml does not support multiple connections at same time. Hence the API returns existing session instead of creating a new session.",
    "crossJoin": "If both the input DataFrame's share same column names, then column names for those columns are prefixed with as 'l' and 'r' in the output DataFrame. Also the order of the columns varies.",
    "join": "If the DataFrame.join() is of type inner, cross, outer, full, fullouter, full_outer, left, leftouter, left_outer, right, rightouter, right_outer and if both the input DataFrame's share same column names, then column names for those columns are prefixed with as 'l' and 'r' in the output DataFrame. Also the order of the columns varies.",
    "colRegex": "Pyspark returns result based on Scala or Java regex, where as teradatamlspk will return based on python regex.",
    "unpivot": "Output DataFrame column names are different when compared with PySpark.",
    "melt": "Output DataFrame column names are different when compared with PySpark.",
    "replace": "Input arguments for DataFrame.replace() must be of same data type or else the types must be compatible. For example, if value is a string, and 'subset' contains a non-string column, then the non-string column are simply ignored in pyspark, but teradatamlspk raises error. Users should specify the value in 'subset' to replace appropriately based on column type.",
    "pivot": "Based on the data, Column names may differ from Pyspark. User can use \"alias\" property to change column name",
    "crosstab": "Based on the data, Column names may differ from PySpark. Incase if these column names are being used in next subsequent lines, make sure to change those lines accordingly. ",
    "agg": "Functions count_distinct and countDistinct are accepted only if it has one column as input.",
    "like": ["Argument 'pattern' accepts only strings. ",
             "Argument 'escapeChar' is not supported. "],
    "ilike": ["Argument 'pattern' accepts only strings. ",
              "Argument 'escapeChar' is not supported. "],
    "shiftleft": "Supports only integer values.",
    "shiftright": "Supports only integer values.",
    "fillna": "All input arguments must be of the same data type or their types must be compatible. For example, If value is a string, and subset contains a non-string column, then teradatamlspk raises an error. User must drop incompatible column(s) or cast them to the compatible ones.",
    "na.fill": "All input arguments must be of the same data type or their types must be compatible. For example, If value is a string, and subset contains a non-string column, then teradatamlspk raises an error. User must drop incompatible column(s) or cast them to the compatible ones.",
    "register": "Lambda functions and pandas_udf are not supported yet. ",
    "call_udf": "Columns passed to the UDF should be present in the corresponding DataFrame. ",
    "convert_timezone": "Arguments 'sourceTz' and 'targetTz' should be teradata vantage time zone strings. 'sourceTz' doesn't accept None value.",
    "RegexTokenizer": ["Input column name should not be a Teradata reserved keyword. ",
                       "Output column will contain the tokenized words in multiple rows. Look at the user guide for more details. "]
  },
  "notification": {
    "current_timezone": "Returns time zone in Hour:Minute format. Refer the Example section in user guide for more details.",
    "broadcast": "Same DataFrame is returned and call is not effective. ",
    "applicationId": "Not applicable for Teradata Vantage. Returns always 0 only to maintain parity. ",
    "defaultMinPartitions": "Not applicable for Teradata Vantage. Returns always 1 only to maintain parity. ",
    "defaultParallelism": "User cannot control parallelism in Teradata Vantage. Returns always 1 only to maintain parity. ",
    "uiWebUrl": "Not applicable for Teradata Vantage.",
    "dtypes": "Shows Teradata types .",
    "cache": "Not applicable for Teradata. Hence API returns the same DataFrame.",
    "checkpoint": "Not applicable for Teradata. Hence API returns the same DataFrame.",
    "localCheckpoint": "Not applicable for Teradata. Hence API returns the same DataFrame.",
    "persist": "Not applicable for Teradata. Hence API returns the same DataFrame.",
    "unpersist": "Not applicable for Teradata. Hence API returns the same DataFrame.",
    "sortWithinPartitions": "Not applicable for Teradata. Hence API returns the same DataFrame.",
    "hint": "Not applicable for Teradata. Hence API returns the same DataFrame.",
    "coalesce": "Not applicable for Teradata. Hence API returns the same DataFrame.",
    "repartition": "Not applicable for Teradata. Hence API returns the same DataFrame.",
    "repartitionByRange": "Not applicable for Teradata. Hence API returns the same DataFrame.",
    "sameSemantics": "Not applicable for Teradata. Hence API returns False always.",
    "inputFiles": "Not applicable for Teradata. Hence API returns empty list.",
    "isLocal": "Not applicable for Teradata. Hence API returns False always.",
    "isStreaming": "Not applicable for Teradata. Hence API returns 0 always.",
    "semanticHash": "Not applicable for Teradata. Hence API returns False always.",
    "cube": "Pyspark performs aggregation on columns used for grouping whereas teradatamlspk ignores the aggregation of grouping columns.",
    "rollup": "Pyspark performs aggregation on columns used for grouping whereas teradatamlspk ignores the aggregation of grouping columns.",
    "SparkConf": "Not applicable for Teradata Vantage. ",
    "SparkContext": "SparkContext changed to TeradataContext. SparkContext methods applicable for Vantage will work with TeradataContext. Not applicable methods will throw error. Remove such methods.",
    "SQLContext": "SQLContext methods applicable for Vantage will work. Not applicable methods will throw error. Remove such methods.",
    "randomSplit": "Argument 'seed' is ignored and not considered while processing.",
    "sample": "Argument 'seed' in DataFrame.sample() is ignored and not considered while processing.",
    "atanh": "Throws error if data inside the column is 1. ",
    "bin": "Supports only integer values. ",
    "cbrt": "Supports only for positive values. ",
    "cot": "Throws error if data inside the column is 0. ",
    "csc": "Throws error if data inside the column is 0. ",
    "ln": "Throws error if data inside the column is 0. ",
    "log10": "Throws error if data inside the column is 0. ",
    "log1p": "Throws error if data inside the column is 0. ",
    "hex": "Supports only integer values. ",
    "format_number": "Value for argument 'd' should not exceed 10. ",
    "substr": "Column type is not support for arguments 'pos' and 'len'. ",
    "to_char": "Argument 'pattern' should be a string value. ",
    "to_number": "Argument 'pattern' should be a string value. ",
    "cast": ["Does not support BooleanType, BinaryType, ArrayType, StructType and MapType. Remove these types or their corresponding string literal.",
             "While casting character data to TimestampType, TimestampNTZType, user can pass 'format' parameter to cast. Example - teraspark_df.select(teraspark_df.col.cast(teraTimestampNTZType(), 'YYYY-MM-DD HH24:MI:SSTZH:TZM').alias('dt'))"],
    "astype": ["Does not support BooleanType, BinaryType, ArrayType, StructType and MapType. Remove these types or their corresponding string literal.",
             "While casting character data to TimestampType, TimestampNTZType, user can pass 'format' parameter to astype. Example - teraspark_df.select(teraspark_df.col.cast(teraTimestampNTZType(), 'YYYY-MM-DD HH24:MI:SSTZH:TZM').alias('dt'))"],
    "setLogLevel": "Use `log` argument in TeradataSession.getOrCreate. Refer to logging in teradatasql for more details."
  }
}