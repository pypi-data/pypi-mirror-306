Metadata-Version: 2.1
Name: baram
Version: 0.5.0
Summary: AWS Framework for python
Author: Kwangsik Lee
Author-email: lks21c@gmail.com
Requires-Python: >=3.9,<4.0
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Requires-Dist: aiohttp (>=3.8.1,<4.0.0)
Requires-Dist: avro (>=1.12.0,<2.0.0)
Requires-Dist: awswrangler[modin] (==3.9.1)
Requires-Dist: boto3 (==1.35.35)
Requires-Dist: fire (>=0.4.0,<0.5.0)
Requires-Dist: nest-asyncio (>=1.5.5,<2.0.0)
Requires-Dist: pytest-cov (>=3.0.0,<4.0.0)
Requires-Dist: requests (>=2.28.1,<3.0.0)
Requires-Dist: sagemaker (>=2.232.2,<3.0.0)
Requires-Dist: six (>=1.16.0,<2.0.0)
Requires-Dist: toml (>=0.10.2,<0.11.0)
Requires-Dist: tzlocal (>=5.2,<6.0)
Requires-Dist: ujson (>=5.5.0,<6.0.0)
Description-Content-Type: text/markdown

## Baram

Python based AWS Framework which leverages boto3 and awswrangler.

Baram means "wind" in Korean which makes cloud move conveniently.

## Features

- TBD
- Convenient S3 Usage(KMS setting, delete directory ...)
- Athena Iceberg
- Athena Performance Management(cache, ctas_approach control)
- Glue Job Management

## Quick Start

```bash
> pip install awswrangler
```

## For Beginner

### S3 Usage

```python
# import S3Manager
from baram.s3_manager import S3Manager

sm = S3Manager("my_bucket_name")

# Upload local file to S3
sm.upload_file(local_file_path="local_file_path",
               s3_file_path="s3_file_path")

# Emphasize Directory Deletion
sm.download_dir(s3_dir_path="s3_directory_path",
                local_dir_path="local_directory_path")

# Copy S3 object
sm.copy_object(from_key="from_s3_key",
               to_key="to_s3_key")

```

## For Data Scientist

### S3 Usage

```python

# Read csv from s3

# EDA

# Merging Datasets from S3(https://aws-sdk-pandas.readthedocs.io/en/stable/tutorials/013%20-%20Merging%20Datasets%20on%20S3.html)

# write dataframe to s3

```

### Athena Usage

```python

# Read rows from Athena

# EDA

# write dataframe to Another Athena Table without schema input.

# Batching(Good for restricted memory environments), https://aws-sdk-pandas.readthedocs.io/en/stable/tutorials/006%20-%20Amazon%20Athena.html

# big table dump to s3

```

## For Data Engineer

### Manage S3

```python

# check_s3_object_exists

# count_csv_row_count

# rename_file

```

## Read The Docs

- [How to import Baram in Glue](TBD)
- [How to import Baram in SageMaker](TBD)
- [S3 Usage with Baram](TBD)
- [Athena Usage with Baram](TBD)
