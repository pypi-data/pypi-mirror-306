Metadata-Version: 2.1
Name: langclient
Version: 0.0.1
Summary: Versatile LLMs that works out of the box.
License: MIT
Author: amoooooo
Author-email: amor.budiyanto2006@gmail.com
Requires-Python: >=3.8,<4.0
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Requires-Dist: openai (>=1.52.2,<2.0.0)
Description-Content-Type: text/markdown


> [!WARNING]
> Project is in its early stage of development. Current version is not stable.

# *ü¶ô LangClient
Versatile AI client that works out of the box. Powered by OpenAI's Python SDK behind the scenes to handle dozens of AI providers without all the complexity. 

Supports the following out of the box:
- OpenAI
- SambaNova
- Anthropic
- Cohere
- Fireworks
- Together
- Vertex AI

## üõ†Ô∏è Usage
Set up a base client from one of the supported providers.
```python
from langclient import Client

client = Client(provider="sambanova",
                api_key="API_KEY")
```
Extend these clients with better functionality.
```python
from langclient.llm import LLM

llm = LLM(client, model="Meta-Llama-3.2-3B-Instruct")
for chunk in llm.stream("Tell me a joke"):
    print(chunk, end="")

```
Want to add your own Provider?
```python
from langclient.providers import Provider
from langclient import Client

perplexity = Provider(name="Perplexity", endpoint="https://api.perplexity.ai")
client = Client(provider=perplexity, api_key="API_KEY")
```

*Docs to be continued...*

## üéØ Motivation
~~Openmacro needed a rewrite.~~ Handling LLMs or AI models from different providers *should* be easy. Made this because it wasn't. LangClient is dirt simple but has tools that makes things nicer. Why not use `LangChain` instead? It's not built for production. 
