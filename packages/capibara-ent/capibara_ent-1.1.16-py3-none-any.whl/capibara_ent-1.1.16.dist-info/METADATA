Metadata-Version: 2.1
Name: capibara-ent
Version: 1.1.16
Summary: A flexible multimodal AI library for advanced contextual understanding and deployment.
Home-page: https://github.com/anacronic-io/capibaraGPT-v2
Author: Marco Durán
Author-email: marco@anachroni.com
Keywords: ai nlp multimodal machine-learning deep-learning language-models ethics tpu training deployment
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Requires-Python: >=3.8
Description-Content-Type: text/markdown
Requires-Dist: tensorflow>=2.5.0
Requires-Dist: docker
Requires-Dist: nltk
Requires-Dist: numpy
Requires-Dist: pandas
Requires-Dist: jax
Requires-Dist: flax
Requires-Dist: optax
Requires-Dist: wandb
Requires-Dist: tensorflow-hub
Requires-Dist: tqdm
Requires-Dist: PyYAML
Requires-Dist: spacy
Requires-Dist: transformers
Requires-Dist: scipy
Requires-Dist: matplotlib
Requires-Dist: seaborn
Provides-Extra: cloud_tpu
Requires-Dist: cloud-tpu-client; extra == "cloud-tpu"
Requires-Dist: google-auth; extra == "cloud-tpu"
Provides-Extra: dev
Requires-Dist: pytest; extra == "dev"
Requires-Dist: pytest-cov; extra == "dev"
Requires-Dist: sphinx; extra == "dev"
Requires-Dist: sphinx-rtd-theme; extra == "dev"
Requires-Dist: myst-parser; extra == "dev"
Requires-Dist: black; extra == "dev"
Requires-Dist: flake8; extra == "dev"
Provides-Extra: gpu
Requires-Dist: tensorflow-gpu>=2.5.0; extra == "gpu"
Provides-Extra: tpu
Requires-Dist: jax[tpu]>=0.2.21; extra == "tpu"
Requires-Dist: flax>=0.3.4; extra == "tpu"
Requires-Dist: optax>=0.0.9; extra == "tpu"

# CapibaraENT CLI

![Capibara SSBD Model](/capibara_model/src/public/capi33B2.webp)

CapibaraENT is a command-line tool for training, evaluating, and deploying Capibara-based language models, optimized for TPUs and featuring hyperparameter optimization.

## Features

- Training and evaluation of Capibara models
- Built-in TPU support
- Hyperparameter optimization
- Model deployment
- Performance measurement
- Docker container execution (optional)
- Integration with Weights & Biases for experiment tracking

## Requirements

- Python 3.7+
- JAX (for TPU optimization)
- TensorFlow
- Weights & Biases
- Docker (optional, for container execution)

## Installation

1. Clone this repository:

   ```bash
   git clone https://github.com/anachroni-io/capibaraent-cli.git
   cd capibaraent-cli
   ```

2. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```

3. Set up Weights & Biases:

   ```bash
   wandb login
   ```

## Usage

The CapibaraENT CLI offers various options for working with Capibara models:

```bash
python capibaraent_cli.py [options]
```

### Available options

- `--log-level`: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
- `--train`: Train the model
- `--evaluate`: Evaluate the model
- `--optimize`: Perform hyperparameter optimization
- `--use-docker`: Run the model inside Docker (optional, commented)
- `--deploy`: Deploy the model
- `--measure-performance`: Measure the model's performance
- `--model`: Path to the model YAML file (for deserialization)

### Usage Examples

1. Train a model:

   ```bash
   python capibaraent_cli.py --train
   ```

2. Evaluate a model:

   ```bash
   python capibaraent_cli.py --evaluate
   ```

3. Perform hyperparameter optimization:

   ```bash
   python optimize_hyperparameters.py
   ```

4. Deploy a model:

   ```bash
   python capibaraent_cli.py --deploy
   ```

5. Measure model performance:

   ```bash
   python capibaraent_cli.py --measure-performance
   ```

6. Run a model in Docker (optional, if Docker is set up):

   ```bash
   python capibaraent_cli.py --use-docker
   ```

## Configuration

Model configuration is handled through environment variables and YAML files. Key configuration parameters include:

- `CAPIBARA_LEARNING_RATE`
- `CAPIBARA_BATCH_SIZE`
- `CAPIBARA_MAX_LENGTH`
- `CAPIBARA_USE_TPU`
- `WANDB_PROJECT`
- `WANDB_ENTITY`

### Example `.env` file

```env
CAPIBARA_LEARNING_RATE=0.001
CAPIBARA_BATCH_SIZE=32
CAPIBARA_MAX_LENGTH=512
CAPIBARA_USE_TPU=True
WANDB_PROJECT=my_project
WANDB_ENTITY=my_entity
```

For a full list of configuration options, refer to the `.env.example` file.

## Hyperparameter Optimization

To perform hyperparameter optimization:

1. Ensure your Weights & Biases project is set up.
2. Run the optimization script:

   ```bash
   python optimize_hyperparameters.py
   ```

3. View the results in your Weights & Biases dashboard.

## Development

To contribute to the project:

1. Fork the repository
2. Create a new branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License

Distributed under the MIT License. See `LICENSE` for more information.

## Contact

Marco Durán - <marco@anachroni.com>

Project Link: [https://github.com/anachroni-io/capibaraent-cli](https://github.com/anachroni-io/capibaraent-cli)
